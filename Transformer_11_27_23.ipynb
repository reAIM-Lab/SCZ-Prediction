{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53be47ea",
   "metadata": {},
   "source": [
    "This notebook is incomplete but contains my work this far on creating a transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6227507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "from scipy.sparse import *\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD\n",
    "import pickle \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535b84f",
   "metadata": {},
   "source": [
    "# Create Data for Transformer\n",
    "### This model is looking at 90-day chunks of time, specifically between 90 days and 120 days pre-index date. This is NOT (yet) a visit-indexed model\n",
    "Limit to data between 7 years (2555 days) and 90 days, inclusive prior to diagnosis. This also means that the psychosis_diagnosis date should be at least **90 days** before diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb43415",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days_prediction = 90\n",
    "df_pop = pd.read_csv('population.csv')\n",
    "df_pop.rename({'psychosis_dx_date':'psychosis_diagnosis_date'}, axis=1, inplace=True)\n",
    "df_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_pop['psychosis_diagnosis_date'], format=\"mixed\")\n",
    "df_pop['cohort_start_date'] = pd.to_datetime(df_pop['cohort_start_date'])\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = pd.read_csv('temporal_visits.csv')\n",
    "all_conds = pd.read_csv('temporal_conditions.csv')\n",
    "all_procedures = pd.read_csv('temporal_procedures.csv')\n",
    "all_labs = pd.read_csv('temporal_labs.csv')\n",
    "all_meds = pd.read_csv('temporal_medications.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9de18",
   "metadata": {},
   "source": [
    "### Restrict data to timing that I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_days = 2555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meds = all_meds.loc[all_meds['person_id'].isin(df_pop['person_id'])]\n",
    "all_meds['cohort_start_date'] = pd.to_datetime(all_meds['cohort_start_date'])\n",
    "all_meds['drug_era_start_date'] = pd.to_datetime(all_meds['drug_era_start_date'])\n",
    "all_meds['drug_era_end_date'] = pd.to_datetime(all_meds['drug_era_end_date'])\n",
    "all_meds['censor_date'] = all_meds['cohort_start_date']-pd.Timedelta(num_days_prediction, 'd')\n",
    "\n",
    "# remove any medications that start after the censor date\n",
    "all_meds = all_meds.loc[all_meds['drug_era_start_date']<=all_meds['censor_date']]\n",
    "print(min((all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days))\n",
    "# replace end date of any medication that ends after the censor date with the censor date\n",
    "all_meds.loc[all_meds['drug_era_end_date']>all_meds['censor_date'], 'drug_era_end_date'] = all_meds.loc[all_meds['drug_era_end_date']>all_meds['censor_date'], 'censor_date']\n",
    "print(min((all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days))\n",
    "\n",
    "\n",
    "# remove any medications that end before the max day\n",
    "all_meds = all_meds.loc[(all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days <= max_days]\n",
    "print(max((all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days))\n",
    "#replace start date of any medication that starts before max days with max days\n",
    "all_meds['early_cutoff'] = all_meds['cohort_start_date']-pd.Timedelta(max_days, 'd')\n",
    "all_meds.loc[all_meds['drug_era_start_date']<all_meds['early_cutoff'], 'drug_era_start_date'] = all_meds.loc[all_meds['drug_era_start_date']<all_meds['early_cutoff'], 'early_cutoff']\n",
    "print(max((all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days))\n",
    "\n",
    "all_meds['days_to_cohort_start'] = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = all_visits.loc[all_visits['person_id'].isin(df_pop['person_id'])]\n",
    "all_visits['cohort_start_date'] = pd.to_datetime(all_visits['cohort_start_date'])\n",
    "all_visits['visit_start_date'] = pd.to_datetime(all_visits['visit_start_date'])\n",
    "all_visits['visit_end_date'] = pd.to_datetime(all_visits['visit_end_date'])\n",
    "all_visits['censor_date'] = all_visits['cohort_start_date']-pd.Timedelta(num_days_prediction, 'd')\n",
    "\n",
    "# remove any medications that start after the censor date\n",
    "all_visits = all_visits.loc[all_visits['visit_start_date']<=all_visits['censor_date']]\n",
    "print(min((all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days))\n",
    "# replace end date of any medication that ends after the censor date with the censor date\n",
    "all_visits.loc[all_visits['visit_end_date']>all_visits['censor_date'], 'visit_end_date'] = all_visits.loc[all_visits['visit_end_date']>all_visits['censor_date'], 'censor_date']\n",
    "print(min((all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days))\n",
    "\n",
    "\n",
    "# remove any medications that end before the max day\n",
    "all_visits = all_visits.loc[(all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days <= max_days]\n",
    "print(max((all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days))\n",
    "#replace start date of any medication that starts before max days with max days\n",
    "all_visits['early_cutoff'] = all_visits['cohort_start_date']-pd.Timedelta(max_days, 'd')\n",
    "all_visits.loc[all_visits['visit_start_date']<all_visits['early_cutoff'], 'visit_start_date'] = all_visits.loc[all_visits['visit_start_date']<all_visits['early_cutoff'], 'early_cutoff']\n",
    "print(max((all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days))\n",
    "\n",
    "all_visits['days_to_cohort_start'] = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67515fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds = all_conds.loc[all_conds['person_id'].isin(df_pop['person_id'])]\n",
    "all_conds['cohort_start_date'] = pd.to_datetime(all_conds['cohort_start_date'])\n",
    "all_conds['condition_start_date'] = pd.to_datetime(all_conds['condition_start_date'])\n",
    "all_conds['days_to_cohort_start'] = (all_conds['cohort_start_date']-all_conds['condition_start_date']).dt.days\n",
    "all_conds = all_conds.loc[all_conds['days_to_cohort_start'] >= num_days_prediction]\n",
    "all_conds = all_conds.loc[all_conds['days_to_cohort_start'] <= max_days]\n",
    "\n",
    "print(all_conds['days_to_cohort_start'].min(),all_conds['days_to_cohort_start'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63444b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_procedures = all_procedures.loc[all_procedures['person_id'].isin(df_pop['person_id'])]\n",
    "all_procedures['cohort_start_date'] = pd.to_datetime(all_procedures['cohort_start_date'])\n",
    "all_procedures['procedure_date'] = pd.to_datetime(all_procedures['procedure_date'])\n",
    "all_procedures['days_to_cohort_start'] = (all_procedures['cohort_start_date']-all_procedures['procedure_date']).dt.days\n",
    "all_procedures = all_procedures.loc[all_procedures['days_to_cohort_start'] >= num_days_prediction]\n",
    "all_procedures = all_procedures.loc[all_procedures['days_to_cohort_start'] <= max_days]\n",
    "\n",
    "print(all_procedures['days_to_cohort_start'].min(), all_procedures['days_to_cohort_start'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs = all_labs.loc[all_labs['person_id'].isin(df_pop['person_id'])]\n",
    "all_labs['cohort_start_date'] = pd.to_datetime(all_labs['cohort_start_date'])\n",
    "all_labs['measurement_date'] = pd.to_datetime(all_labs['measurement_date'])\n",
    "all_labs['days_to_cohort_start'] = (all_labs['cohort_start_date']-all_labs['measurement_date']).dt.days\n",
    "all_labs = all_labs.loc[all_labs['days_to_cohort_start'] >= num_days_prediction]\n",
    "all_labs = all_labs.loc[all_labs['days_to_cohort_start'] <= max_days]\n",
    "\n",
    "print(all_labs['days_to_cohort_start'].min(), all_labs['days_to_cohort_start'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa315f",
   "metadata": {},
   "source": [
    "### concatenate all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae90f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rare_occurrences(df, col_concept, col_id = 'person_id', size_pop = len(df_pop)):\n",
    "    unique_occurrences = df[['person_id', col_concept]].drop_duplicates()\n",
    "    unique_occurrences = unique_occurrences.value_counts(col_concept)\n",
    "    common_occurrences = unique_occurrences[unique_occurrences/size_pop > 0.01].index\n",
    "    return df.loc[df[col_concept].isin(common_occurrences)]\n",
    "all_conds = drop_rare_occurrences(all_conds, 'condition_concept_id')\n",
    "all_meds = drop_rare_occurrences(all_meds, 'drug_concept_id')\n",
    "all_procedures = drop_rare_occurrences(all_procedures, 'procedure_concept_id')\n",
    "all_labs = drop_rare_occurrences(all_labs, 'measurement_concept_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs = all_labs[['person_id', 'days_to_cohort_start', 'measurement_concept_id']].drop_duplicates()\n",
    "all_labs.rename({'measurement_concept_id':'concept_id'}, axis=1, inplace=True)\n",
    "len(all_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fbab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits['los'] = (all_visits['visit_end_date']-all_visits['visit_start_date']).dt.days\n",
    "los_df = all_visits.loc[all_visits['visit_concept_id'].isin([9201, 9203])]\n",
    "los_df = los_df.loc[los_df['los']>0]\n",
    "los_df = los_df[['person_id', 'visit_concept_id', 'los', 'days_to_cohort_start']]\n",
    "\n",
    "list_temp_arrs = []\n",
    "for i in tqdm(range(0,len(los_df))):\n",
    "    n_repeats = los_df.iloc[i]['los']\n",
    "    temp_arr = los_df.iloc[i].values.reshape(-1, 1).repeat(n_repeats, axis=0).reshape(4,n_repeats).T\n",
    "    replace_days_to_cohort_start = np.arange(los_df.iloc[i]['days_to_cohort_start'], los_df.iloc[i]['days_to_cohort_start']-n_repeats, -1)\n",
    "    temp_arr[:,-1] = replace_days_to_cohort_start\n",
    "    list_temp_arrs.append(temp_arr)\n",
    "\n",
    "los_df = pd.DataFrame(np.vstack(list_temp_arrs), columns = ['person_id', 'concept_id', 'los', 'days_to_cohort_start'])\n",
    "los_df['concept_id'].replace({9201:9205, 9203:9207}, inplace=True)\n",
    "los_df.drop(['los'], axis=1, inplace=True)\n",
    "len(los_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7855ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = all_visits[['person_id', 'days_to_cohort_start', 'visit_concept_id']].drop_duplicates()\n",
    "all_visits.rename({'visit_concept_id':'concept_id'}, axis=1, inplace=True)\n",
    "len(all_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meds = all_meds[['person_id', 'drug_exposure_count','drug_concept_id', 'days_to_cohort_start']].drop_duplicates()\n",
    "# change the name to make merging the dfs easier later\n",
    "all_meds.rename({'drug_concept_id':'concept_id'}, axis=1, inplace=True)\n",
    "\n",
    "single_day_meds = all_meds.loc[all_meds['drug_exposure_count']==1]\n",
    "print(len(single_day_meds))\n",
    "multi_day_meds = all_meds.loc[all_meds['drug_exposure_count']>1]\n",
    "print(len(multi_day_meds))\n",
    "list_temp_arrs = []\n",
    "for i in tqdm(range(0,len(multi_day_meds))):\n",
    "    n_repeats = int(multi_day_meds.iloc[i]['drug_exposure_count'])\n",
    "    temp_arr = multi_day_meds.iloc[i].values.reshape(-1, 1).repeat(n_repeats, axis=0).reshape(4,n_repeats).T\n",
    "    replace_days_to_cohort_start = np.arange(multi_day_meds.iloc[i]['days_to_cohort_start'], multi_day_meds.iloc[i]['days_to_cohort_start']-n_repeats, -1)\n",
    "    temp_arr[:,-1] = replace_days_to_cohort_start\n",
    "    list_temp_arrs.append(temp_arr)\n",
    "    \n",
    "multi_day_meds = pd.DataFrame(np.vstack(list_temp_arrs), columns = single_day_meds.columns)\n",
    "all_meds = pd.concat([multi_day_meds, single_day_meds])\n",
    "\n",
    "all_meds.drop(['drug_exposure_count'], axis=1, inplace=True)\n",
    "\n",
    "len(all_meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21990724",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds = all_conds[['person_id', 'condition_concept_id', 'days_to_cohort_start']].drop_duplicates()\n",
    "# change the name to make merging the dfs easier later\n",
    "all_conds.rename({'condition_concept_id':'concept_id'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_procedures = all_procedures[['person_id', 'days_to_cohort_start', 'procedure_concept_id']].drop_duplicates()\n",
    "# change the name to make merging the dfs easier later\n",
    "all_procedures.rename({'procedure_concept_id':'concept_id'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8328e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([all_conds, all_meds, los_df, all_visits, all_labs, all_procedures])\n",
    "all_features['days_to_cohort_start'] = all_features['days_to_cohort_start']//90\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7897c7",
   "metadata": {},
   "source": [
    "### Create sparse dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddfb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpr_row = all_features.groupby(['person_id', 'days_to_cohort_start']).grouper\n",
    "idx_row = grpr_row.group_info[0]\n",
    "\n",
    "grpr_col = all_features.groupby('concept_id').grouper\n",
    "idx_col = grpr_col.group_info[0]\n",
    "\n",
    "sparse_data = csr_matrix((all_features['concept_id'].values, (idx_row, idx_col)),shape=(grpr_row.ngroups, grpr_col.ngroups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = grpr_row.result_index\n",
    "df_columns = list(grpr_col.result_index)\n",
    "sparse_df = pd.DataFrame.sparse.from_spmatrix(sparse_data, index = df_index, columns = df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2725fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sparse_df.columns:\n",
    "    sparse_df[i] = sparse_df[i]/i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569f57a",
   "metadata": {},
   "source": [
    "### Create multi-indexed dataframe that is a standard shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab21327",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_patients = sparse_df.index.get_level_values(0).unique()\n",
    "patient_indices = np.arange(0,len(index_patients), 1)\n",
    "\n",
    "cohort_start_days = np.arange(1, all_features['days_to_cohort_start'].max()+1, 1)\n",
    "feature_indices = np.arange(0, sparse_df.shape[1], 1)\n",
    "\n",
    "df_features = pd.DataFrame(data = 0, index = pd.MultiIndex.from_product([index_patients, cohort_start_days], names=[\"person_id\", \"months\"]), columns = sparse_df.columns, dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df = sparse_df[sparse_df.index.get_level_values(1)>0]\n",
    "print(datetime.now())\n",
    "df_features.loc[sparse_df.index] = sparse_df.values\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my y_values vector in the correct order (according to index_patients)\n",
    "mat_y = np.asarray(df_pop.set_index('person_id').loc[index_patients, 'sz_flag']).reshape(len(index_patients), 1)\n",
    "df_pop = df_pop.loc[df_pop['person_id'].isin(index_patients)] # remove people who have no entries in person_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60a1b4",
   "metadata": {},
   "source": [
    "### train test split and basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12339164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first train-test split (to get test test data)\n",
    "train_pop, test_pop, train_labels, test_labels, train_inds, test_inds = train_test_split(df_pop, mat_y, np.arange(0, len(df_pop), 1), random_state=23, test_size=0.2, stratify=mat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a739f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_mat = csr_matrix(df_features.loc[train_pop['person_id']].values)\n",
    "print(train_sparse_mat.shape)\n",
    "\n",
    "test_sparse_mat = csr_matrix(df_features.loc[test_pop['person_id']].values)\n",
    "print(test_sparse_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sparse_mat = test_sparse_mat.todense()\n",
    "train_sparse_mat = train_sparse_mat.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286630a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_sparse_mat = scaler.fit_transform(train_sparse_mat)\n",
    "print('done with fit/first transform')\n",
    "test_sparse_mat = scaler.transform(test_sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels to go along with train/test mats\n",
    "y_train = mat_y[train_inds]\n",
    "print(len(y_train))\n",
    "y_test = mat_y[test_inds]\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d801f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_features\n",
    "del sparse_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a348cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('standard_scaled_mats.npz', train_sparse_mat, test_sparse_mat)\n",
    "pd.DataFrame(y_train, index=train_inds).to_csv('training_labels.csv')\n",
    "pd.DataFrame(y_test, index=test_inds).to_csv('testing_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pca = IncrementalPCA(n_components=1990, batch_size = 100)\n",
    "chunk_size = 10000\n",
    "for i in tqdm(range(0, len(train_sparse_mat)//chunk_size)):\n",
    "    pca.partial_fit(train_sparse_mat[i*chunk_size : (i+1)*chunk_size,:])\n",
    "\n",
    "print('done fitting')\n",
    "\n",
    "pickle.dump(pca, open(\"psychosis_prediction/pca.pkl\",\"wb\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a94a27",
   "metadata": {},
   "source": [
    "## Read in data and transform to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e65118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257164, 2026)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_mats = np.load('standard_scaled_mats.npz')\n",
    "y_train = pd.read_csv('training_labels.csv', index_col = 0)\n",
    "y_test = pd.read_csv('testing_labels.csv', index_col = 0)\n",
    "\n",
    "train_sparse_mat = loaded_mats['arr_0']\n",
    "test_sparse_mat = loaded_mats['arr_1']\n",
    "train_sparse_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb169f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npca = pickle.load(open(\"psychosis_prediction/pca.pkl\",\\'rb\\'))\\nprint(datetime.now())\\ntrain_sparse_mat = pca.transform(train_sparse_mat)\\nprint(datetime.now())\\ntest_sparse_mat = pca.transform(test_sparse_mat)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pca = pickle.load(open(\"psychosis_prediction/pca.pkl\",'rb'))\n",
    "print(datetime.now())\n",
    "train_sparse_mat = pca.transform(train_sparse_mat)\n",
    "print(datetime.now())\n",
    "test_sparse_mat = pca.transform(test_sparse_mat)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66ad450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation split\n",
    "train_pop, val_pop, train_labels, test_labels, train_inds, val_inds = train_test_split(y_train.index, y_train, np.arange(0, len(y_train), 1), random_state=24, test_size=0.1, stratify=y_train)\n",
    "y_val = y_train.loc[val_pop]\n",
    "y_train = y_train.loc[train_pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576211",
   "metadata": {},
   "source": [
    "reshape gives me patients * timeseries * features\n",
    "(https://stackoverflow.com/questions/54615882/how-to-convert-a-pandas-multiindex-dataframe-into-a-3d-array)\n",
    "\n",
    "then I reorder to patients * features * timeseries (0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd47a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20154, 2026, 28)\n",
      "(80613, 2026, 28)\n"
     ]
    }
   ],
   "source": [
    "num_train_pop = len(y_train)\n",
    "num_test_pop = len(y_test)\n",
    "num_val_pop = len(y_val)\n",
    "len_sequence = 28\n",
    "num_features = train_sparse_mat.shape[1]\n",
    "#val_mat_features = df_val_features.astype('float32').values.reshape(num_val_pop, len_sequence, num_features).transpose(0, 2, 1)\n",
    "test_mat_features = test_sparse_mat.reshape(num_test_pop, len_sequence, num_features).transpose(0, 2, 1)\n",
    "print(test_mat_features.shape)\n",
    "train_mat_features = train_sparse_mat.reshape(num_train_pop+num_val_pop, len_sequence, num_features).transpose(0, 2, 1)\n",
    "print(train_mat_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b84f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mat_features = train_mat_features[val_inds, :, :]\n",
    "train_mat_features = train_mat_features[train_inds, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb191f1",
   "metadata": {},
   "source": [
    "### Create Dataloader Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ab8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedSampler(object):\n",
    "    \"\"\"Stratified Sampling\n",
    "\n",
    "    https://github.com/ncullen93/torchsample/tree/master/torchsample\n",
    "    \"\"\"\n",
    "    def __init__(self, class_vector, batch_size):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        class_vector : torch tensor\n",
    "            a vector of class labels\n",
    "        batch_size : integer\n",
    "            batch_size\n",
    "        \"\"\"\n",
    "        self.n_splits = int(class_vector.shape[0] / batch_size)\n",
    "        self.class_vector = class_vector\n",
    "\n",
    "    def gen_sample_array(self):\n",
    "        try:\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "        except:\n",
    "            print('Need scikit-learn for this functionality')\n",
    "        import numpy as np\n",
    "        \n",
    "        s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5)\n",
    "        X = torch.randn(self.class_vector.shape[0],2).numpy()\n",
    "        y = self.class_vector\n",
    "        s.get_n_splits(X, y)\n",
    "\n",
    "        train_index, test_index = next(s.split(X, y))\n",
    "        return np.hstack([train_index, test_index])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_sample_array())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26963f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = StratifiedSampler(class_vector=np.asarray(y_train), batch_size=256)\n",
    "train_data = torch.utils.data.TensorDataset(torch.Tensor(train_mat_features), torch.Tensor(y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=256, sampler=train_sampler)\n",
    "\n",
    "val_sampler = StratifiedSampler(class_vector=np.asarray(y_val), batch_size=256)\n",
    "val_data = torch.utils.data.TensorDataset(torch.Tensor(val_mat_features), torch.Tensor(y_val.values))\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=256, sampler = val_sampler)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(torch.Tensor(test_mat_features), torch.Tensor(y_test.values))\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_loader, 'train_loader.pth')\n",
    "torch.save(val_loader, 'val_loader.pth')\n",
    "torch.save(test_loader, 'test_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14abdb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_sparse_mat\n",
    "del train_data\n",
    "del test_data\n",
    "del val_data\n",
    "del train_mat_features\n",
    "del test_mat_features\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825c998",
   "metadata": {},
   "source": [
    "# Moving on to Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d34e8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # from pytorch \n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, input_size, n_features, hidden_size, num_layers, num_heads, output_size):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embedding = nn.Linear(n_features, hidden_size)\n",
    "        #self.embedding = nn.Conv1d(n_features, hidden_size, 1, padding='same')\n",
    "        self.pos_enc = PositionalEncoding(hidden_size, dropout=0.1)\n",
    "        self.transformerenc = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=2048, dropout = 0.5),\n",
    "                                                    num_layers = num_layers)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "                           nn.Linear(hidden_size, hidden_size//2),\n",
    "                           nn.Dropout(0.5),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(hidden_size//2, hidden_size//4),\n",
    "                           nn.Dropout(0.5),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(hidden_size//4, hidden_size//8),\n",
    "                           nn.Dropout(0.5),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(hidden_size//8, 1),\n",
    "                           nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        print(x.shape)\n",
    "        x = self.embedding(x) #linear looks at last dim so batch, sequence, features)\n",
    "        # TransformerEncoder expects input in the shape (sequence, batch, features)\n",
    "        print(x.shape)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        print(x.shape)\n",
    "        x = self.pos_enc(x)\n",
    "        print(x.shape)\n",
    "        x = self.transformerenc(x)\n",
    "        print(x.shape)\n",
    "        # Take the output from the last time step\n",
    "        x = x[-1, :, :]\n",
    "        print(x.shape)\n",
    "        x = self.regressor(x)\n",
    "        print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3702e",
   "metadata": {},
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, n_features, hidden_size, num_layers, num_heads, output_size):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.embedding = nn.Linear(n_features, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=num_heads, \n",
    "                                             num_encoder_layers=num_layers, num_decoder_layers=num_layers, \n",
    "                                             dim_feedforward=hidden_size*3, dropout=0.1, \n",
    "                                             activation='relu', device=self.device)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.embedding(x) #linear looks at last dim so batch, sequence, features)\n",
    "        # TransformerEncoder expects input in the shape (sequence, batch, features)\n",
    "        #x = x.permute(2, 0, 1)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        # Take the output from the last time step\n",
    "        x = x[-1, :, :]\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef715f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_BCELoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(Weighted_BCELoss, self).__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, output, target, smooth=1):\n",
    "        loss = self.weights[1] * (target * torch.log(output)) + self.weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4faaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") \n",
    "#loss_weights = torch.tensor(sum(y_train.values==0)/sum(y_train.values)).to(device)#; loss_criterion=torch.nn.BCEWithLogitsLoss(pos_weight = loss_weights)\n",
    "#loss_weights = torch.tensor(np.asarray([sum(y_train.values)/len(y_train), 1-sum(y_train.values)/len(y_train)])).to(device)\n",
    "loss_weights = torch.tensor(np.asarray([sum(y_train.values==0)/ sum(y_train.values==0), 0.25*sum(y_train.values==0)/sum(y_train.values)])).to(device)\n",
    "def train(train_loader, model, device, optimizer, criterion=Weighted_BCELoss(weights=loss_weights)):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    list_training_loss = []\n",
    "    \n",
    "    true_ys = []\n",
    "    pred_ys = []\n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        signals, labels = signals.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        list_training_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        true_ys.append(labels.detach().cpu().numpy())\n",
    "        pred_ys.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    true_ys_flattened = np.concatenate(true_ys).ravel()\n",
    "    pred_ys_flattened = np.concatenate(pred_ys).ravel()\n",
    "    pred_labels = (pred_ys_flattened>0.5)*1\n",
    "    \n",
    "    auc_train = roc_auc_score(true_ys_flattened, pred_ys_flattened)\n",
    "    f1_train = f1_score(true_ys_flattened, pred_labels)\n",
    "    correct_label = accuracy_score(true_ys_flattened, pred_labels)\n",
    "\n",
    "    return auc_train, f1_train, correct_label, np.mean(list_training_loss)\n",
    "\n",
    "def test(test_loader, model, device, criteria=Weighted_BCELoss(weights=loss_weights), verbose=True):\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    list_testing_loss = []\n",
    "    true_ys = []\n",
    "    pred_ys = []\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        x, y = torch.Tensor(x.float()).to(device), torch.Tensor(y.float()).to(device)\n",
    "        out = model(x)\n",
    "        \n",
    "        true_ys.append(y.detach().cpu().numpy())\n",
    "        pred_ys.append(out.detach().cpu().numpy())\n",
    "\n",
    "        loss = criteria(out.squeeze(), y.squeeze())\n",
    "        total_loss += loss.item()\n",
    "        list_testing_loss.append(loss.item())\n",
    "    \n",
    "    true_ys_flattened = np.concatenate(true_ys).ravel()\n",
    "    pred_ys_flattened = np.concatenate(pred_ys).ravel()\n",
    "    pred_labels = (pred_ys_flattened>0.5)*1\n",
    "    \n",
    "    print(sum(pred_labels))\n",
    "    auc_test = roc_auc_score(true_ys_flattened, pred_ys_flattened)\n",
    "    f1_test = f1_score(true_ys_flattened, pred_labels)\n",
    "    correct_label = accuracy_score(true_ys_flattened, pred_labels)\n",
    "\n",
    "    return auc_test, f1_test, correct_label, np.mean(list_testing_loss)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, optimizer, n_epochs, device ,cv=0):\n",
    "    train_loss_trend = []\n",
    "    test_loss_trend = []\n",
    "\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        auc_train, f1_train, accuracy_train, mean_loss_train = train(train_loader, model, device, optimizer)\n",
    "        auc_test, f1_test, accuracy_test, mean_loss_test = test(valid_loader, model, device)\n",
    "        \n",
    "        train_loss_trend.append(mean_loss_train)\n",
    "        test_loss_trend.append(mean_loss_test)\n",
    "        #lr_scheduler.step()\n",
    "        if epoch % 5 == 0:\n",
    "            print('\\nEpoch %d' % (epoch))\n",
    "            print('Training ===>loss: ', mean_loss_train,\n",
    "                  ' Accuracy: %.2f percent' % (100*accuracy_train),\n",
    "                  ' AUC: %.2f' % (auc_train),\n",
    "                 ' F1-Score: %.2f' % (f1_train))\n",
    "            print('Test ===>loss: ', mean_loss_test,\n",
    "                  ' Accuracy: %.2f percent' % (100*accuracy_test),\n",
    "                  ' AUC: %.2f' % (auc_test),\n",
    "                 ' F1-Score: %.2f' % (f1_test))\n",
    "        \"\"\"\n",
    "        if epoch > 10:\n",
    "            if (test_loss_trend[-2] < test_loss_trend[-1]) & (test_loss_trend[-3] < test_loss_trend[-1]):\n",
    "                print('Breaking at epoch', epoch)\n",
    "                break\n",
    "        \"\"\"\n",
    "\n",
    "    # Save model and results\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.join(\"./ckpt/\", data)):\n",
    "        os.mkdir(os.path.join(\"./ckpt/\", data))\n",
    "    if not os.path.exists(os.path.join(\"./plots/\", data)):\n",
    "        os.mkdir(os.path.join(\"./plots/\", data))\n",
    "    \"\"\"\n",
    "    #torch.save(model.state_dict(), 'psychosis_prediction/models/transformer1.pt')\n",
    "    plt.plot(train_loss_trend, label='Train loss')\n",
    "    plt.plot(test_loss_trend, label='Validation loss')\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43aee0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256, 2026, 28])\n",
      "torch.Size([256, 28, 2026])\n",
      "torch.Size([256, 28, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([28, 256, 1024])\n",
      "torch.Size([256, 1024])\n",
      "torch.Size([256, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_471930/2026822970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# last thing i did was add more layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_471930/461939431.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, optimizer, n_epochs, device, cv)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mauc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_loss_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_471930/461939431.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, device, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlist_training_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Train the model\n",
    "n_epochs = 20\n",
    "input_size = val_mat_features.shape[2]\n",
    "n_features = val_mat_features.shape[1]\n",
    "hidden_size = 1024 # also try 512; 2048 did not work\n",
    "num_layers = 4 #usually 4\n",
    "num_heads = 8 # have tried 4, 8\n",
    "output_size = 1\n",
    "learning_rate = 1e-5\n",
    "batch_size = 256\n",
    "\n",
    "model = EncoderModel(input_size, n_features, hidden_size, num_layers, num_heads, output_size)\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, n_epochs, device,cv=0)\n",
    "# last thing i did was add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now look at summary stats for whole train data\n",
    "list_outputs = []\n",
    "list_labels = []\n",
    "for _, [batch_x, batch_y] in enumerate(tqdm(val_loader)):\n",
    "    outputs = model(batch_x.to(device))\n",
    "    list_outputs.append(outputs)\n",
    "    list_labels.append(batch_y)\n",
    "\n",
    "list_outputs = np.round(torch.concat(list_outputs).to('cpu').detach().numpy())\n",
    "list_labels = torch.concat(list_labels).to('cpu').numpy()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(list_labels, list_outputs).ravel()\n",
    "print('sensitivity', tp/(tp+fn))\n",
    "print('specificity', tn/(tn+fp))\n",
    "print('roc_auc', roc_auc_score(list_labels, list_outputs))\n",
    "print('auprc', precision_score(list_labels, list_outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
