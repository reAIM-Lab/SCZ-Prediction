{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2829428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "import pickle \n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "sys.path.append('../')\n",
    "from preprocessing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9fe65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codes_path = '../../codes_mappings/'\n",
    "validation_set = False\n",
    "\n",
    "# how much data we are using to make predictions\n",
    "forward_iterations = 14 # 3 years\n",
    "backwards_iterations = 19 # full history: 65; median history: 19 (4.3 years)\n",
    "days_per_iter = 90 # interval size\n",
    "\n",
    "# censor date to cohort start date\n",
    "num_days_prediction = 90\n",
    "\n",
    "with open(f'{int_path}/{dataset_prefix}du_snomed_colnames', \"rb\") as fp:   #Pickling\n",
    "    data_columns = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be3102",
   "metadata": {},
   "source": [
    "# Description\n",
    "1. Load data and run checks on dates + duplicates + presence of schizophrenia information\n",
    "2. Map data to the rolled up concepts where appropriate\n",
    "3. Create a population dataframe with **sequence length** entries per patient. This should have the start date (inclusive) and end date (exclusive) for each subsequence\n",
    "4. Limit to the date range for the given iteration for each person and call the function that creates the features\n",
    "   - Conditions, procedures, and labs: counts per subsequence time\n",
    "   - Medications: total days (end day-start day + 1 to account for single-day medications)\n",
    "   - Visits: number of visits (frequency)and mean + summed length of stay (end day - start day to count overnights)\n",
    "5. Populate time since psychosis\n",
    "6. Fill in \"blank\" iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fab9b",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Also constrict to patients with psychosis at least 6 months pre-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db507ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107626 0.1280452678720755 107626\n",
      "102739 0.11423120723386446 102739\n"
     ]
    }
   ],
   "source": [
    "# read in population dataframe\n",
    "df_pop = pd.read_csv(f'{data_path}/population_2dx.csv', parse_dates = ['psychosis_diagnosis_date', 'scz_diagnosis_date', 'cohort_start_date'])\n",
    "print(len(df_pop), df_pop['sz_flag'].sum()/len(df_pop), len(df_pop['person_id'].unique()))\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]\n",
    "print(len(df_pop), df_pop['sz_flag'].sum()/len(df_pop), len(df_pop['person_id'].unique()))\n",
    "df_pop['censor_date'] = df_pop['cohort_start_date'] - pd.Timedelta(days=num_days_prediction)\n",
    "\n",
    "count_visits = pd.read_csv(f'{int_path}/hcu_visit_counts.csv', parse_dates = ['first_visit'])\n",
    "df_pop = df_pop.merge(count_visits[['person_id', 'first_visit']], how = 'left', on = 'person_id')\n",
    "\n",
    "if dataset == 'mdcd_1yr': \n",
    "    df_3yr_pop = pd.read_csv(f'{path}/raw_data_mdcd_3yrs/population.csv')\n",
    "    print(len(df_pop))\n",
    "    df_pop = df_pop.loc[~df_pop['person_id'].isin(df_3yr_pop['person_id'])]\n",
    "    print(len(df_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ea9328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Visits (should be True) True True\n"
     ]
    }
   ],
   "source": [
    "all_visits = pd.read_csv(f'{data_path}/temporal_visits.csv', parse_dates = ['cohort_start_date', 'visit_start_date', 'visit_end_date'])\n",
    "all_visits = pre_censor_data(all_visits, df_pop, 'visit_start_date')\n",
    "all_visits.loc[all_visits['visit_end_date'] > all_visits['censor_date'], 'visit_end_date'] = all_visits.loc[all_visits['visit_end_date'] > all_visits['censor_date'], 'censor_date']\n",
    "print('Duplicate Visits (should be True)', 'Unnamed: 0' not in all_visits.columns, len(all_visits) == len(all_visits['visit_occurrence_id'].unique()))\n",
    "\n",
    "all_meds = pd.read_csv(f'{data_path}/temporal_medications.csv', parse_dates = ['cohort_start_date', 'drug_era_start_date', 'drug_era_end_date'])\n",
    "all_meds = pre_censor_data(all_meds, df_pop, 'drug_era_start_date')\n",
    "all_meds.loc[all_meds['drug_era_end_date'] > all_meds['censor_date'], 'drug_era_end_date'] = all_meds.loc[all_meds['drug_era_end_date'] > all_meds['censor_date'], 'censor_date']\n",
    "\n",
    "all_conds = pd.read_csv(f'{data_path}/temporal_conditions.csv', parse_dates = ['cohort_start_date', 'condition_start_date'])\n",
    "all_conds = pre_censor_data(all_conds, df_pop, 'condition_start_date')\n",
    "\n",
    "all_procedures = pd.read_csv(f'{data_path}/temporal_procedures.csv', parse_dates = ['cohort_start_date', 'procedure_date'])\n",
    "all_procedures = pre_censor_data(all_procedures, df_pop, 'procedure_date')\n",
    "\n",
    "all_labs = pd.read_csv(f'{data_path}/temporal_labs.csv', parse_dates = ['cohort_start_date', 'measurement_date'])\n",
    "all_labs = pre_censor_data(all_labs, df_pop, 'measurement_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ad5c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check granular presence of SCZ: 0\n"
     ]
    }
   ],
   "source": [
    "# check for schizophrenia in fine-grained conditions\n",
    "scz_codes = pd.read_csv(codes_path+'all_scz_codes.csv')\n",
    "print('Check granular presence of SCZ:',len(all_conds.loc[all_conds['condition_concept_id'].isin(scz_codes['standard_concept_id'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ddc75",
   "metadata": {},
   "source": [
    "### Map data to other concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42a157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Meds (Expect True) True True\n"
     ]
    }
   ],
   "source": [
    "rolled_medications = pd.read_csv(codes_path + 'rolled_medications.csv')\n",
    "all_meds = all_meds.merge(rolled_medications[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'drug_concept_id', right_on = 'descendant_concept_id')\n",
    "all_meds = all_meds[['person_id','drug_era_id','drug_era_start_date', 'drug_era_end_date', 'cohort_start_date', 'drug_concept_id', 'rolled_concept_name', 'drug_exposure_count', 'censor_date']].drop_duplicates()\n",
    "all_meds.loc[all_meds['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_meds.loc[all_meds['rolled_concept_name'].isna(), 'drug_concept_id']\n",
    "\n",
    "list_med_concepts = list(all_meds['rolled_concept_name'])\n",
    "list_med_concepts = [str(i) + '_meds' for i in list_med_concepts]\n",
    "all_meds['rolled_concept_name'] = list_med_concepts\n",
    "\n",
    "print('Duplicate Meds (Expect True)', 'Unnamed: 0' not in all_meds.columns, len(all_meds) == len(all_meds[['person_id', 'rolled_concept_name', 'drug_concept_id', 'drug_era_start_date', 'drug_era_end_date']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d538bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Conds (Expect True) True True\n"
     ]
    }
   ],
   "source": [
    "rolled_conditions = pd.read_csv(codes_path + 'rolled_conditions_level4.csv')\n",
    "all_conds = all_conds.merge(rolled_conditions[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'condition_concept_id', right_on = 'descendant_concept_id')\n",
    "all_conds = all_conds[['person_id','condition_occurrence_id','condition_start_date', 'condition_concept_id', 'concept_name', 'rolled_concept_name', 'cohort_start_date', 'visit_occurrence_id', 'censor_date']].drop_duplicates()\n",
    "all_conds.loc[all_conds['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_conds.loc[all_conds['rolled_concept_name'].isna(), 'concept_name']\n",
    "\n",
    "list_cond_concepts = list(all_conds['rolled_concept_name'])\n",
    "list_cond_concepts = [str(i) + '_conds' for i in list_cond_concepts]\n",
    "all_conds['rolled_concept_name'] = list_cond_concepts\n",
    "\n",
    "# now check in more granular conditions\n",
    "for i in list_cond_concepts:\n",
    "    if 'schizo' in i.lower():\n",
    "        print(i)\n",
    "# check uniqueness\n",
    "print('Duplicate Conds (Expect True)', 'Unnamed: 0' not in all_conds.columns, len(all_conds) == len(all_conds[['person_id', 'rolled_concept_name', 'condition_start_date', 'condition_occurrence_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11cc8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Procedures (Expect True) True True\n"
     ]
    }
   ],
   "source": [
    "rolled_procedures = pd.read_csv(codes_path + 'rolled_procedures_level4.csv')\n",
    "all_procedures = all_procedures.merge(rolled_procedures[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'procedure_concept_id', right_on = 'descendant_concept_id')\n",
    "all_procedures = all_procedures[['person_id','procedure_occurrence_id','procedure_date', 'procedure_concept_id','concept_name', 'rolled_concept_name', 'cohort_start_date', 'censor_date']].drop_duplicates()\n",
    "all_procedures.loc[all_procedures['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_procedures.loc[all_procedures['rolled_concept_name'].isna(), 'concept_name']\n",
    "\n",
    "list_procedure_concepts = list(all_procedures['rolled_concept_name'])\n",
    "list_procedure_concepts = [str(i) + '_procedure' for i in list_procedure_concepts]\n",
    "all_procedures['rolled_concept_name'] = list_procedure_concepts\n",
    "\n",
    "print('Duplicate Procedures (Expect True)', 'Unnamed: 0' not in all_procedures.columns, len(all_procedures) == len(all_procedures[['person_id','rolled_concept_name', 'procedure_concept_id', 'procedure_date', 'procedure_occurrence_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299dcb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Labs (Expect True) True True\n"
     ]
    }
   ],
   "source": [
    "all_labs['rolled_concept_name'] = all_labs['concept_name'].astype(str) + '_lab'\n",
    "all_labs = all_labs[['person_id', 'measurement_concept_id', 'measurement_date', 'measurement_id', 'rolled_concept_name', 'censor_date']].drop_duplicates()\n",
    "print('Duplicate Labs (Expect True)', 'Unnamed: 0' not in all_labs.columns, len(all_labs) == len(all_labs[['person_id', 'measurement_concept_id', 'measurement_date', 'measurement_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7151c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds['start_date'] = all_conds['condition_start_date'].copy()\n",
    "all_procedures['start_date'] = all_procedures['procedure_date'].copy()\n",
    "all_labs['start_date'] = all_labs['measurement_date'].copy()\n",
    "\n",
    "all_cond_lab_pro = pd.concat([all_conds, all_procedures, all_labs])\n",
    "all_cond_lab_pro = all_cond_lab_pro[['person_id', 'rolled_concept_name','cohort_start_date', 'start_date', 'censor_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9ee2d",
   "metadata": {},
   "source": [
    "### Only keep columns that we had in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3dd275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138917603 39703694\n",
      "133881232 39654646\n"
     ]
    }
   ],
   "source": [
    "print(len(all_cond_lab_pro), len(all_meds))\n",
    "all_cond_lab_pro = drop_unshared_features(all_cond_lab_pro, 'rolled_concept_name', data_columns)\n",
    "all_meds = drop_unshared_features(all_meds, 'rolled_concept_name', data_columns)\n",
    "print(len(all_cond_lab_pro), len(all_meds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2897e",
   "metadata": {},
   "source": [
    "### Check for Data Leakage: \n",
    "Minimum times should be at least 90 days and cohort start date should be same across all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a89ac569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labs, conditions, procedures: 90.0 5478.0\n",
      "Meds (Start of prescription): 90 5478\n",
      "Meds (End of prescription): 90 5478\n",
      "Visits (Start of visit): 90 5478\n",
      "Visits (End of visit): 90 5478\n"
     ]
    }
   ],
   "source": [
    "check = (all_cond_lab_pro['cohort_start_date']-all_cond_lab_pro['start_date']).dt.days\n",
    "print('Labs, conditions, procedures:', check.min(), check.max())\n",
    "\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days\n",
    "print('Meds (Start of prescription):', check.min(), check.max())\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days\n",
    "print('Meds (End of prescription):', check.min(), check.max())\n",
    "\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days\n",
    "print('Visits (Start of visit):', check.min(), check.max())\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days\n",
    "print('Visits (End of visit):', check.min(), check.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e13e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of places where cohort start date doesnt align: 0\n",
      "Number of places where censor date doesnt align: 0\n"
     ]
    }
   ],
   "source": [
    "check_cohort_start = df_pop[['person_id','cohort_start_date']]\n",
    "check_cohort_start = check_cohort_start.merge(all_cond_lab_pro[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_cohort_start = check_cohort_start.merge(all_visits[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_cohort_start = check_cohort_start.merge(all_meds[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_cohort_start.set_index('person_id',inplace=True)\n",
    "check_cohort_start = check_cohort_start.T\n",
    "num_unique = check_cohort_start.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where cohort start date doesnt align:',(num_unique>1).sum())\n",
    "\n",
    "check_censor = df_pop[['person_id','censor_date']]\n",
    "check_censor = check_censor.merge(all_cond_lab_pro[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_censor = check_censor.merge(all_visits[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_censor = check_censor.merge(all_meds[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_censor.set_index('person_id',inplace=True)\n",
    "check_censor = check_censor.T\n",
    "num_unique = check_censor.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where censor date doesnt align:',(num_unique>1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba5b3",
   "metadata": {},
   "source": [
    "### \"Psychiatric\" visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1343aac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1742113/468789484.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mh_visits['visit_concept_id'] = df_mh_visits['visit_concept_id'].astype(str) + '_MH'\n"
     ]
    }
   ],
   "source": [
    "mental_health_conds = pd.read_csv(f'{codes_path}/mental_disorder_descendants.csv')\n",
    "mh_visits = all_conds.loc[all_conds['condition_concept_id'].isin(mental_health_conds['descendant_concept_id']), 'visit_occurrence_id']\n",
    "df_mh_visits = all_visits.loc[all_visits['visit_occurrence_id'].isin(mh_visits)]\n",
    "\n",
    "all_visits['visit_concept_id'] = all_visits['visit_concept_id'].astype(str) + '_ALL'\n",
    "df_mh_visits['visit_concept_id'] = df_mh_visits['visit_concept_id'].astype(str) + '_MH'\n",
    "all_visits = pd.concat([all_visits, df_mh_visits], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ff0a79-a1eb-4525-9f71-cf9c2333dd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49134238"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15380d59",
   "metadata": {},
   "source": [
    "# Start Processing Timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7af4734-7270-44fa-bfd9-5ed0581758d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_start_date</th>\n",
       "      <th>timedelta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal mass_conds</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal mass_conds</td>\n",
       "      <td>2017-09-27</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal organ finding_conds</td>\n",
       "      <td>2013-12-18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal organ finding_conds</td>\n",
       "      <td>2014-01-08</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal organ finding_conds</td>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal organ finding_conds</td>\n",
       "      <td>2015-04-04</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal organ finding_conds</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>818.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Abdominal organ finding_conds</td>\n",
       "      <td>2017-09-27</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613229</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Active or passive immunization_procedure</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613230</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Active or passive immunization_procedure</td>\n",
       "      <td>2016-05-11</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Acute respiratory disease_conds</td>\n",
       "      <td>2015-04-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Adjustment disorder with disturbance of conduc...</td>\n",
       "      <td>2011-09-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Allergic disorder of respiratory system_conds</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Anxiety_conds</td>\n",
       "      <td>2011-03-28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Anxiety_conds</td>\n",
       "      <td>2011-07-19</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613468</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Application of short arm cast_procedure</td>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613483</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Application of short arm splint, forearm to ha...</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614184</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Application of short arm splint, forearm to ha...</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>1001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614203</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Application of short arm splint, forearm to ha...</td>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614164</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Application of short leg cast_procedure</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2008-11-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2016-06-03</td>\n",
       "      <td>2752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2016-06-09</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2016-07-07</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2017-02-09</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2019-04-03</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Arm injury_conds</td>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Asthma_conds</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Asthma_conds</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Asthma_conds</td>\n",
       "      <td>2021-10-14</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Asthma_conds</td>\n",
       "      <td>2021-11-15</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243480</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Bacterial culture, urine, with colony count_lab</td>\n",
       "      <td>2009-08-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243502</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Bacterial culture, urine, with colony count_lab</td>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243537</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Bacterial culture, urine, with colony count_lab</td>\n",
       "      <td>2013-12-18</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243547</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Bacterial culture, urine, with colony count_lab</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243538</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Bacterial culture, urine, with organism identi...</td>\n",
       "      <td>2013-12-18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243495</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Basic metabolic panel (Calcium, total) This pa...</td>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614217</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Behavioral health screening to determine eligi...</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243473</th>\n",
       "      <td>2.000000e+10</td>\n",
       "      <td>Blood cell count, automated_lab</td>\n",
       "      <td>2008-01-31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           person_id                                       feature_name  \\\n",
       "1029    2.000000e+10                               Abdominal mass_conds   \n",
       "1051    2.000000e+10                               Abdominal mass_conds   \n",
       "551     2.000000e+10                      Abdominal organ finding_conds   \n",
       "567     2.000000e+10                      Abdominal organ finding_conds   \n",
       "570     2.000000e+10                      Abdominal organ finding_conds   \n",
       "703     2.000000e+10                      Abdominal organ finding_conds   \n",
       "1031    2.000000e+10                      Abdominal organ finding_conds   \n",
       "1053    2.000000e+10                      Abdominal organ finding_conds   \n",
       "613229  2.000000e+10           Active or passive immunization_procedure   \n",
       "613230  2.000000e+10           Active or passive immunization_procedure   \n",
       "700     2.000000e+10                    Acute respiratory disease_conds   \n",
       "240     2.000000e+10  Adjustment disorder with disturbance of conduc...   \n",
       "2547    2.000000e+10      Allergic disorder of respiratory system_conds   \n",
       "167     2.000000e+10                                      Anxiety_conds   \n",
       "218     2.000000e+10                                      Anxiety_conds   \n",
       "613468  2.000000e+10            Application of short arm cast_procedure   \n",
       "613483  2.000000e+10  Application of short arm splint, forearm to ha...   \n",
       "614184  2.000000e+10  Application of short arm splint, forearm to ha...   \n",
       "614203  2.000000e+10  Application of short arm splint, forearm to ha...   \n",
       "614164  2.000000e+10            Application of short leg cast_procedure   \n",
       "70      2.000000e+10                                   Arm injury_conds   \n",
       "840     2.000000e+10                                   Arm injury_conds   \n",
       "870     2.000000e+10                                   Arm injury_conds   \n",
       "887     2.000000e+10                                   Arm injury_conds   \n",
       "895     2.000000e+10                                   Arm injury_conds   \n",
       "915     2.000000e+10                                   Arm injury_conds   \n",
       "988     2.000000e+10                                   Arm injury_conds   \n",
       "1077    2.000000e+10                                   Arm injury_conds   \n",
       "1093    2.000000e+10                                   Arm injury_conds   \n",
       "1112    2.000000e+10                                   Arm injury_conds   \n",
       "1279    2.000000e+10                                   Arm injury_conds   \n",
       "1290    2.000000e+10                                   Arm injury_conds   \n",
       "2153    2.000000e+10                                   Arm injury_conds   \n",
       "2161    2.000000e+10                                   Arm injury_conds   \n",
       "2335    2.000000e+10                                   Arm injury_conds   \n",
       "2357    2.000000e+10                                   Arm injury_conds   \n",
       "2371    2.000000e+10                                   Arm injury_conds   \n",
       "2387    2.000000e+10                                   Arm injury_conds   \n",
       "1274    2.000000e+10                                       Asthma_conds   \n",
       "2556    2.000000e+10                                       Asthma_conds   \n",
       "2566    2.000000e+10                                       Asthma_conds   \n",
       "2575    2.000000e+10                                       Asthma_conds   \n",
       "243480  2.000000e+10    Bacterial culture, urine, with colony count_lab   \n",
       "243502  2.000000e+10    Bacterial culture, urine, with colony count_lab   \n",
       "243537  2.000000e+10    Bacterial culture, urine, with colony count_lab   \n",
       "243547  2.000000e+10    Bacterial culture, urine, with colony count_lab   \n",
       "243538  2.000000e+10  Bacterial culture, urine, with organism identi...   \n",
       "243495  2.000000e+10  Basic metabolic panel (Calcium, total) This pa...   \n",
       "614217  2.000000e+10  Behavioral health screening to determine eligi...   \n",
       "243473  2.000000e+10                    Blood cell count, automated_lab   \n",
       "\n",
       "       feature_start_date  timedelta  \n",
       "1029           2017-06-30        NaN  \n",
       "1051           2017-09-27       89.0  \n",
       "551            2013-12-18        NaN  \n",
       "567            2014-01-08       21.0  \n",
       "570            2014-01-09        1.0  \n",
       "703            2015-04-04      450.0  \n",
       "1031           2017-06-30      818.0  \n",
       "1053           2017-09-27       89.0  \n",
       "613229         2016-02-29        NaN  \n",
       "613230         2016-05-11       72.0  \n",
       "700            2015-04-04        NaN  \n",
       "240            2011-09-20        NaN  \n",
       "2547           2021-08-31        NaN  \n",
       "167            2011-03-28        NaN  \n",
       "218            2011-07-19      113.0  \n",
       "613468         2016-06-16        NaN  \n",
       "613483         2017-10-16        NaN  \n",
       "614184         2020-07-13     1001.0  \n",
       "614203         2020-11-16      126.0  \n",
       "614164         2019-12-06        NaN  \n",
       "70             2008-11-20        NaN  \n",
       "840            2016-06-03     2752.0  \n",
       "870            2016-06-09        6.0  \n",
       "887            2016-06-16        7.0  \n",
       "895            2016-07-07       21.0  \n",
       "915            2016-09-26       81.0  \n",
       "988            2017-02-09      136.0  \n",
       "1077           2017-10-16      249.0  \n",
       "1093           2017-10-20        4.0  \n",
       "1112           2017-11-17       28.0  \n",
       "1279           2019-03-31      499.0  \n",
       "1290           2019-04-03        3.0  \n",
       "2153           2019-04-15       12.0  \n",
       "2161           2019-07-19       95.0  \n",
       "2335           2020-07-13      360.0  \n",
       "2357           2020-07-28       15.0  \n",
       "2371           2020-08-18       21.0  \n",
       "2387           2020-09-03       16.0  \n",
       "1274           2019-03-13        NaN  \n",
       "2556           2021-08-31      902.0  \n",
       "2566           2021-10-14       44.0  \n",
       "2575           2021-11-15       32.0  \n",
       "243480         2009-08-12        NaN  \n",
       "243502         2013-02-01     1269.0  \n",
       "243537         2013-12-18      320.0  \n",
       "243547         2015-10-29      680.0  \n",
       "243538         2013-12-18        NaN  \n",
       "243495         2013-01-29        NaN  \n",
       "614217         2021-11-30        NaN  \n",
       "243473         2008-01-31        NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cond_lab_pro = all_cond_lab_pro[['person_id', 'rolled_concept_name', 'start_date']].drop_duplicates()\n",
    "all_cond_lab_pro = all_cond_lab_pro.sort_values(['person_id', 'rolled_concept_name', 'start_date'])\n",
    "all_cond_lab_pro['timedelta'] = all_cond_lab_pro.groupby(['person_id', 'rolled_concept_name'])['start_date'].diff().dt.days\n",
    "all_cond_lab_pro.rename({'rolled_concept_name':'feature_name', 'start_date':'feature_start_date'}, axis=1, inplace=True)\n",
    "list_unique_cpl = list(all_cond_lab_pro['feature_name'].unique())\n",
    "\n",
    "all_cond_lab_pro.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d82d5a-d742-418e-9a83-81e2e2673154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_start_date</th>\n",
       "      <th>feature_end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>2008-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2009-02-06</td>\n",
       "      <td>2009-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2009-03-18</td>\n",
       "      <td>2009-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2009-03-23</td>\n",
       "      <td>2009-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2009-05-09</td>\n",
       "      <td>2009-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2009-08-31</td>\n",
       "      <td>2009-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>2009-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>2010-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2010-03-04</td>\n",
       "      <td>2010-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>2010-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2010-04-21</td>\n",
       "      <td>2010-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2010-04-29</td>\n",
       "      <td>2010-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2010-08-09</td>\n",
       "      <td>2010-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2010-08-27</td>\n",
       "      <td>2010-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2010-08-31</td>\n",
       "      <td>2010-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2010-09-13</td>\n",
       "      <td>2010-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2010-09-20</td>\n",
       "      <td>2010-09-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2010-11-24</td>\n",
       "      <td>2010-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2011-01-28</td>\n",
       "      <td>2011-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>2011-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>2011-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2011-03-02</td>\n",
       "      <td>2011-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2011-03-18</td>\n",
       "      <td>2011-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2011-08-05</td>\n",
       "      <td>2011-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2011-09-03</td>\n",
       "      <td>2011-09-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2011-11-07</td>\n",
       "      <td>2011-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2011-11-30</td>\n",
       "      <td>2011-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>2012-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>9202_ALL</td>\n",
       "      <td>2012-01-27</td>\n",
       "      <td>2012-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.000001e+10</td>\n",
       "      <td>581458_ALL</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       person_id feature_name feature_start_date feature_end_date\n",
       "0   2.000001e+10   581458_ALL         2008-12-03       2008-12-03\n",
       "1   2.000001e+10     9202_ALL         2009-02-06       2009-02-06\n",
       "2   2.000001e+10     9202_ALL         2009-03-18       2009-03-18\n",
       "3   2.000001e+10     9202_ALL         2009-03-23       2009-03-23\n",
       "4   2.000001e+10     9202_ALL         2009-05-09       2009-05-09\n",
       "5   2.000001e+10     9202_ALL         2009-08-31       2009-08-31\n",
       "6   2.000001e+10     9202_ALL         2009-09-16       2009-09-16\n",
       "7   2.000001e+10     9202_ALL         2010-01-06       2010-01-06\n",
       "8   2.000001e+10   581458_ALL         2010-03-04       2010-03-04\n",
       "9   2.000001e+10     9202_ALL         2010-03-26       2010-03-26\n",
       "10  2.000001e+10     9202_ALL         2010-04-21       2010-04-21\n",
       "11  2.000001e+10   581458_ALL         2010-04-29       2010-04-29\n",
       "12  2.000001e+10   581458_ALL         2010-08-09       2010-08-09\n",
       "13  2.000001e+10     9202_ALL         2010-08-27       2010-08-27\n",
       "14  2.000001e+10   581458_ALL         2010-08-31       2010-08-31\n",
       "15  2.000001e+10   581458_ALL         2010-09-13       2010-09-13\n",
       "16  2.000001e+10   581458_ALL         2010-09-20       2010-09-20\n",
       "17  2.000001e+10     9202_ALL         2010-11-24       2010-11-24\n",
       "18  2.000001e+10     9202_ALL         2011-01-28       2011-01-28\n",
       "19  2.000001e+10   581458_ALL         2011-02-02       2011-02-02\n",
       "20  2.000001e+10   581458_ALL         2011-03-01       2011-03-01\n",
       "21  2.000001e+10     9202_ALL         2011-03-02       2011-03-02\n",
       "22  2.000001e+10     9202_ALL         2011-03-18       2011-03-18\n",
       "23  2.000001e+10     9202_ALL         2011-08-05       2011-08-05\n",
       "24  2.000001e+10   581458_ALL         2011-09-03       2011-09-03\n",
       "25  2.000001e+10     9202_ALL         2011-11-07       2011-11-07\n",
       "26  2.000001e+10     9202_ALL         2011-11-30       2011-11-30\n",
       "27  2.000001e+10     9202_ALL         2012-01-12       2012-01-12\n",
       "28  2.000001e+10     9202_ALL         2012-01-27       2012-01-27\n",
       "29  2.000001e+10   581458_ALL         2012-01-28       2012-01-28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_visits = all_visits[['person_id', 'visit_concept_id', 'visit_start_date', 'visit_end_date']].drop_duplicates()\n",
    "all_visits.rename({'visit_concept_id':'feature_name', 'visit_start_date':'feature_start_date', 'visit_end_date':'feature_end_date'}, axis=1, inplace=True)\n",
    "\n",
    "all_meds = all_meds[['person_id', 'rolled_concept_name', 'drug_era_start_date', 'drug_era_end_date']].drop_duplicates()\n",
    "all_meds.rename({'rolled_concept_name':'feature_name', 'drug_era_start_date':'feature_start_date', 'drug_era_end_date':'feature_end_date'}, axis=1, inplace=True)\n",
    "\n",
    "visit_meds = pd.concat([all_visits, all_meds])\n",
    "list_unique_vm = list(visit_meds['feature_name'].unique())\n",
    "\n",
    "visit_meds.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965c2bc",
   "metadata": {},
   "source": [
    "### Create the \"iterative\" population dataframe: \n",
    "- Start at the date of initial psychosis diagnosis, then go every XX days (120 days), cutting off at the censor date (if you go over the censor date, chop to the censor date).\n",
    "- Practically, this means that start date 1 is first visit & end date 1 is psychosis; then start date 2 is psychosis dx date and end date 2 is psychosis dx + 90... \n",
    "- Then, starting at the date of psychosis, go back in XX-day increments for 3 years (9 iterations). The earliest iteration (furthest away from psychosis date) should consist of all prior data, and if a person has less than 3 years of data pre-psychosis, they should have fewer early visits.\n",
    "- Note that start dates are inclusive and end dates are exclusive\n",
    "- **KEEP IN MIND FOR LATER: WE WANT TO PAD AT THE BEGINNING, NOT AT THE END. So then we move each person to be aligned at the end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8745fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102739\n",
      "102739\n",
      "0.5115778818170315\n"
     ]
    }
   ],
   "source": [
    "df_pop = df_pop[['person_id', 'first_visit', 'cohort_start_date', 'psychosis_diagnosis_date', 'censor_date']]\n",
    "\n",
    "df_pop['0_end'] = df_pop['psychosis_diagnosis_date']\n",
    "df_pop['0_start'] = df_pop['psychosis_diagnosis_date'] - pd.Timedelta(days_per_iter, 'days')\n",
    "print(len(df_pop))\n",
    "df_pop.loc[df_pop['0_end']>df_pop['censor_date'], '0_end'] = df_pop.loc[df_pop['0_end']>df_pop['censor_date'], 'censor_date']\n",
    "print(len(df_pop))\n",
    "\n",
    "# after the loops, remove people for whom 0_start-0_end > 0\n",
    "\n",
    "# FORWARD LOOP: starting at psychosis dx, every XX days till censor date\n",
    "for count in range(1, forward_iterations+1): \n",
    "    # get the start date as the same day as prev end date and the end date as start + 120 days\n",
    "    df_pop[str(count)+'_start'] = df_pop[str(count-1)+'_end']\n",
    "    df_pop[str(count)+'_end'] = df_pop[str(count)+'_start'] + pd.Timedelta(days_per_iter, 'days')\n",
    "    \n",
    "    # update start/end dates to make sure it is at max, the censor date\n",
    "    df_pop.loc[df_pop[str(count)+'_start'] > df_pop['censor_date'], str(count)+'_start'] = df_pop.loc[df_pop[str(count)+'_start'] > df_pop['censor_date'], 'censor_date']\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] > df_pop['censor_date'], str(count)+'_end'] = df_pop.loc[df_pop[str(count)+'_end'] > df_pop['censor_date'], 'censor_date']\n",
    "    \n",
    "    # if start date == censor date: set start and end to NaT\n",
    "    df_pop.loc[df_pop[str(count)+'_start'] == df_pop['censor_date'], [str(count)+'_start', str(count)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')]\n",
    "\n",
    "for count in np.arange(-1, -1*backwards_iterations, -1):\n",
    "    df_pop[str(count)+'_end'] = df_pop[str(count+1)+'_start']\n",
    "    df_pop[str(count)+'_start'] = df_pop[str(count)+'_end']-pd.Timedelta(days_per_iter, 'days')\n",
    "    \n",
    "    # if the visit starts or ends prior to first_visit, set start/end to first_visit\n",
    "    df_pop.loc[df_pop[str(count)+'_start']<df_pop['first_visit'], str(count)+'_start'] = df_pop.loc[df_pop[str(count)+'_start']<df_pop['first_visit'], 'first_visit']\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] < df_pop['first_visit'], str(count)+'_end'] = df_pop.loc[df_pop[str(count)+'_end'] < df_pop['first_visit'], 'first_visit']\n",
    "    \n",
    "    # if end date == first visit date: set start and end to NaT\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] == df_pop['first_visit'], [str(count)+'_start', str(count)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')]\n",
    "    \n",
    "    df_pop[str(count)+'_end'] = pd.to_datetime(df_pop[str(count)+'_end'], format = '%Y-%m-%d')\n",
    "\n",
    "# set the last backwards iteration start date to be the first visit ONLY if the date is not nan\n",
    "df_pop.loc[~(df_pop[f'{str(-1*backwards_iterations+1)}_start'].isna()), f'{str(-1*backwards_iterations+1)}_start'] = df_pop.loc[~(df_pop[f'{str(-1*backwards_iterations+1)}_start'].isna()), 'first_visit']\n",
    "print(df_pop[f'{str(-1*backwards_iterations+1)}_start'].isna().sum()/len(df_pop)) # check that this is ~50% or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6bdb403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "remove_cols = []\n",
    "for col in df_pop.columns: \n",
    "    if df_pop[col].isna().sum() == len(df_pop):\n",
    "        remove_cols.append(col)\n",
    "print(remove_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c57181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all \"timesteps\" are at least 1 day\n",
    "# also make sure none of them start/end after the censor date\n",
    "for i in np.arange(-1*backwards_iterations+1, forward_iterations):\n",
    "    df_pop.loc[df_pop[str(i)+'_end']==df_pop[str(i)+'_start'], [str(i)+'_start', str(i)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')] \n",
    "df_pop.drop(remove_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594569",
   "metadata": {},
   "source": [
    "### Loop through the iterative population dataframe to get the features for each person in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01537cee-81ae-4559-8475-728932a1334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15, -16, -17, -18]\n",
      "102739 102739 102739\n"
     ]
    }
   ],
   "source": [
    "iter_cols = [col for col in list(df_pop.columns) if '_end' in col]\n",
    "iter_cols = [int(i.split('_')[0]) for i in iter_cols]\n",
    "print(iter_cols)\n",
    "print(len(visit_meds['person_id'].unique()), len(df_pop), len(all_cond_lab_pro['person_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635978c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18\n",
      "-17\n",
      "-16\n",
      "-15\n",
      "-14\n",
      "-12\n",
      "-11\n",
      "-10\n",
      "-9\n",
      "-8\n",
      "-7\n",
      "-6\n",
      "-5\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "list_timedeltas = []\n",
    "for iteration in np.arange(np.min(iter_cols), np.max(iter_cols), 1): \n",
    "    temp_df_iter_pop = df_pop.copy()\n",
    "    temp_df_iter_pop['iter_start_date'] = temp_df_iter_pop[str(iteration)+'_start']\n",
    "    temp_df_iter_pop['iter_end_date'] = temp_df_iter_pop[str(iteration)+'_end']\n",
    "\n",
    "    # constrict to people with a valid iteration\n",
    "    temp_df_iter_pop = temp_df_iter_pop.loc[~(temp_df_iter_pop['iter_start_date'].isna())]\n",
    "    temp_df_iter_pop['years_obs'] = (temp_df_iter_pop['iter_end_date']-temp_df_iter_pop['iter_start_date']).dt.days/365\n",
    "\n",
    "    temp_df_iter_pop['iteration'] = iteration\n",
    "    all_rows = temp_df_iter_pop[['person_id', 'iteration']]\n",
    "    \n",
    "    # CONDITIONS LABS PROCEDURES\n",
    "    # for conditions, labs, procedures, just compare the start_date to the cutoff date\n",
    "    within_iter_cond_pro_labs = all_cond_lab_pro.loc[all_cond_lab_pro['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    within_iter_cond_pro_labs = within_iter_cond_pro_labs.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    within_iter_cond_pro_labs = within_iter_cond_pro_labs.loc[within_iter_cond_pro_labs['feature_start_date']>= within_iter_cond_pro_labs['iter_start_date']]\n",
    "    within_iter_cond_pro_labs = within_iter_cond_pro_labs.loc[within_iter_cond_pro_labs['feature_start_date']< within_iter_cond_pro_labs['iter_end_date']]\n",
    "\n",
    "    pre_iter_cond_pro_labs = all_cond_lab_pro.loc[all_cond_lab_pro['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    pre_iter_cond_pro_labs = pre_iter_cond_pro_labs.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    pre_iter_cond_pro_labs = pre_iter_cond_pro_labs.loc[pre_iter_cond_pro_labs['feature_start_date']< pre_iter_cond_pro_labs['iter_start_date']]\n",
    "\n",
    "    if len(within_iter_cond_pro_labs.loc[within_iter_cond_pro_labs['feature_start_date']>=within_iter_cond_pro_labs['iter_end_date']])+len(within_iter_cond_pro_labs.loc[within_iter_cond_pro_labs['feature_start_date']<within_iter_cond_pro_labs['iter_start_date']]) > 0:\n",
    "        print('Leakage in conds/labs/procedures')\n",
    "    if len(pre_iter_cond_pro_labs.loc[pre_iter_cond_pro_labs['feature_start_date']>pre_iter_cond_pro_labs['iter_start_date']]) > 0:\n",
    "        print('Leakage in conds/labs/procedures')\n",
    "\n",
    "    # Sort to ensure order then get the first occurrence\n",
    "    combinations = list(product(list(temp_df_iter_pop['person_id'].unique()), list_unique_cpl))\n",
    "    full_pt_feature_df = pd.DataFrame(columns = ['person_id', 'feature_name'], data = combinations)\n",
    "\n",
    "    # IF SOMETHING EXISTS IN THE ITERATION: GET IT\n",
    "    within_iter_cond_pro_labs = within_iter_cond_pro_labs.sort_values(['person_id', 'feature_name', 'feature_start_date'])\n",
    "    within_iter_cond_pro_labs = within_iter_cond_pro_labs.groupby(['person_id', 'feature_name']).first().reset_index()\n",
    "    full_pt_feature_df = full_pt_feature_df.merge(within_iter_cond_pro_labs[['person_id', 'feature_name', 'feature_start_date']], how='outer', on=['person_id', 'feature_name'])\n",
    "    # ELSE put in the last date in the timestep\n",
    "    full_pt_feature_df = full_pt_feature_df.merge(temp_df_iter_pop[['person_id', 'iter_end_date']], how='outer', on='person_id')\n",
    "    full_pt_feature_df.loc[full_pt_feature_df['feature_start_date'].isna(), 'feature_start_date'] = full_pt_feature_df.loc[full_pt_feature_df['feature_start_date'].isna(), 'iter_end_date']\n",
    "\n",
    "    # Next get the most recent occurrence of a pre-timestep value\n",
    "    pre_iter_cond_pro_labs = pre_iter_cond_pro_labs.groupby(['person_id', 'feature_name']).max()['feature_start_date'].reset_index()\n",
    "    pre_iter_cond_pro_labs.rename({'feature_start_date':'pre_iter_feature_date'}, axis=1, inplace=True)\n",
    "    full_pt_feature_df = full_pt_feature_df.merge(pre_iter_cond_pro_labs, how='inner', on = ['person_id', 'feature_name'])\n",
    "    full_pt_feature_df['time_since_last_feat'] = (full_pt_feature_df['feature_start_date']-full_pt_feature_df['pre_iter_feature_date']).dt.days\n",
    "\n",
    "    # Pivot the table to get person_id as rows and feature_name as columns with timedelta as values\n",
    "    pivot_temp_cond_pro_labs = full_pt_feature_df.pivot(index='person_id', columns='feature_name', values='time_since_last_feat')\n",
    "    pivot_temp_cond_pro_labs.columns.name = None  # Remove the pivot column name\n",
    "    pivot_temp_cond_pro_labs = pivot_temp_cond_pro_labs.reset_index()  # Reset index to bring person_id back as a column\n",
    "    pivot_temp_cond_pro_labs.fillna(0, inplace=True)\n",
    "    \n",
    "    # VISITS AND MEDICATIONS: \n",
    "    \"\"\"create a list of dfs that has patient, iteration and \n",
    "    per feature first and last engagement (first visit start date and \n",
    "    last visit end date per feature)\n",
    "    \"\"\"\n",
    "    # note: for meds and visits, replace iter_end_date with equal_end_date, which is the \n",
    "    # day before the actual end date since is the last day that we are allowing the visit to \"equal\"\n",
    "    # ie visit < iter_end_date but visit <= equal_end_date\n",
    "    \n",
    "    \n",
    "    #for medications and visits, we want to look at \n",
    "    #1. med start date needs to be before iteration end date\n",
    "    #2. med end date needs to be on or after iteration start date\n",
    "    \n",
    "    #3. if med start date is before iteration start date -- make med start date iteration start date\n",
    "    #4. if med end date is after iteration end date -- make med end date iteration end date\n",
    "\n",
    "    # get all the minimum visit/med start dates\n",
    "    temp_visit_meds = visit_meds.loc[visit_meds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_visit_meds = temp_visit_meds.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_visit_meds['equal_end_date'] = temp_visit_meds['iter_end_date']-pd.Timedelta(1, 'days')\n",
    "\n",
    "    temp_visit_meds = temp_visit_meds.loc[(temp_visit_meds['feature_start_date']<temp_visit_meds['iter_end_date'])&(temp_visit_meds['feature_end_date']>=temp_visit_meds['iter_start_date'])]\n",
    "    temp_visit_meds.loc[temp_visit_meds['feature_start_date']<temp_visit_meds['iter_start_date'], 'feature_start_date'] = temp_visit_meds.loc[temp_visit_meds['feature_start_date']<temp_visit_meds['iter_start_date'], 'iter_start_date']\n",
    "    temp_visit_meds.loc[temp_visit_meds['feature_end_date']>temp_visit_meds['equal_end_date'], 'feature_end_date'] = temp_visit_meds.loc[temp_visit_meds['feature_end_date']>temp_visit_meds['equal_end_date'], 'equal_end_date']\n",
    "\n",
    "    if len(temp_visit_meds.loc[temp_visit_meds['feature_start_date']>temp_visit_meds['iter_end_date']])+len(temp_visit_meds.loc[temp_visit_meds['feature_end_date']>temp_visit_meds['iter_end_date']]) > 0:\n",
    "        print('Leakage in visit/med ends')\n",
    "    if len(temp_visit_meds.loc[temp_visit_meds['feature_end_date']<temp_visit_meds['iter_start_date']])+len(temp_visit_meds.loc[temp_visit_meds['feature_start_date']<temp_visit_meds['iter_start_date']]) > 0:\n",
    "        print('Leakage in visit/med starts')\n",
    "\n",
    "    temp_visit_meds = temp_visit_meds.groupby(['person_id', 'feature_name']).agg({'feature_start_date':'min', 'feature_end_date':'max'}).reset_index()\n",
    "\n",
    "    combinations = list(product(list(temp_df_iter_pop['person_id'].unique()), list_unique_vm))\n",
    "    full_pt_vm_df = pd.DataFrame(columns = ['person_id', 'feature_name'], data = combinations)\n",
    "    temp_visit_meds['feature_name'] = temp_visit_meds['feature_name'].astype('object')\n",
    "    full_pt_vm_df = full_pt_vm_df.merge(temp_visit_meds[['person_id', 'feature_name', 'feature_start_date']], how='outer', on=['person_id', 'feature_name'])\n",
    "    # ELSE put in the last date in the timestep\n",
    "    full_pt_vm_df = full_pt_vm_df.merge(temp_df_iter_pop[['person_id', 'iter_end_date']], how='outer', on='person_id')\n",
    "    full_pt_vm_df.loc[full_pt_vm_df['feature_start_date'].isna(), 'feature_start_date'] = full_pt_vm_df.loc[full_pt_vm_df['feature_start_date'].isna(), 'iter_end_date']\n",
    "\n",
    "    # get all the pre-iteration visit/med start dates\n",
    "    pre_visit_meds = visit_meds.loc[visit_meds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    pre_visit_meds = pre_visit_meds.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    pre_visit_meds['equal_end_date'] = pre_visit_meds['iter_start_date']-pd.Timedelta(1, 'days')\n",
    "\n",
    "    pre_visit_meds = pre_visit_meds.loc[(pre_visit_meds['feature_start_date']<pre_visit_meds['iter_start_date'])]\n",
    "    pre_visit_meds.loc[pre_visit_meds['feature_end_date']>pre_visit_meds['equal_end_date'], 'feature_end_date'] = pre_visit_meds.loc[pre_visit_meds['feature_end_date']>pre_visit_meds['equal_end_date'], 'equal_end_date']\n",
    "\n",
    "    if len(pre_visit_meds.loc[pre_visit_meds['feature_start_date']>pre_visit_meds['iter_start_date']])+len(pre_visit_meds.loc[pre_visit_meds['feature_end_date']>pre_visit_meds['iter_start_date']]) > 0:\n",
    "        print('Leakage in visit/med pre-iter')\n",
    "\n",
    "    pre_visit_meds = pre_visit_meds.groupby(['person_id', 'feature_name']).agg({'feature_end_date':'max'}).reset_index()\n",
    "    pre_visit_meds['feature_name'] = pre_visit_meds['feature_name'].astype('object')\n",
    "    full_pt_vm_df = full_pt_vm_df.merge(pre_visit_meds, how='inner', on=['person_id', 'feature_name'])\n",
    "\n",
    "    full_pt_vm_df['time_since_last_feat'] = (full_pt_vm_df['feature_start_date']-full_pt_vm_df['feature_end_date']).dt.days\n",
    "\n",
    "    # Pivot the table to get person_id as rows and feature_name as columns with timedelta as values\n",
    "    pivot_temp_meds_visits = full_pt_vm_df.pivot(index='person_id', columns='feature_name', values='time_since_last_feat')\n",
    "    pivot_temp_meds_visits.columns.name = None  # Remove the pivot column name\n",
    "    pivot_temp_meds_visits = pivot_temp_meds_visits.reset_index()  # Reset index to bring person_id back as a column\n",
    "    pivot_temp_meds_visits.fillna(0, inplace=True)\n",
    "\n",
    "    full_pivot_table = pivot_temp_cond_pro_labs.merge(pivot_temp_meds_visits, how='outer', on='person_id')\n",
    "    full_pivot_table['iteration'] = iteration\n",
    "    print(iteration)\n",
    "        \n",
    "    full_pivot_table = full_pivot_table.merge(all_rows, how = 'right', on = ['person_id', 'iteration'])\n",
    "    full_pivot_table.fillna(0, inplace=True)\n",
    "\n",
    "    if len(full_pivot_table) != len(temp_df_iter_pop):\n",
    "        print('Mismatched patient size in iteration', iteration)\n",
    "    \n",
    "    list_timedeltas.append(full_pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db8109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timedeltas = pd.concat(list_timedeltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5296bf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_timedeltas\n",
    "del all_visits\n",
    "del all_conds\n",
    "del all_meds\n",
    "del all_cond_lab_pro\n",
    "del visit_meds\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a92379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_set == True:\n",
    "    missing_cols = list(set(list_mdcd_cols).difference(all_timedeltas.columns))\n",
    "    all_timedeltas.loc[:,missing_cols] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "796e2f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_timedeltas.fillna(0, inplace=True)\n",
    "print(all_timedeltas.isna().sum().sum())\n",
    "print(len(all_timedeltas.loc[all_timedeltas['person_id'].isna()]))\n",
    "print(len(all_timedeltas.loc[all_timedeltas['person_id']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d5d69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.24657534 0.49315068 0.73972603 0.98630137 1.23287671\n",
      " 1.47945205 1.7260274  1.97260274 2.21917808 2.46575342 2.71232877\n",
      " 2.95890411 3.20547945]\n",
      "\n",
      "\n",
      "Pre-psychosis tsp [0.]\n",
      "\n",
      "\n",
      "Post-psychosis tsp [0.24657534 0.49315068 0.73972603 0.98630137 1.23287671 1.47945205\n",
      " 1.7260274  1.97260274 2.21917808 2.46575342 2.71232877 2.95890411\n",
      " 3.20547945]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1742113/1289913160.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  all_timedeltas['time_since_psychosis'] = all_timedeltas['iteration'] * days_per_iter/365\n"
     ]
    }
   ],
   "source": [
    "all_timedeltas['time_since_psychosis'] = all_timedeltas['iteration'] * days_per_iter/365\n",
    "all_timedeltas.loc[all_timedeltas['iteration'] <= 0, 'time_since_psychosis'] = 0\n",
    "# check that time_since_psychosis is correct\n",
    "print(all_timedeltas['time_since_psychosis'].unique())\n",
    "print('\\n\\nPre-psychosis tsp', all_timedeltas.loc[all_timedeltas['iteration']<=0, 'time_since_psychosis'].unique())\n",
    "print('\\n\\nPost-psychosis tsp', all_timedeltas.loc[all_timedeltas['iteration']>0, 'time_since_psychosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28a2e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2375123, 1567)\n"
     ]
    }
   ],
   "source": [
    "print(all_timedeltas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73ec4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timedeltas.to_csv(f'{int_path}/{dataset_prefix}timedeltas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a458c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549cd7e-56e8-44cc-b6fc-d39ea86dee57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981b042-ee8e-49b1-9d2d-47c0563de710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
