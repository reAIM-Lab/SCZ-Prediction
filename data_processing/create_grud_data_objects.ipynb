{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import sys\n",
    "import gc\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle \n",
    "from joblib import dump, load\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_cols = True\n",
    "make_scaler = False\n",
    "\n",
    "bw_only = False\n",
    "bm_only = False\n",
    "wm_only = False\n",
    "mf_only = False\n",
    "\n",
    "demo_aware = True\n",
    "keep_iters = None # [-10, 31]\n",
    "\n",
    "\"paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters = pd.read_csv(int_path + 'MDCD_12_1_dl_data_snomed.csv')\n",
    "df_all_iters.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_all_iters = df_all_iters.sort_values(['person_id', 'iteration'])\n",
    "\n",
    "if keep_iters is not None:\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['iteration']>=keep_iters[0]]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['iteration']<=keep_iters[1]]\n",
    "    print('Check keep iters', df_all_iters['iteration'].min(), df_all_iters['iteration'].max())\n",
    "\n",
    "num_days_prediction = 90\n",
    "df_pop = pd.read_csv(raw_path + 'population.csv')\n",
    "df_pop.rename({'psychosis_dx_date':'psychosis_diagnosis_date'}, axis=1, inplace=True)\n",
    "df_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_pop['psychosis_diagnosis_date'], format=\"mixed\", dayfirst = False)\n",
    "df_pop['cohort_start_date'] = pd.to_datetime(df_pop['cohort_start_date'], format=\"mixed\", dayfirst = False)\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]\n",
    "\n",
    "print(len(df_all_iters))\n",
    "if bw_only == True:\n",
    "    df_pop = df_pop.loc[df_pop['race_concept_id'].isin([8516, 8527])]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "    \n",
    "if bm_only == True:\n",
    "    # LIMIT TO non-White patients ONLY\n",
    "    df_pop = df_pop.loc[~(df_pop['race_concept_id'].isin([8527]))]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "\n",
    "if wm_only == True:\n",
    "    # LIMIT TO non-Black patients ONLY\n",
    "    df_pop = df_pop.loc[~(df_pop['race_concept_id'].isin([8516]))]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "    \n",
    "if mf_only == True:\n",
    "    df_pop = df_pop.loc[df_pop['gender_concept_id'].isin([8532, 8507])]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "    \n",
    "print(len(df_all_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_aware == True:\n",
    "    df_pop['is_White'] = 0\n",
    "    df_pop.loc[df_pop['race_concept_id']==8527, 'is_White'] = 1\n",
    "    df_pop['is_Black'] = 0\n",
    "    df_pop.loc[df_pop['race_concept_id']==8516, 'is_Black'] = 1\n",
    "    df_pop['is_Male'] = 0\n",
    "    df_pop.loc[df_pop['gender_concept_id']==8507, 'is_Male'] = 1\n",
    "    \n",
    "    print(len(df_all_iters))\n",
    "    df_all_iters = df_all_iters.merge(df_pop[['person_id', 'is_White', 'is_Black', 'is_Male']], how='inner', left_on = 'person_id', right_on='person_id')\n",
    "    print(len(df_all_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85781061",
   "metadata": {},
   "source": [
    "### Load in timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a12337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in and outer merge for both of them so they are the same length. Merge ON person_id and iteration\n",
    "df_timedeltas = pd.read_csv(int_path + 'MDCD_12_1_grud_timedeltas_snomed.csv')\n",
    "df_timedeltas = df_timedeltas.sort_values(['person_id', 'iteration'])\n",
    "\n",
    "if keep_iters is not None:\n",
    "    df_timedeltas = df_timedeltas.loc[df_timedeltas['iteration']>=keep_iters[0]]\n",
    "    df_timedeltas = df_timedeltas.loc[df_timedeltas['iteration']<=keep_iters[1]]\n",
    "    print('Check keep iters', df_timedeltas['iteration'].min(), df_timedeltas['iteration'].max())\n",
    "\n",
    "print(len(df_timedeltas), len(df_all_iters))\n",
    "df_timedeltas = df_timedeltas.merge(df_all_iters[['person_id','iteration']], how='outer', on=['person_id','iteration'])\n",
    "print(len(df_timedeltas), len(df_all_iters))\n",
    "df_all_iters = df_all_iters.merge(df_timedeltas[['person_id','iteration']], how='outer', on=['person_id','iteration'])\n",
    "print(len(df_timedeltas), len(df_all_iters))\n",
    "\n",
    "df_timedeltas = df_timedeltas.loc[df_timedeltas['person_id'].isin(df_pop['person_id'])]\n",
    "df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "print(len(df_timedeltas), len(df_all_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters.fillna(0, inplace=True)\n",
    "time_per_iter = 120\n",
    "df_all_iters['time_since_psychosis'] = df_all_iters['iteration'] * time_per_iter/365\n",
    "df_all_iters.loc[df_all_iters['iteration'] <= 0, 'time_since_psychosis'] = 0\n",
    "# check that time_since_psychosis is correct\n",
    "print(df_all_iters['time_since_psychosis'].unique())\n",
    "print('\\n\\nPre-psychosis tsp', df_all_iters.loc[df_all_iters['iteration']<=0, 'time_since_psychosis'].unique())\n",
    "print('\\n\\nPost-psychosis tsp', df_all_iters.loc[df_all_iters['iteration']>0, 'time_since_psychosis'].unique())\n",
    "\n",
    "df_timedeltas.fillna(0, inplace=True)\n",
    "df_timedeltas['time_since_psychosis'] = df_timedeltas['iteration'] * time_per_iter/365\n",
    "df_timedeltas.loc[df_timedeltas['iteration'] <= 0, 'time_since_psychosis'] = 0\n",
    "# check that time_since_psychosis is correct\n",
    "print(df_timedeltas['time_since_psychosis'].unique())\n",
    "print('\\n\\nPre-psychosis tsp', df_timedeltas.loc[df_timedeltas['iteration']<=0, 'time_since_psychosis'].unique())\n",
    "print('\\n\\nPost-psychosis tsp', df_timedeltas.loc[df_timedeltas['iteration']>0, 'time_since_psychosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_vals = df_timedeltas.reset_index().groupby('person_id')['iteration'].rank(method='first').values\n",
    "df_timedeltas['ranked_iteration'] = ranked_vals\n",
    "overall_max = df_timedeltas['ranked_iteration'].max()\n",
    "print(overall_max)\n",
    "\n",
    "df_timedeltas = df_timedeltas.sort_values(['person_id', 'ranked_iteration'])\n",
    "df_timedeltas.set_index(['person_id','ranked_iteration'], inplace=True)\n",
    "df_timedeltas.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "ranked_vals = df_all_iters.reset_index().groupby('person_id')['iteration'].rank(method='first').values\n",
    "df_all_iters['ranked_iteration'] = ranked_vals\n",
    "overall_max = df_all_iters['ranked_iteration'].max()\n",
    "print(overall_max)\n",
    "\n",
    "df_all_iters = df_all_iters.sort_values(['person_id', 'ranked_iteration'])\n",
    "df_all_iters.set_index(['person_id','ranked_iteration'], inplace=True)\n",
    "df_all_iters.sort_index(inplace=True)\n",
    "\n",
    "# check that there is at most a difference of 1 between each pid from one iteration to the next\n",
    "def find_largest_diff(df):\n",
    "    # Sort by pid and iteration\n",
    "    df_sorted = df.sort_values(by=['person_id', 'iteration'])\n",
    "    \n",
    "    # Calculate the largest difference for each pid\n",
    "    result = df_sorted.groupby('person_id')['iteration'].apply(\n",
    "        lambda x: x.diff().max()\n",
    "    ).reset_index(name='largest_diff')\n",
    "    \n",
    "    return result\n",
    "print('Check largest difference', find_largest_diff(df_all_iters)['largest_diff'].max()) # should be 1)\n",
    "\n",
    "print('Checking Match:', (df_all_iters['iteration'].reset_index().values == df_timedeltas['iteration'].reset_index().values).all())\n",
    "df_all_iters.drop('iteration', axis=1, inplace=True)\n",
    "df_timedeltas.drop('iteration', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb034e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_iters.isna().sum().sum(), df_timedeltas.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1ff5a",
   "metadata": {},
   "source": [
    "### Handle data split + scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7988ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.read_csv(int_path + 'MDCD_10_30_tvt_split.csv')\n",
    "df_split = df_split.loc[df_split['person_id'].isin(df_all_iters.index.get_level_values(0))]\n",
    "train_pids = list(df_split.loc[df_split['split']=='train', 'person_id'])\n",
    "val_pids = list(df_split.loc[df_split['split']=='val', 'person_id'])\n",
    "test_pids = list(df_split.loc[df_split['split']=='test', 'person_id'])\n",
    "print(len(train_pids)/len(df_split), len(val_pids)/len(df_split), len(test_pids)/len(df_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03de733",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvt_split = {\n",
    "    \"train_pids\": tuple(train_pids),\n",
    "    \"val_pids\": tuple(val_pids),\n",
    "    \"test_pids\": tuple(test_pids)\n",
    "}\n",
    "with open(int_path + \"MDCD_2_10_grud_dl_da_tvt_order.json\", \"w\") as f:\n",
    "    json.dump(tvt_split, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b39418",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shared_cols == False:\n",
    "    save_cols = list(df_all_iters.columns)\n",
    "\n",
    "    with open(int_path + \"CCAE_1_27_grud_dl_individualfeats_colnames\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(save_cols, fp)\n",
    "else: \n",
    "    save_cols = load(int_path + 'MDCD_2_10_dl_da_colnames')\n",
    "\n",
    "df_all_iters = df_all_iters[save_cols]    \n",
    "print('Check for unnamed col (should be False):', 'Unnamed: 0' in save_cols)\n",
    "print(len(save_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the actual data points\n",
    "train_data = df_all_iters.loc[train_pids]\n",
    "val_data = df_all_iters.loc[val_pids]\n",
    "test_data = df_all_iters.loc[test_pids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the masks of the data points\n",
    "train_data_mask = (train_data>0)*1\n",
    "val_data_mask = (val_data>0)*1\n",
    "test_data_mask = (test_data>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60789d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_scaler:\n",
    "    scaler = StandardScaler()\n",
    "    train_data_mat = scaler.fit_transform(train_data)\n",
    "    print('done with fit/first transform')\n",
    "    val_data_mat = scaler.transform(val_data)\n",
    "    test_data_mat = scaler.transform(test_data)\n",
    "\n",
    "    # save the standard scaler\n",
    "    dump(scaler, int_path + 'CCAE_1_27_grud_dl_individualfeats_scaler.bin', compress=True)\n",
    "    \n",
    "else:\n",
    "    scaler = load(int_path + 'MDCD_2_10_dl_da_scaler.bin')\n",
    "    train_data_mat = scaler.transform(train_data)\n",
    "    val_data_mat = scaler.transform(val_data)\n",
    "    test_data_mat = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138c548",
   "metadata": {},
   "source": [
    "### Fix up columns for timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97392b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(save_cols).difference(df_timedeltas.columns):\n",
    "    if '42898160' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['42898160']\n",
    "    elif 'num_visits_nonhospital' in i:\n",
    "        df_timedeltas['num_visits_nonhospital'] = df_timedeltas['42898160']\n",
    "    elif '9201' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['9201']\n",
    "    elif 'num_visits_inpatient' in i:\n",
    "        df_timedeltas['num_visits_inpatient'] = df_timedeltas['9201']\n",
    "    elif '9202' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['9202']\n",
    "    elif 'num_visits_outpatient' in i:\n",
    "        df_timedeltas['num_visits_outpatient'] = df_timedeltas['9202']\n",
    "    elif '9203' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['9203']\n",
    "    elif 'num_visits_ED' in i:\n",
    "        df_timedeltas['num_visits_ED'] = df_timedeltas['9203']\n",
    "    elif 'psych' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['psych_visits']\n",
    "    elif '262' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['262.0']\n",
    "    elif '38004222' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['38004222.0']\n",
    "    elif '38004228' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['38004228.0']\n",
    "    elif '38004238' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['38004238.0']\n",
    "    elif '38004250' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['38004250.0']\n",
    "    elif '8883' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['8883.0']\n",
    "    elif '8971' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['8971.0']\n",
    "    elif '5083' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['5083.0']\n",
    "    elif '581477' in i:\n",
    "        df_timedeltas[i] = df_timedeltas['581477.0'] \n",
    "    else:\n",
    "        print(i)\n",
    "\n",
    "if demo_aware == True:\n",
    "    df_timedeltas['is_Male'] = 0\n",
    "    df_timedeltas['is_Black'] = 0\n",
    "    df_timedeltas['is_White'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide timedeltas into train/test/split\n",
    "df_timedeltas = df_timedeltas[save_cols]\n",
    "print(df_timedeltas.shape, len(save_cols))\n",
    "# get the actual data points\n",
    "train_tds = df_timedeltas.loc[train_pids]\n",
    "val_tds = df_timedeltas.loc[val_pids]\n",
    "test_tds = df_timedeltas.loc[test_pids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbfb23",
   "metadata": {},
   "source": [
    "### Pad the data: add 0s to the beginning of each patient trajectory\n",
    "- Get the max timestep for each patient --> subtract from the max possible timestep\n",
    "- Add this max per patient to each iteration so that we now have \"backwards-aligned\" patient timesteps\n",
    "- Pad the earliest timesteps with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_df(original_df, scaled_df_mat, pids, overall_max=overall_max):\n",
    "    # get the maximum iterations per patient and subtract from the number of timesteps in the matrix \n",
    "    # for psychosis SCZ, that is 41\n",
    "    save_cols = original_df.columns\n",
    "    original_df = original_df[original_df.columns[0:1]]\n",
    "    max_iter = original_df.reset_index().groupby('person_id')['ranked_iteration'].max()\n",
    "    max_iter.name = 'max_iter'\n",
    "    max_iter = overall_max-max_iter\n",
    "    \n",
    "    # add the number of padding rows that need to happen per patient to the dataframe\n",
    "    original_df = original_df.merge(max_iter, how='left', left_index=True, right_index=True)\n",
    "    original_df.reset_index(inplace=True)\n",
    "    original_df['ranked_iteration'] = original_df['ranked_iteration']+original_df['max_iter']\n",
    "    \n",
    "    # replace the data with the scaled data\n",
    "    original_df.set_index(['person_id', 'ranked_iteration'], inplace=True)\n",
    "    original_df.drop('max_iter', axis=1, inplace=True)\n",
    "    \n",
    "    # create a new dataframe that goes through each patient and each timestep\n",
    "    new_df = pd.DataFrame(index=[np.repeat(pids, overall_max), np.tile(np.arange(1, overall_max+1), len(pids))], columns=save_cols)\n",
    "    \n",
    "    # then fill it in with the existing data\n",
    "    new_df.loc[original_df.index] = scaled_df_mat\n",
    "    \n",
    "    # convert to matrix and fillna\n",
    "    new_df = new_df.values.astype(float)\n",
    "    new_df[np.isnan(new_df)] = 0\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_pop[['person_id', 'sz_flag']].set_index('person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc53473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_iters.isna().sum().sum(), df_timedeltas.isna().sum().sum())\n",
    "print('Unnamed: 0' in df_timedeltas.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95683699",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all_iters\n",
    "del df_timedeltas\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_data = get_full_df(val_data, val_data_mat, tvt_split['val_pids'])\n",
    "val_data_data = val_data_data.reshape(len(val_pids), int(len(val_data_data)/len(val_pids)), val_data_mat.shape[1])\n",
    "\n",
    "val_data_mask = get_full_df(val_data_mask, val_data_mask.values, val_pids)\n",
    "val_data_mask = val_data_mask.reshape(len(val_pids), int(len(val_data_mask)/len(val_pids)), val_data_mask.shape[1])\n",
    "\n",
    "val_last_obs = torch.Tensor(val_data_data * val_data_mask)\n",
    "val_last_obs_mat = torch.cat((torch.zeros((val_last_obs.shape[0], 1, val_last_obs.shape[2])), val_last_obs), 1)\n",
    "val_last_obs_mat = val_last_obs_mat[:,0:-1,:]\n",
    "print(val_last_obs_mat.shape)\n",
    "\n",
    "val_data_deltas = get_full_df(val_tds, val_tds.values, tvt_split['val_pids'])\n",
    "val_data_deltas = val_data_deltas.reshape(len(val_pids), int(len(val_data_deltas)/len(val_pids)), val_tds.shape[1])\n",
    "\n",
    "val_labels = labels.loc[val_pids]\n",
    "stacked_val_data = np.stack([val_data_data, val_last_obs_mat, val_data_mask, val_data_deltas], axis=1)\n",
    "print(stacked_val_data.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876de3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_data = get_full_df(test_data, test_data_mat, tvt_split['test_pids'])\n",
    "test_data_data = test_data_data.reshape(len(test_pids), int(len(test_data_data)/len(test_pids)), test_data_mat.shape[1])\n",
    "\n",
    "test_data_mask = get_full_df(test_data_mask, test_data_mask.values, test_pids)\n",
    "test_data_mask = test_data_mask.reshape(len(test_pids), int(len(test_data_mask)/len(test_pids)), test_data_mask.shape[1])\n",
    "\n",
    "test_last_obs = torch.Tensor(test_data_data * test_data_mask)\n",
    "test_last_obs_mat = torch.cat((torch.zeros((test_last_obs.shape[0], 1, test_last_obs.shape[2])), test_last_obs), 1)\n",
    "test_last_obs_mat = test_last_obs_mat[:,0:-1,:]\n",
    "\n",
    "test_data_deltas = get_full_df(test_tds, test_tds.values, tvt_split['test_pids'])\n",
    "test_data_deltas = test_data_deltas.reshape(len(test_pids), int(len(test_data_deltas)/len(test_pids)), test_data_deltas.shape[1])\n",
    "\n",
    "test_labels = labels.loc[test_pids]\n",
    "stacked_test_data = np.stack([test_data_data, test_last_obs_mat, test_data_mask, test_data_deltas], axis=1)\n",
    "print(stacked_test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864263b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(stacked_val_data), torch.Tensor(val_labels.values))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2048, shuffle=True)\n",
    "torch.save(val_loader, int_path+'MDCD_2_10_grud_dl_da_val_loader_shuffled.pth')\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "torch.save(val_loader, int_path+'MDCD_2_10_grud_dl_da_val_loader_unshuffled.pth')\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(stacked_test_data), torch.Tensor(test_labels.values))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2048, shuffle = False)\n",
    "torch.save(test_loader, int_path+'MDCD_2_10_grud_dl_da_test_loader_unshuffled.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del val_dataset\n",
    "del val_loader\n",
    "del val_data_data\n",
    "del val_data_mask\n",
    "del val_tds\n",
    "del stacked_val_data\n",
    "del val_data_deltas\n",
    "del val_data\n",
    "del val_last_obs_mat\n",
    "del val_last_obs\n",
    "\n",
    "del test_data_mask\n",
    "del test_dataset\n",
    "del test_loader\n",
    "del test_data_data\n",
    "del test_tds\n",
    "del stacked_test_data\n",
    "del test_data_deltas\n",
    "del test_data\n",
    "del test_last_obs_mat\n",
    "del test_last_obs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_data = get_full_df(train_data, train_data_mat, tvt_split['train_pids'])\n",
    "train_data_data = train_data_data.reshape(len(train_pids), int(len(train_data_data)/len(train_pids)), train_data_mat.shape[1])\n",
    "\n",
    "train_data_mask = get_full_df(train_data_mask, train_data_mask.values, tvt_split['train_pids'])\n",
    "train_data_mask = train_data_mask.reshape(len(train_pids), int(len(train_data_mask)/len(train_pids)), train_data_mask.shape[1])\n",
    "\n",
    "train_last_obs = torch.Tensor(train_data_data * train_data_mask)\n",
    "train_last_obs_mat = torch.cat((torch.zeros((train_last_obs.shape[0], 1, train_last_obs.shape[2])), train_last_obs), 1)\n",
    "train_last_obs_mat = train_last_obs_mat[:,0:-1,:]\n",
    "print(train_last_obs_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3bfa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_deltas = get_full_df(train_tds, train_tds.values, tvt_split['train_pids'])\n",
    "train_data_deltas = train_data_deltas.reshape(len(train_pids), int(len(train_data_deltas)/len(train_pids)), train_data_deltas.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_tds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels.loc[train_pids]\n",
    "stacked_train_data = np.stack([train_data_data, train_last_obs_mat, train_data_mask, train_data_deltas], axis=1)\n",
    "print(stacked_train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X_mean\n",
    "nonzero_mask = torch.Tensor(train_data_data) != 0\n",
    "\n",
    "nonzero_counts = nonzero_mask.sum(dim=0) # Count nonzero entries along dim 0\n",
    "nonzero_sum = torch.where(nonzero_mask, torch.Tensor(train_data_data), torch.tensor(0.0)).sum(dim=0) # sum the values\n",
    "safe_nonzero_counts = torch.where(nonzero_counts == 0, torch.tensor(1), nonzero_counts) # avoid division by 0\n",
    "mean_matrix = nonzero_sum / safe_nonzero_counts\n",
    "\n",
    "# Replace positions with zero count back to 0\n",
    "mean_matrix = torch.where(nonzero_counts == 0, torch.tensor(0.0), mean_matrix)\n",
    "mean_matrix = mean_matrix.unsqueeze(0)\n",
    "print(train_data_data.shape, mean_matrix.shape)\n",
    "\n",
    "torch.save(mean_matrix, int_path + 'MDCD_2_10_grud_dl_da_means.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27954114",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data_deltas\n",
    "del train_data_mask\n",
    "del train_data_data\n",
    "del train_last_obs\n",
    "del train_data_mat\n",
    "del nonzero_mask\n",
    "del mean_matrix\n",
    "del safe_nonzero_counts\n",
    "del nonzero_sum\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600778b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(stacked_train_data), torch.Tensor(train_labels.values))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2048, shuffle = True)\n",
    "torch.save(train_loader, int_path + 'MDCD_2_10_grud_dl_da_cfa_train_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_loader\n",
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a183399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c2531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
