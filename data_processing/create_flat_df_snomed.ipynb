{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "import pickle \n",
    "import math\n",
    "\n",
    "from preprocessing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fe65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_path = '../../codes_mappings/'\n",
    "validation_set = False\n",
    "remove_corr_features = False\n",
    "\n",
    "# how much data we are using to make predictions\n",
    "forward_iterations = 13 # 3 years\n",
    "backwards_iterations = 14 # full history: 65; median history: 19 (4.3 years)\n",
    "days_per_iter = 90 # interval size\n",
    "\n",
    "# censor date to cohort start date\n",
    "num_days_prediction = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be3102",
   "metadata": {},
   "source": [
    "# Description\n",
    "1. Load data and run checks on dates + duplicates + presence of schizophrenia information\n",
    "2. Map data to the rolled up concepts where appropriate\n",
    "3. Create a population dataframe with **sequence length** entries per patient. This should have the start date (inclusive) and end date (exclusive) for each subsequence\n",
    "4. Limit to the date range for the given iteration for each person and call the function that creates the features\n",
    "   - Conditions, procedures, and labs: counts per subsequence time\n",
    "   - Medications: total days (end day-start day + 1 to account for single-day medications)\n",
    "   - Visits: number of visits (frequency)and mean + summed length of stay (end day - start day to count overnights)\n",
    "5. Populate time since psychosis\n",
    "6. Fill in \"blank\" iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fab9b",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Also constrict to patients with psychosis at least 6 months pre-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db507ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in population dataframe\n",
    "df_pop = pd.read_csv(f'{data_path}/population_2dx.csv', parse_dates = ['psychosis_diagnosis_date', 'scz_diagnosis_date', 'cohort_start_date'])\n",
    "print(len(df_pop), df_pop['sz_flag'].sum()/len(df_pop), len(df_pop['person_id'].unique()))\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]\n",
    "print(len(df_pop), df_pop['sz_flag'].sum()/len(df_pop), len(df_pop['person_id'].unique()))\n",
    "df_pop['censor_date'] = df_pop['cohort_start_date'] - pd.Timedelta(days=num_days_prediction)\n",
    "\n",
    "count_visits = pd.read_csv(f'{int_path}/hcu_visit_counts.csv', parse_dates = ['first_visit'])\n",
    "df_pop = df_pop.merge(count_visits[['person_id', 'first_visit']], how = 'left', on = 'person_id')\n",
    "\n",
    "if dataset == 'mdcd_1yr': \n",
    "    df_3yr_pop = pd.read_csv(f'{path}/raw_data_mdcd_3yrs/population_2dx.csv')\n",
    "    print(len(df_pop))\n",
    "    df_pop = df_pop.loc[~df_pop['person_id'].isin(df_3yr_pop['person_id'])]\n",
    "    print(len(df_pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = pd.read_csv(f'{data_path}/temporal_visits.csv', parse_dates = ['cohort_start_date', 'visit_start_date', 'visit_end_date'])\n",
    "all_visits = pre_censor_data(all_visits, df_pop, 'visit_start_date')\n",
    "all_visits.loc[all_visits['visit_end_date'] > all_visits['censor_date'], 'visit_end_date'] = all_visits.loc[all_visits['visit_end_date'] > all_visits['censor_date'], 'censor_date']\n",
    "print('Duplicate Visits (should be True)', 'Unnamed: 0' not in all_visits.columns, len(all_visits) == len(all_visits['visit_occurrence_id'].unique()))\n",
    "\n",
    "all_meds = pd.read_csv(f'{data_path}/temporal_medications.csv', parse_dates = ['cohort_start_date', 'drug_era_start_date', 'drug_era_end_date'])\n",
    "all_meds = pre_censor_data(all_meds, df_pop, 'drug_era_start_date')\n",
    "all_meds.loc[all_meds['drug_era_end_date'] > all_meds['censor_date'], 'drug_era_end_date'] = all_meds.loc[all_meds['drug_era_end_date'] > all_meds['censor_date'], 'censor_date']\n",
    "\n",
    "all_conds = pd.read_csv(f'{data_path}/temporal_conditions.csv', parse_dates = ['cohort_start_date', 'condition_start_date'])\n",
    "all_conds = pre_censor_data(all_conds, df_pop, 'condition_start_date')\n",
    "\n",
    "all_procedures = pd.read_csv(f'{data_path}/temporal_procedures.csv', parse_dates = ['cohort_start_date', 'procedure_date'])\n",
    "all_procedures = pre_censor_data(all_procedures, df_pop, 'procedure_date')\n",
    "\n",
    "all_labs = pd.read_csv(f'{data_path}/temporal_labs.csv', parse_dates = ['cohort_start_date', 'measurement_date'])\n",
    "all_labs = pre_censor_data(all_labs, df_pop, 'measurement_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for schizophrenia in fine-grained conditions\n",
    "scz_codes = pd.read_csv(codes_path+'all_scz_codes.csv')\n",
    "print('Check granular presence of SCZ:',len(all_conds.loc[all_conds['condition_concept_id'].isin(scz_codes['standard_concept_id'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ddc75",
   "metadata": {},
   "source": [
    "### Map data to other concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolled_medications = pd.read_csv(codes_path + 'rolled_medications.csv')\n",
    "all_meds = all_meds.merge(rolled_medications[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'drug_concept_id', right_on = 'descendant_concept_id')\n",
    "all_meds = all_meds[['person_id','drug_era_id','drug_era_start_date', 'drug_era_end_date', 'cohort_start_date', 'drug_concept_id', 'rolled_concept_name', 'drug_exposure_count', 'censor_date']].drop_duplicates()\n",
    "all_meds.loc[all_meds['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_meds.loc[all_meds['rolled_concept_name'].isna(), 'drug_concept_id']\n",
    "\n",
    "list_med_concepts = list(all_meds['rolled_concept_name'])\n",
    "list_med_concepts = [str(i) + '_meds' for i in list_med_concepts]\n",
    "all_meds['rolled_concept_name'] = list_med_concepts\n",
    "\n",
    "print('Duplicate Meds (Expect True)', 'Unnamed: 0' not in all_meds.columns, len(all_meds) == len(all_meds[['person_id', 'rolled_concept_name', 'drug_concept_id', 'drug_era_start_date', 'drug_era_end_date']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolled_conditions = pd.read_csv(codes_path + 'rolled_conditions_level4.csv')\n",
    "all_conds = all_conds.merge(rolled_conditions[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'condition_concept_id', right_on = 'descendant_concept_id')\n",
    "all_conds = all_conds[['person_id','condition_occurrence_id','condition_start_date', 'condition_concept_id', 'concept_name', 'rolled_concept_name', 'cohort_start_date', 'visit_occurrence_id', 'censor_date']].drop_duplicates()\n",
    "all_conds.loc[all_conds['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_conds.loc[all_conds['rolled_concept_name'].isna(), 'concept_name']\n",
    "\n",
    "list_cond_concepts = list(all_conds['rolled_concept_name'])\n",
    "list_cond_concepts = [str(i) + '_conds' for i in list_cond_concepts]\n",
    "all_conds['rolled_concept_name'] = list_cond_concepts\n",
    "\n",
    "# now check in more granular conditions\n",
    "for i in list_cond_concepts:\n",
    "    if 'schizo' in i.lower():\n",
    "        print(i)\n",
    "# check uniqueness\n",
    "print('Duplicate Conds (Expect True)', 'Unnamed: 0' not in all_conds.columns, len(all_conds) == len(all_conds[['person_id', 'rolled_concept_name', 'condition_start_date', 'condition_occurrence_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolled_procedures = pd.read_csv(codes_path + 'rolled_procedures_level4.csv')\n",
    "all_procedures = all_procedures.merge(rolled_procedures[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'procedure_concept_id', right_on = 'descendant_concept_id')\n",
    "all_procedures = all_procedures[['person_id','procedure_occurrence_id','procedure_date', 'procedure_concept_id','concept_name', 'rolled_concept_name', 'cohort_start_date', 'censor_date']].drop_duplicates()\n",
    "all_procedures.loc[all_procedures['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_procedures.loc[all_procedures['rolled_concept_name'].isna(), 'concept_name']\n",
    "\n",
    "list_procedure_concepts = list(all_procedures['rolled_concept_name'])\n",
    "list_procedure_concepts = [str(i) + '_procedure' for i in list_procedure_concepts]\n",
    "all_procedures['rolled_concept_name'] = list_procedure_concepts\n",
    "\n",
    "print('Duplicate Procedures (Expect True)', 'Unnamed: 0' not in all_procedures.columns, len(all_procedures) == len(all_procedures[['person_id','rolled_concept_name', 'procedure_concept_id', 'procedure_date', 'procedure_occurrence_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs['rolled_concept_name'] = all_labs['concept_name'].astype(str) + '_lab'\n",
    "all_labs = all_labs[['person_id', 'measurement_concept_id', 'measurement_date', 'measurement_id', 'rolled_concept_name', 'censor_date']].drop_duplicates()\n",
    "print('Duplicate Labs (Expect True)', 'Unnamed: 0' not in all_labs.columns, len(all_labs) == len(all_labs[['person_id', 'measurement_concept_id', 'measurement_date', 'measurement_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7151c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds['start_date'] = all_conds['condition_start_date'].copy()\n",
    "all_procedures['start_date'] = all_procedures['procedure_date'].copy()\n",
    "all_labs['start_date'] = all_labs['measurement_date'].copy()\n",
    "\n",
    "all_cond_lab_pro = pd.concat([all_conds, all_procedures, all_labs])\n",
    "all_cond_lab_pro = all_cond_lab_pro[['person_id', 'rolled_concept_name','cohort_start_date', 'start_date', 'censor_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9ee2d",
   "metadata": {},
   "source": [
    "### Delete Rare Features: anything that does not occur in at least 1% of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3dd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_set == False:\n",
    "    print(len(all_cond_lab_pro))\n",
    "    all_cond_lab_pro = drop_rare_occurrences(all_cond_lab_pro, 'rolled_concept_name', 'person_id', len(df_pop), threshold = 0.01)\n",
    "    print(len(all_cond_lab_pro))\n",
    "    \n",
    "    print(len(all_meds))\n",
    "    all_meds = drop_rare_occurrences(all_meds, 'rolled_concept_name', 'person_id', len(df_pop), threshold = 0.01)\n",
    "    print(len(all_meds))\n",
    "\n",
    "    print(len(all_visits))\n",
    "    all_visits = drop_rare_occurrences(all_visits, 'visit_concept_id', 'person_id', len(df_pop), threshold = 0.01)\n",
    "    print(len(all_visits))\n",
    "else:    \n",
    "    with open(f'{path}/intermediate_data_mdcd_3yrs/9_26_mdcd_2dx_fullhistory_du_snomed_colnames', \"rb\") as fp:   #Pickling\n",
    "        list_mdcd_cols = pickle.load(fp)\n",
    "    all_cond_lab_pro = drop_unshared_features(all_cond_lab_pro, 'rolled_concept_name', list_mdcd_cols)\n",
    "    all_meds = drop_unshared_features(all_meds, 'rolled_concept_name', list_mdcd_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2897e",
   "metadata": {},
   "source": [
    "### Check for Data Leakage: \n",
    "Minimum times should be at least 90 days and cohort start date should be same across all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac569",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = (all_cond_lab_pro['cohort_start_date']-all_cond_lab_pro['start_date']).dt.days\n",
    "print('Labs, conditions, procedures:', check.min(), check.max())\n",
    "\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days\n",
    "print('Meds (Start of prescription):', check.min(), check.max())\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days\n",
    "print('Meds (End of prescription):', check.min(), check.max())\n",
    "\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days\n",
    "print('Visits (Start of visit):', check.min(), check.max())\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days\n",
    "print('Visits (End of visit):', check.min(), check.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e13e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cohort_start = df_pop[['person_id','cohort_start_date']]\n",
    "check_cohort_start = check_cohort_start.merge(all_cond_lab_pro[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_cohort_start = check_cohort_start.merge(all_visits[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_cohort_start = check_cohort_start.merge(all_meds[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_cohort_start.set_index('person_id',inplace=True)\n",
    "check_cohort_start = check_cohort_start.T\n",
    "num_unique = check_cohort_start.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where cohort start date doesnt align:',(num_unique>1).sum())\n",
    "\n",
    "check_censor = df_pop[['person_id','censor_date']]\n",
    "check_censor = check_censor.merge(all_cond_lab_pro[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_censor = check_censor.merge(all_visits[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_censor = check_censor.merge(all_meds[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_censor.set_index('person_id',inplace=True)\n",
    "check_censor = check_censor.T\n",
    "num_unique = check_censor.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where censor date doesnt align:',(num_unique>1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "print('Num Features Conds/Labs/Procedures', len(all_cond_lab_pro['rolled_concept_name'].unique()))\n",
    "print('Num Features Meds', len(all_meds['rolled_concept_name'].unique()))\n",
    "print('Num Features Visits', len(all_visits['visit_concept_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba5b3",
   "metadata": {},
   "source": [
    "### \"Psychiatric\" visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health_conds = pd.read_csv(f'{codes_path}/mental_disorder_descendants.csv')\n",
    "mh_visits = all_conds.loc[all_conds['condition_concept_id'].isin(mental_health_conds['descendant_concept_id']), 'visit_occurrence_id']\n",
    "df_mh_visits = all_visits.loc[all_visits['visit_occurrence_id'].isin(mh_visits)]\n",
    "\n",
    "all_visits['visit_concept_id'] = all_visits['visit_concept_id'].astype(str) + '_ALL'\n",
    "df_mh_visits['visit_concept_id'] = df_mh_visits['visit_concept_id'].astype(str) + '_MH'\n",
    "all_visits = pd.concat([all_visits, df_mh_visits], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff0a79-a1eb-4525-9f71-cf9c2333dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15380d59",
   "metadata": {},
   "source": [
    "# Function for processing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp pop needs to have a \"years obs\" column\n",
    "def make_static_df(temp_pop, temp_cond_pro_labs, temp_meds, temp_visits):\n",
    "    ### CONDITIONS/PROCEDURES/LABS\n",
    "    condprolab_features = temp_cond_pro_labs.pivot_table(index='person_id', columns='rolled_concept_name', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    # get conditions per year\n",
    "    condprolab_features = condprolab_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    condprolab_features = condprolab_features.div(condprolab_features.years_obs, axis=0) \n",
    "    condprolab_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    ### MEDICATIONS\n",
    "    temp_meds['drug_exposure_days'] = (temp_meds['drug_era_end_date']-temp_meds['drug_era_start_date']).dt.days + 1 # +1 so a 1-day prescription will not be 0 days\n",
    "    count_meds = temp_meds[['person_id', 'rolled_concept_name', 'drug_exposure_days']].groupby(['person_id', 'rolled_concept_name']).sum().reset_index()\n",
    "    meds_features = count_meds.pivot_table(index='person_id', columns='rolled_concept_name', values='drug_exposure_days', fill_value=0)\n",
    "    \n",
    "    # get medications per year\n",
    "    meds_features = meds_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    meds_features = meds_features.div(meds_features.years_obs, axis=0) \n",
    "    meds_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "\n",
    "    ###VISITS\n",
    "    # Number of visits\n",
    "    num_visits = temp_visits.groupby(['person_id', 'visit_concept_id']).count()['cohort_start_date'].reset_index()\n",
    "    num_visits = num_visits.pivot_table(index='person_id', columns = 'visit_concept_id', values = 'cohort_start_date', fill_value=0)\n",
    "    num_visits_columns = [str(i)+'_num_visits' for i in num_visits.columns]\n",
    "    num_visits.columns = num_visits_columns\n",
    "    # adjust so that num_visits is per year of observation\n",
    "    num_visits = num_visits.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    num_visits = num_visits.div(num_visits.years_obs, axis=0) \n",
    "    num_visits.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    # Length of Stay\n",
    "\n",
    "    temp_visits['los'] = (temp_visits['visit_end_date']-temp_visits['visit_start_date']).dt.days\n",
    "    los = temp_visits.groupby(['person_id', 'visit_concept_id'])['los'].sum()\n",
    "    los = los.reset_index()\n",
    "    los = los.pivot_table(index='person_id', columns = 'visit_concept_id', values='los', fill_value = 0)\n",
    "    los.columns = [col+'_los' for col in los.columns]\n",
    "\n",
    "    visits_features = num_visits.merge(los, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    # TODO: check that all three of these have person_id index\n",
    "    atemporal_features = condprolab_features.merge(meds_features, how = 'outer', left_index=True, right_index=True)\n",
    "    atemporal_features = atemporal_features.merge(visits_features, how = 'outer', left_index=True, right_index=True)\n",
    "    \n",
    "    return atemporal_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965c2bc",
   "metadata": {},
   "source": [
    "### Create the \"iterative\" population dataframe: \n",
    "- Start at the date of initial psychosis diagnosis, then go every XX days (120 days), cutting off at the censor date (if you go over the censor date, chop to the censor date).\n",
    "- Practically, this means that start date 1 is first visit & end date 1 is psychosis; then start date 2 is psychosis dx date and end date 2 is psychosis dx + 90... \n",
    "- Then, starting at the date of psychosis, go back in XX-day increments for 3 years (9 iterations). The earliest iteration (furthest away from psychosis date) should consist of all prior data, and if a person has less than 3 years of data pre-psychosis, they should have fewer early visits.\n",
    "- Note that start dates are inclusive and end dates are exclusive\n",
    "- **KEEP IN MIND FOR LATER: WE WANT TO PAD AT THE BEGINNING, NOT AT THE END. So then we move each person to be aligned at the end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745fecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pop = df_pop[['person_id', 'first_visit', 'cohort_start_date', 'psychosis_diagnosis_date', 'censor_date']]\n",
    "\n",
    "df_pop['0_end'] = df_pop['psychosis_diagnosis_date']\n",
    "df_pop['0_start'] = df_pop['psychosis_diagnosis_date'] - pd.Timedelta(days_per_iter, 'days')\n",
    "print(len(df_pop))\n",
    "df_pop.loc[df_pop['0_end']>df_pop['censor_date'], '0_end'] = df_pop.loc[df_pop['0_end']>df_pop['censor_date'], 'censor_date']\n",
    "print(len(df_pop))\n",
    "\n",
    "# after the loops, remove people for whom 0_start-0_end > 0\n",
    "\n",
    "# FORWARD LOOP: starting at psychosis dx, every XX days till censor date\n",
    "for count in range(1, forward_iterations+1): \n",
    "    # get the start date as the same day as prev end date and the end date as start + 120 days\n",
    "    df_pop[str(count)+'_start'] = df_pop[str(count-1)+'_end']\n",
    "    df_pop[str(count)+'_end'] = df_pop[str(count)+'_start'] + pd.Timedelta(days_per_iter, 'days')\n",
    "    \n",
    "    # update start/end dates to make sure it is at max, the censor date\n",
    "    df_pop.loc[df_pop[str(count)+'_start'] > df_pop['censor_date'], str(count)+'_start'] = df_pop.loc[df_pop[str(count)+'_start'] > df_pop['censor_date'], 'censor_date']\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] > df_pop['censor_date'], str(count)+'_end'] = df_pop.loc[df_pop[str(count)+'_end'] > df_pop['censor_date'], 'censor_date']\n",
    "    \n",
    "    # if start date == censor date: set start and end to NaT\n",
    "    df_pop.loc[df_pop[str(count)+'_start'] == df_pop['censor_date'], [str(count)+'_start', str(count)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')]\n",
    "\n",
    "for count in np.arange(-1, -1*backwards_iterations, -1):\n",
    "    df_pop[str(count)+'_end'] = df_pop[str(count+1)+'_start']\n",
    "    df_pop[str(count)+'_start'] = df_pop[str(count)+'_end']-pd.Timedelta(days_per_iter, 'days')\n",
    "    \n",
    "    # if the visit starts or ends prior to first_visit, set start/end to first_visit\n",
    "    df_pop.loc[df_pop[str(count)+'_start']<df_pop['first_visit'], str(count)+'_start'] = df_pop.loc[df_pop[str(count)+'_start']<df_pop['first_visit'], 'first_visit']\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] < df_pop['first_visit'], str(count)+'_end'] = df_pop.loc[df_pop[str(count)+'_end'] < df_pop['first_visit'], 'first_visit']\n",
    "    \n",
    "    # if end date == first visit date: set start and end to NaT\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] == df_pop['first_visit'], [str(count)+'_start', str(count)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')]\n",
    "    \n",
    "    df_pop[str(count)+'_end'] = pd.to_datetime(df_pop[str(count)+'_end'], format = '%Y-%m-%d')\n",
    "\n",
    "# set the last backwards iteration start date to be the first visit ONLY if the date is not nan\n",
    "df_pop.loc[~(df_pop[f'{str(-1*backwards_iterations+1)}_start'].isna()), f'{str(-1*backwards_iterations+1)}_start'] = df_pop.loc[~(df_pop[f'{str(-1*backwards_iterations+1)}_start'].isna()), 'first_visit']\n",
    "print(df_pop[f'{str(-1*backwards_iterations+1)}_start'].isna().sum()/len(df_pop)) # check that this is ~50% or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdb403",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = []\n",
    "for col in df_pop.columns: \n",
    "    if df_pop[col].isna().sum() == len(df_pop):\n",
    "        remove_cols.append(col)\n",
    "print(remove_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all \"timesteps\" are at least 1 day\n",
    "# also make sure none of them start/end after the censor date\n",
    "for i in np.arange(-1*backwards_iterations+1, forward_iterations):\n",
    "    df_pop.loc[df_pop[str(i)+'_end']==df_pop[str(i)+'_start'], [str(i)+'_start', str(i)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')] \n",
    "df_pop.drop(remove_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594569",
   "metadata": {},
   "source": [
    "### Loop through the iterative population dataframe to get the features for each person in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01537cee-81ae-4559-8475-728932a1334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_cols = [col for col in list(df_pop.columns) if '_end' in col]\n",
    "iter_cols = [int(i.split('_')[0]) for i in iter_cols]\n",
    "print(iter_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635978c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_feature_dfs = []\n",
    "\n",
    "for iteration in np.arange(np.min(iter_cols), np.max(iter_cols), 1): \n",
    "    temp_df_iter_pop = df_pop.copy()\n",
    "    temp_df_iter_pop['iter_start_date'] = temp_df_iter_pop[str(iteration)+'_start']\n",
    "    temp_df_iter_pop['iter_end_date'] = temp_df_iter_pop[str(iteration)+'_end']\n",
    "    \n",
    "    # constrict to people with a valid iteration\n",
    "    temp_df_iter_pop = temp_df_iter_pop.loc[~(temp_df_iter_pop['iter_start_date'].isna())]\n",
    "    temp_df_iter_pop['years_obs'] = (temp_df_iter_pop['iter_end_date']-temp_df_iter_pop['iter_start_date']).dt.days/365\n",
    "\n",
    "    temp_df_iter_pop['iteration'] = iteration\n",
    "    all_rows = temp_df_iter_pop[['person_id', 'iteration']]\n",
    "    \n",
    "    # for conditions, labs, procedures, just compare the start_date to the cutoff date\n",
    "    temp_cond_lab_pro = all_cond_lab_pro.loc[all_cond_lab_pro['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_cond_lab_pro = temp_cond_lab_pro.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_cond_lab_pro = temp_cond_lab_pro.loc[temp_cond_lab_pro['start_date']>= temp_cond_lab_pro['iter_start_date']]\n",
    "    temp_cond_lab_pro = temp_cond_lab_pro.loc[temp_cond_lab_pro['start_date']< temp_cond_lab_pro['iter_end_date']]\n",
    "    \n",
    "    # note: for meds and visits, replace iter_end_date with equal_end_date, which is the \n",
    "    # day before the actual end date since is the last day that we are allowing the visit to \"equal\"\n",
    "    # ie visit < iter_end_date but visit <= equal_end_date\n",
    "    \n",
    "    #for medications and visits, we want to look at \n",
    "    #1. med start date needs to be before iteration end date\n",
    "    #2. med end date needs to be on or after iteration start date\n",
    "    \n",
    "    #3. if med start date is before iteration start date -- make med start date iteration start date\n",
    "    #4. if med end date is after iteration end date -- make med end date iteration end date\n",
    "        \n",
    "    temp_meds = all_meds.loc[all_meds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_meds = temp_meds.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_meds['equal_end_date'] = temp_meds['iter_end_date']-pd.Timedelta(1, 'days')\n",
    "    \n",
    "    temp_meds = temp_meds.loc[(temp_meds['drug_era_start_date']<temp_meds['iter_end_date'])&(temp_meds['drug_era_end_date']>=temp_meds['iter_start_date'])]\n",
    "    temp_meds.loc[temp_meds['drug_era_start_date']<temp_meds['iter_start_date'], 'drug_era_start_date'] = temp_meds.loc[temp_meds['drug_era_start_date']<temp_meds['iter_start_date'], 'iter_start_date']\n",
    "    temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['equal_end_date'], 'drug_era_end_date'] = temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['equal_end_date'], 'equal_end_date']\n",
    "\n",
    "    temp_meds['days_to_cohort_start'] = (temp_meds['cohort_start_date']-temp_meds['drug_era_start_date']).dt.days\n",
    "    \n",
    "    # Repeat for visits\n",
    "    temp_visits = all_visits.loc[all_visits['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_visits = temp_visits.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_visits['equal_end_date'] = temp_visits['iter_end_date']-pd.Timedelta(1, 'days')\n",
    "    \n",
    "    temp_visits = temp_visits.loc[(temp_visits['visit_start_date']<temp_visits['iter_end_date'])&(temp_visits['visit_end_date']>=temp_visits['iter_start_date'])]\n",
    "    temp_visits.loc[temp_visits['visit_start_date']<temp_visits['iter_start_date'], 'visit_start_date'] = temp_visits.loc[temp_visits['visit_start_date']<temp_visits['iter_start_date'], 'iter_start_date']\n",
    "    temp_visits.loc[temp_visits['visit_end_date']>temp_visits['equal_end_date'], 'visit_end_date'] = temp_visits.loc[temp_visits['visit_end_date']>temp_visits['equal_end_date'], 'equal_end_date']\n",
    "    \n",
    "    temp_visits['days_to_cohort_start'] = (temp_visits['cohort_start_date']-temp_visits['visit_start_date']).dt.days\n",
    "\n",
    "    \n",
    "    if len(temp_cond_lab_pro.loc[temp_cond_lab_pro['start_date']>=temp_cond_lab_pro['iter_end_date']])+len(temp_cond_lab_pro.loc[temp_cond_lab_pro['start_date']<temp_cond_lab_pro['iter_start_date']]) > 0:\n",
    "        print('Leakage in conds')        \n",
    "        \n",
    "    if len(temp_meds.loc[temp_meds['drug_era_start_date']>temp_meds['iter_end_date']])+len(temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['iter_end_date']]) > 0:\n",
    "        print('Leakage in med ends')\n",
    "    if len(temp_meds.loc[temp_meds['drug_era_end_date']<temp_meds['iter_start_date']])+len(temp_meds.loc[temp_meds['drug_era_start_date']<temp_meds['iter_start_date']]) > 0:\n",
    "        print('Leakage in med starts')\n",
    "        \n",
    "    if len(temp_visits.loc[temp_visits['visit_start_date']>temp_visits['iter_end_date']])+len(temp_visits.loc[temp_visits['visit_end_date']>temp_visits['iter_end_date']]) > 0:\n",
    "        print('Leakage in visit ends')\n",
    "    if len(temp_visits.loc[temp_visits['visit_end_date']<temp_visits['iter_start_date']])+len(temp_visits.loc[temp_visits['visit_start_date']<temp_visits['iter_start_date']]) > 0:\n",
    "        print('Leakage in visit starts')\n",
    "\n",
    "    all_features = make_static_df(temp_df_iter_pop, temp_cond_lab_pro, temp_meds, temp_visits)\n",
    "    all_features['iteration'] = iteration \n",
    "\n",
    "    all_features = all_features.merge(all_rows, how = 'right', on = ['person_id', 'iteration'])\n",
    "    all_features.fillna(0, inplace=True)\n",
    "    \n",
    "    list_feature_dfs.append(all_features)\n",
    "    print(iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters = pd.concat(list_feature_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_feature_dfs\n",
    "del all_visits\n",
    "del all_conds\n",
    "del all_meds\n",
    "del all_procedures\n",
    "del all_labs\n",
    "del all_cond_lab_pro\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_set == True:\n",
    "    missing_cols = list(set(list_mdcd_cols).difference(df_all_iters.columns))\n",
    "    print(len(missing_cols))\n",
    "    df_all_iters.loc[:,missing_cols] = 0\n",
    "    \n",
    "# remove correlated features: cumc\n",
    "if remove_corr_features == True:\n",
    "    train_df = df_all_iters.loc[df_all_iters['person_id'].isin(train_pids)]\n",
    "    corr_df = train_df.corr()\n",
    "    corr_df.drop(['person_id', 'iteration'], axis=1, inplace=True)\n",
    "    corr_df.drop(['person_id', 'iteration'], axis=0, inplace=True)\n",
    "\n",
    "    print(len(corr_df.columns))\n",
    "    reduced_cols_1 = remove_highcorr_cols(1, corr_df.columns, corr_df)\n",
    "    print(len(reduced_cols_1))\n",
    "    corr_df = corr_df.loc[reduced_cols_1, reduced_cols_1]\n",
    "    reduced_cols_2 = remove_highcorr_cols(0.95, corr_df.columns, corr_df)\n",
    "    print(len(reduced_cols_2))\n",
    "    \n",
    "    df_all_iters = df_all_iters.loc[:, ['person_id', 'iteration']+reduced_cols_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters['time_since_psychosis'] = df_all_iters['iteration'] * days_per_iter/365\n",
    "df_all_iters.loc[df_all_iters['iteration'] <= 0, 'time_since_psychosis'] = 0\n",
    "(df_all_iters['iteration']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that time_since_psychosis is correct\n",
    "print('Pre-psychosis tsp', df_all_iters.loc[df_all_iters['iteration']<=0, 'time_since_psychosis'].unique())\n",
    "print('Post-psychosis tsp', df_all_iters.loc[df_all_iters['iteration']>0, 'time_since_psychosis'].unique())\n",
    "print(len(df_all_iters.loc[df_all_iters['person_id'].isna()]))\n",
    "print(len(df_all_iters.loc[df_all_iters['person_id']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_iters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79003012-b515-4f07-856d-64d8f68ac51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the time to event vector (in 2d form): \n",
    "df_just_iters = df_all_iters[['person_id', 'iteration']]\n",
    "df_just_iters['ranked_iteration'] = df_just_iters['iteration'] - df_just_iters['iteration'].min()\n",
    "\n",
    "print('Len of all iterations', len(df_just_iters))\n",
    "list_end_dates = []\n",
    "min_iteration = df_just_iters['iteration'].min()\n",
    "max_iteration = df_just_iters['iteration'].max()\n",
    "for i in range(min_iteration, max_iteration + 1):\n",
    "    end_dates = df_pop[['person_id', f'{str(i)}_end']]\n",
    "    end_dates['iteration'] = i\n",
    "    end_dates.dropna(inplace=True)\n",
    "    end_dates.rename({f'{str(i)}_end':'interval_end_date'}, axis=1, inplace=True)\n",
    "    list_end_dates.append(end_dates)\n",
    "\n",
    "end_dates = pd.concat(list_end_dates)\n",
    "end_dates['interval_end_date'] = pd.to_datetime(end_dates['interval_end_date'])\n",
    "df_just_iters = df_just_iters.merge(end_dates, how = 'left', on = ['person_id', 'iteration'])\n",
    "df_just_iters = df_just_iters.merge(df_pop[['person_id', 'cohort_start_date']], how = 'inner', on = 'person_id')\n",
    "df_just_iters['time_to_event'] = (df_just_iters['cohort_start_date']-df_just_iters['interval_end_date']).dt.days\n",
    "print('Len of all iterations', len(df_just_iters))\n",
    "df_just_iters = df_just_iters.loc[df_just_iters['iteration'] >= 0]\n",
    "df_just_iters.to_csv(f'{int_path}/{dataset_prefix}time_to_event.csv', index=False)\n",
    "print('Len of positive iterations', len(df_just_iters))\n",
    "\n",
    "# CHECK: COHORT START DATE MATCHES IN DF_POP\n",
    "check_cohort_start = df_just_iters.merge(df_pop[['person_id', 'cohort_start_date']], how = 'inner', on = 'person_id')\n",
    "print('Len of positive iterations', len(check_cohort_start))\n",
    "print('Len of cohort matching', len(check_cohort_start['cohort_start_date_x'] == check_cohort_start['cohort_start_date_y']))\n",
    "\n",
    "# CHECK: EACH PERSON HAS A SINGLE INTERVAL END DATE THAT CORRESPONDS TO PSYCHOSIS DIAGNOSIS DATE\n",
    "df_just_iters = df_just_iters.merge(df_pop[['person_id', 'psychosis_diagnosis_date']], how = 'inner', on = 'person_id')\n",
    "print('All ppl should have psychosis interval', sum(df_just_iters['psychosis_diagnosis_date'] == df_just_iters['interval_end_date'])/len(df_just_iters['person_id'].unique()))\n",
    "# AND NO PERSON HAS AN INTERVAL END DATE FROM BEFORE PSYCHOSIS DIAGNOSIS DATE\n",
    "print('no pre-psychosis interval',len(df_just_iters.loc[df_just_iters['interval_end_date'] < df_just_iters['psychosis_diagnosis_date']]))\n",
    "# AND THIS SHOULD ALL CORRESPOND TO A SINGLE RANKED ITERATION TAHT WE WILL USE IN CREATE DATALOADERS\n",
    "print(df_just_iters.loc[df_just_iters['interval_end_date'] == df_just_iters['psychosis_diagnosis_date'], 'ranked_iteration'].unique())\n",
    "print(df_just_iters['ranked_iteration'].min())\n",
    "# CHECK: what is the minimum number of iterations a person has\n",
    "print(df_just_iters.groupby(['person_id'])['ranked_iteration'].count().min())\n",
    "\n",
    "df_just_iters.set_index(['person_id','ranked_iteration'], inplace=True)\n",
    "df_just_iters.sort_index(inplace=True)\n",
    "print('Check largest difference', find_largest_diff(df_just_iters)['largest_diff'].max()) # should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = list(df_all_iters.columns)\n",
    "\n",
    "data_columns.remove('person_id')\n",
    "data_columns.remove('iteration')\n",
    "\n",
    "with open(f'{int_path}/{dataset_prefix}du_snomed_colnames', \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(data_columns, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a458c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters.to_csv(f'{int_path}/{dataset_prefix}snomed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99936780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is df_pop with the iterations\n",
    "df_pop.to_csv(f'{int_path}/{dataset_prefix}iteration_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b398ee-4b44-47ab-84f2-bc1ab8b8b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27f41b-3549-44f8-b252-4f1f3c5bae73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82add2f4-5750-4bcc-bed5-b540cd3b9822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ceda7-15bd-47dd-bac4-22e4925d2ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a3300-347b-40a1-ad89-c2f808df01e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaea03d-5414-4072-959b-05d59dc84898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
