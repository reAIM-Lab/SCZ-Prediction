{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "import pymssql\n",
    "import pickle \n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = \"connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fe65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccae_validation_set = False\n",
    "remove_corr_features = True\n",
    "\"path stuff\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be3102",
   "metadata": {},
   "source": [
    "# Create Data\n",
    "1. Read in data\n",
    "2. Create a population dataframe with N (N = len_seq) entries per patient. This should have the start date (inclusive) and end date (exclusive) for each \"iteration\" \n",
    "3. In a loop: \n",
    "- Limit data to that date-range for each person\n",
    "- Run a modified version of the function used by the XGBoost model to generate features. \n",
    "- Divide by the number of years in the time chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fab9b",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Also constrict to patients with psychosis at least 90 days pre-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db507ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days_prediction = 90\n",
    "df_pop = pd.read_csv(raw_path+'population.csv')\n",
    "df_pop.rename({'psychosis_dx_date':'psychosis_diagnosis_date'}, axis=1, inplace=True)\n",
    "df_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_pop['psychosis_diagnosis_date'], format=\"mixed\", dayfirst=False)\n",
    "df_pop['cohort_start_date'] = pd.to_datetime(df_pop['cohort_start_date'], format=\"mixed\", dayfirst=False)\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = pd.read_csv(raw_path+'temporal_visits.csv')\n",
    "all_visits['visit_start_date'] = pd.to_datetime(all_visits['visit_start_date'], format=\"mixed\", dayfirst = False)\n",
    "df_pop = df_pop.merge(all_visits.groupby('person_id').min()['visit_start_date'], how='left', left_on='person_id',right_index=True)\n",
    "df_pop.rename({'visit_start_date':'first_visit'}, axis=1, inplace=True)\n",
    "df_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds = pd.read_csv(raw_path+'temporal_conditions.csv')\n",
    "all_meds = pd.read_csv(raw_path+'temporal_medications.csv')\n",
    "all_procedures = pd.read_csv(raw_path+'temporal_procedures.csv')\n",
    "all_labs = pd.read_csv(raw_path+'temporal_labs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ddc75",
   "metadata": {},
   "source": [
    "### Restrict all data to appropriate time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meds = all_meds.loc[all_meds['person_id'].isin(df_pop['person_id'])]\n",
    "all_meds['cohort_start_date'] = pd.to_datetime(all_meds['cohort_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_meds['drug_era_start_date'] = pd.to_datetime(all_meds['drug_era_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_meds['drug_era_end_date'] = pd.to_datetime(all_meds['drug_era_end_date'], format=\"mixed\", dayfirst=False)\n",
    "all_meds = all_meds.loc[(all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days >= num_days_prediction]\n",
    "all_meds['days_to_cohort_start'] = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days\n",
    "\n",
    "# medications mapping \n",
    "medications_mapping_query = (\"SELECT c_atc.concept_id as rolled_concept_id, c_atc.concept_name as rolled_concept_name, c_standard.concept_id as descendant_concept_id, c_standard.concept_name as descendant_concept_name \"+\n",
    "                             \"FROM dbo.concept as c_atc \"+\n",
    "                             \"LEFT JOIN dbo.concept_ancestor as ca on ancestor_concept_id=c_atc.concept_id \"+\n",
    "                             \"LEFT JOIN dbo.concept as c_standard on c_standard.concept_id = descendant_concept_id \"+\n",
    "                             \"WHERE c_atc.concept_class_id = 'ATC 3rd' AND c_standard.standard_concept = 'S'\")\n",
    "\n",
    "medications_mapping = pd.io.sql.read_sql(medications_mapping_query, conn)\n",
    "\n",
    "\n",
    "\n",
    "# medications mapping: move Lithium to the antiepileptics category\n",
    "def generate_code_list(drugtype, concept_class):\n",
    "    sql_query = (\"SELECT ancestor_concept_id, descendant_concept_id, concept_name \" + \n",
    "               \"FROM dbo.concept_ancestor JOIN dbo.concept ON descendant_concept_id = concept_id \"+\n",
    "               \"WHERE ancestor_concept_id = (SELECT concept_id from dbo.concept WHERE concept_class_id = '\"+concept_class+\"' AND concept_name = '\"+drugtype+\"');\")\n",
    "    codes_list = pd.read_sql(sql_query, conn)\n",
    "    return list(codes_list['descendant_concept_id'])\n",
    "\n",
    "lithium_list = generate_code_list('Lithium', 'ATC 4th')\n",
    "medications_mapping.loc[(medications_mapping['descendant_concept_id'].isin(lithium_list))&(medications_mapping['rolled_concept_name']=='ANTIPSYCHOTICS'), 'rolled_concept_name'] = 'ANTIEPILEPTICS'\n",
    "medications_mapping['rolled_concept_name'].replace({'ANTIEPILEPTICS': 'MOOD STABILIZERS'}, inplace=True)\n",
    "\n",
    "all_meds = all_meds.merge(medications_mapping[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'drug_concept_id', right_on = 'descendant_concept_id')\n",
    "all_meds = all_meds[['person_id','drug_era_id','drug_era_start_date', 'drug_era_end_date', 'cohort_start_date', 'drug_concept_id', 'rolled_concept_name', 'drug_exposure_count']].drop_duplicates()\n",
    "all_meds.loc[all_meds['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_meds.loc[all_meds['rolled_concept_name'].isna(), 'drug_concept_id']\n",
    "\n",
    "list_med_concepts = list(all_meds['rolled_concept_name'])\n",
    "list_med_concepts = [str(i) + '_meds' for i in list_med_concepts]\n",
    "all_meds['rolled_concept_name'] = list_med_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = all_visits.loc[all_visits['person_id'].isin(df_pop['person_id'])]\n",
    "all_visits['cohort_start_date'] = pd.to_datetime(all_visits['cohort_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_visits['visit_start_date'] = pd.to_datetime(all_visits['visit_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_visits['visit_end_date'] = pd.to_datetime(all_visits['visit_end_date'], format=\"mixed\", dayfirst=False)\n",
    "all_visits = all_visits.loc[(all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days >= num_days_prediction]\n",
    "all_visits['days_to_cohort_start'] = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds = all_conds.loc[all_conds['person_id'].isin(df_pop['person_id'])]\n",
    "all_conds['cohort_start_date'] = pd.to_datetime(all_conds['cohort_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_conds['condition_start_date'] = pd.to_datetime(all_conds['condition_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_conds['days_to_cohort_start'] = (all_conds['cohort_start_date']-all_conds['condition_start_date']).dt.days\n",
    "all_conds = all_conds.loc[all_conds['days_to_cohort_start'] >= num_days_prediction]\n",
    "\n",
    "rollup_conds = \"\"\"WITH rolled_conditions AS (SELECT descendant_concept_id FROM dbo.concept_ancestor WHERE ancestor_concept_id = 441840 AND max_levels_of_separation = 4)\n",
    "                        SELECT rolled_conditions.descendant_concept_id as rolled_concept_id, ca.descendant_concept_id, concept_name as rolled_concept_name\n",
    "                        FROM rolled_conditions\n",
    "                        LEFT JOIN dbo.concept_ancestor as ca ON ca.ancestor_concept_id = rolled_conditions.descendant_concept_id\n",
    "                        LEFT JOIN dbo.concept on rolled_conditions.descendant_concept_id = concept_id\"\"\"\n",
    "\n",
    "conditions_mapping = pd.io.sql.read_sql(rollup_conds, conn)\n",
    "all_conds = all_conds.merge(conditions_mapping[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'condition_concept_id', right_on = 'descendant_concept_id')\n",
    "all_conds = all_conds[['person_id','condition_occurrence_id','condition_start_date', 'condition_concept_id', 'concept_name', 'rolled_concept_name', 'cohort_start_date', 'days_to_cohort_start']].drop_duplicates()\n",
    "all_conds.loc[all_conds['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_conds.loc[all_conds['rolled_concept_name'].isna(), 'concept_name']\n",
    "\n",
    "list_cond_concepts = list(all_conds['rolled_concept_name'])\n",
    "list_cond_concepts = [str(i) + '_conds' for i in list_cond_concepts]\n",
    "all_conds['rolled_concept_name'] = list_cond_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_procedures = all_procedures.loc[all_procedures['person_id'].isin(df_pop['person_id'])]\n",
    "all_procedures['cohort_start_date'] = pd.to_datetime(all_procedures['cohort_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_procedures['procedure_date'] = pd.to_datetime(all_procedures['procedure_date'], format=\"mixed\", dayfirst=False)\n",
    "all_procedures['days_to_cohort_start'] = (all_procedures['cohort_start_date']-all_procedures['procedure_date']).dt.days\n",
    "all_procedures = all_procedures.loc[all_procedures['days_to_cohort_start'] >= num_days_prediction]\n",
    "\n",
    "rollup_procedures = \"\"\"WITH rolled_procedures AS (SELECT descendant_concept_id FROM dbo.concept_ancestor WHERE ancestor_concept_id = 45889197 AND max_levels_of_separation = 4)\n",
    "                        SELECT rolled_procedures.descendant_concept_id as rolled_concept_id, ca.descendant_concept_id, concept_name as rolled_concept_name\n",
    "                        FROM rolled_procedures\n",
    "                        LEFT JOIN dbo.concept_ancestor as ca ON ancestor_concept_id = rolled_procedures.descendant_concept_id\n",
    "                        LEFT JOIN dbo.concept on rolled_procedures.descendant_concept_id = concept_id\"\"\"\n",
    "\n",
    "procedures_mapping = pd.io.sql.read_sql(rollup_procedures, conn)\n",
    "all_procedures = all_procedures.merge(procedures_mapping[['descendant_concept_id', 'rolled_concept_name', 'rolled_concept_id']], how='left', left_on = 'procedure_concept_id', right_on = 'descendant_concept_id')\n",
    "all_procedures = all_procedures[['person_id','procedure_occurrence_id','procedure_date', 'procedure_concept_id','concept_name', 'rolled_concept_name', 'cohort_start_date', 'days_to_cohort_start']].drop_duplicates()\n",
    "all_procedures.loc[all_procedures['rolled_concept_name'].isna(), 'rolled_concept_name'] = all_procedures.loc[all_procedures['rolled_concept_name'].isna(), 'concept_name']\n",
    "\n",
    "list_procedure_concepts = list(all_procedures['rolled_concept_name'])\n",
    "list_procedure_concepts = [str(i) + '_procedure' for i in list_procedure_concepts]\n",
    "all_procedures['rolled_concept_name'] = list_procedure_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs = all_labs.loc[all_labs['person_id'].isin(df_pop['person_id'])]\n",
    "all_labs['cohort_start_date'] = pd.to_datetime(all_labs['cohort_start_date'], format=\"mixed\", dayfirst=False)\n",
    "all_labs['measurement_date'] = pd.to_datetime(all_labs['measurement_date'], format=\"mixed\", dayfirst=False)\n",
    "all_labs['days_to_cohort_start'] = (all_labs['cohort_start_date']-all_labs['measurement_date']).dt.days\n",
    "all_labs = all_labs.loc[all_labs['days_to_cohort_start'] >= num_days_prediction]\n",
    "all_labs['concept_name'] = all_labs['concept_name'].astype(str) + '_lab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7151c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_merge = all_conds[['person_id','condition_start_date']].drop_duplicates()\n",
    "conds_merge.rename({'condition_start_date':'start_date'}, axis=1, inplace=True)\n",
    "\n",
    "meds_merge = all_meds[['person_id','drug_era_start_date']].drop_duplicates()\n",
    "meds_merge.rename({'drug_era_start_date':'start_date'}, axis=1, inplace=True)\n",
    "\n",
    "procedures_merge = all_procedures[['person_id','procedure_date']].drop_duplicates()\n",
    "procedures_merge.rename({'procedure_date':'start_date'}, axis=1, inplace=True)\n",
    "\n",
    "labs_merge = all_labs[['person_id','measurement_date']].drop_duplicates()\n",
    "procedures_merge.rename({'measurement_date':'start_date'}, axis=1, inplace=True)\n",
    "\n",
    "visits_merge = all_visits[['person_id','visit_start_date']].drop_duplicates()\n",
    "visits_merge.rename({'visit_start_date':'start_date'}, axis=1, inplace=True)\n",
    "\n",
    "all_merge = pd.concat([conds_merge, meds_merge, procedures_merge, visits_merge]).drop_duplicates()\n",
    "\n",
    "calc_cutoff_date = df_pop.merge(all_merge, how = 'left', on = 'person_id')\n",
    "calc_cutoff_date['time_since_psychosis'] = (calc_cutoff_date['start_date']-calc_cutoff_date['psychosis_diagnosis_date']).dt.days\n",
    "\n",
    "max_calc_cutoff_date = calc_cutoff_date.groupby('person_id').max()\n",
    "print('50th percentile for max time after psychosis',np.percentile(max_calc_cutoff_date['time_since_psychosis'], 75))\n",
    "print('90th percentile for max time after psychosis',np.percentile(max_calc_cutoff_date['time_since_psychosis'], 90))\n",
    "print('95th percentile for max time after psychosis',np.percentile(max_calc_cutoff_date['time_since_psychosis'], 95))\n",
    "\n",
    "min_calc_cutoff_date = calc_cutoff_date.groupby('person_id').min()\n",
    "print('50th percentile for max time before psychosis',np.percentile(min_calc_cutoff_date['time_since_psychosis'], 25))\n",
    "print('90th percentile for max time before psychosis',np.percentile(min_calc_cutoff_date['time_since_psychosis'], 10))\n",
    "print('95th percentile for max time before psychosis',np.percentile(min_calc_cutoff_date['time_since_psychosis'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove things that happen more than 31 iterations after \n",
    "days_per_iter = 120\n",
    "iteration_cutoff = 1+np.ceil(np.percentile(max_calc_cutoff_date['time_since_psychosis'], 95)/days_per_iter)\n",
    "print(iteration_cutoff)\n",
    "\n",
    "df_pop['censor_date'] = df_pop['cohort_start_date']-pd.Timedelta(90, 'days')\n",
    "print((df_pop['censor_date']-df_pop['psychosis_diagnosis_date']).dt.days.max()/120)\n",
    "df_pop.loc[(df_pop['censor_date']-df_pop['psychosis_diagnosis_date']).dt.days > iteration_cutoff*days_per_iter, 'censor_date'] = (df_pop.loc[(df_pop['censor_date']-df_pop['psychosis_diagnosis_date']).dt.days > iteration_cutoff*days_per_iter, 'psychosis_diagnosis_date'] + pd.Timedelta(iteration_cutoff*days_per_iter, 'days'))\n",
    "print((df_pop['censor_date']-df_pop['psychosis_diagnosis_date']).dt.days.max()/120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = all_visits.merge(df_pop[['person_id', 'censor_date']], how='inner', on='person_id')\n",
    "all_conds = all_conds.merge(df_pop[['person_id', 'censor_date']], how='inner', on='person_id')\n",
    "all_procedures = all_procedures.merge(df_pop[['person_id', 'censor_date']], how='inner', on='person_id')\n",
    "all_labs = all_labs.merge(df_pop[['person_id', 'censor_date']], how='inner', on='person_id')\n",
    "all_meds = all_meds.merge(df_pop[['person_id', 'censor_date']], how='inner', on='person_id')\n",
    "\n",
    "print(len(all_visits))\n",
    "all_visits = all_visits.loc[all_visits['visit_start_date'] <  all_visits['censor_date']]\n",
    "print(len(all_visits))\n",
    "\n",
    "print(len(all_conds))\n",
    "all_conds = all_conds.loc[all_conds['condition_start_date'] <  all_conds['censor_date']]\n",
    "print(len(all_conds))\n",
    "\n",
    "print(len(all_procedures))\n",
    "all_procedures = all_procedures.loc[all_procedures['procedure_date'] <  all_procedures['censor_date']]\n",
    "print(len(all_procedures))\n",
    "\n",
    "print(len(all_labs))\n",
    "all_labs = all_labs.loc[all_labs['measurement_date'] <  all_labs['censor_date']]\n",
    "print(len(all_labs))\n",
    "\n",
    "print(len(all_meds))\n",
    "all_meds = all_meds.loc[all_meds['drug_era_start_date'] <  all_meds['censor_date']]\n",
    "print(len(all_meds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9ee2d",
   "metadata": {},
   "source": [
    "### Delete Rare Features: anything that does not occur in at least 1% of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3dd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ccae_validation_set == False:\n",
    "    def drop_rare_occurrences(df, col_concept, col_id = 'person_id', size_pop = len(df_pop)):\n",
    "        print(len(df))\n",
    "        unique_occurrences = df[['person_id', col_concept]].drop_duplicates()\n",
    "        unique_occurrences = unique_occurrences.value_counts(col_concept)\n",
    "        common_occurrences = unique_occurrences[unique_occurrences/size_pop > 0.01].index\n",
    "        return df.loc[df[col_concept].isin(common_occurrences)]\n",
    "    all_conds = drop_rare_occurrences(all_conds, 'rolled_concept_name')\n",
    "    print(len(all_conds))\n",
    "    all_meds = drop_rare_occurrences(all_meds, 'rolled_concept_name')\n",
    "    print(len(all_meds))\n",
    "    all_procedures = drop_rare_occurrences(all_procedures, 'rolled_concept_name')\n",
    "    print(len(all_procedures))\n",
    "    all_labs = drop_rare_occurrences(all_labs, 'measurement_concept_id')\n",
    "    print(len(all_labs))\n",
    "    all_visits = drop_rare_occurrences(all_visits, 'visit_concept_id')\n",
    "    print(len(all_visits))\n",
    "else:\n",
    "    def drop_unshared_features(df, col_concept, list_cols):\n",
    "        print(len(df))\n",
    "        df = df.loc[df[col_concept].isin(list_cols)]\n",
    "        print(len(df))\n",
    "        return df\n",
    "    \n",
    "    with open(path + \"raw_data_3yrs/intermediate_data_mdcd/MDCD_12_1_dl_colnames_snomed\", \"rb\") as fp:   #Pickling\n",
    "        list_mdcd_cols = pickle.load(fp)\n",
    "    all_conds = drop_unshared_features(all_conds, 'rolled_concept_name', list_mdcd_cols)\n",
    "    all_meds = drop_unshared_features(all_meds, 'rolled_concept_name', list_mdcd_cols)\n",
    "    all_procedures = drop_unshared_features(all_procedures, 'rolled_concept_name', list_mdcd_cols)\n",
    "    all_labs = drop_unshared_features(all_labs, 'concept_name', list_mdcd_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2897e",
   "metadata": {},
   "source": [
    "### Check for Data Leakage: \n",
    "Minimum times should be at least 90 days and cohort start date should be same across all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac569",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = (all_labs['cohort_start_date']-all_labs['measurement_date']).dt.days\n",
    "print('Labs:', check.min(), check.max())\n",
    "\n",
    "check = (all_procedures['cohort_start_date']-all_procedures['procedure_date']).dt.days\n",
    "print('Procedures:', check.min(), check.max())\n",
    "\n",
    "check = (all_conds['cohort_start_date']-all_conds['condition_start_date']).dt.days\n",
    "print('Conditions:', check.min(), check.max())\n",
    "\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days\n",
    "print('Meds (Start of prescription):', check.min(), check.max())\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days\n",
    "print('Meds (End of prescription):', check.min(), check.max())\n",
    "\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days\n",
    "print('Visits (Start of visit):', check.min(), check.max())\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days\n",
    "print('Visits (End of visit):', check.min(), check.max())\n",
    "\n",
    "print('Check presence of SCZ:',len(all_conds.loc[all_conds['concept_name'].isin(['Schizophrenia', 'Paranoid schizophrenia'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e13e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cohort_start = df_pop[['person_id','cohort_start_date']]\n",
    "check_cohort_start = check_cohort_start.merge(all_conds[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_cohort_start = check_cohort_start.merge(all_visits[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_cohort_start = check_cohort_start.merge(all_procedures[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old2','_pro'])\n",
    "check_cohort_start = check_cohort_start.merge(all_labs[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old3','_labs'])\n",
    "check_cohort_start = check_cohort_start.merge(all_meds[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_cohort_start.set_index('person_id',inplace=True)\n",
    "check_cohort_start = check_cohort_start.T\n",
    "num_unique = check_cohort_start.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where cohort start date doesnt align:',(num_unique>1).sum())\n",
    "\n",
    "check_censor = df_pop[['person_id','censor_date']]\n",
    "check_censor = check_censor.merge(all_conds[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_censor = check_censor.merge(all_visits[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_censor = check_censor.merge(all_procedures[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old2','_pro'])\n",
    "check_censor = check_censor.merge(all_labs[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old3','_labs'])\n",
    "check_censor = check_censor.merge(all_meds[['person_id','censor_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_censor.set_index('person_id',inplace=True)\n",
    "check_censor = check_censor.T\n",
    "num_unique = check_censor.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where censor date doesnt align:',(num_unique>1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate codes\n",
    "print('Conds', 'Unnamed: 0' not in all_conds.columns, len(all_conds) == len(all_conds[['person_id', 'rolled_concept_name', 'condition_start_date', 'condition_occurrence_id']].drop_duplicates()))\n",
    "print('Meds', 'Unnamed: 0' not in all_meds.columns, len(all_meds) == len(all_meds[['person_id', 'rolled_concept_name', 'drug_concept_id', 'drug_era_start_date', 'drug_era_end_date']].drop_duplicates()))\n",
    "print('Visits', 'Unnamed: 0' not in all_visits.columns, len(all_visits) == len(all_visits['visit_occurrence_id'].unique()))\n",
    "print('Procedures', 'Unnamed: 0' not in all_procedures.columns, len(all_procedures) == len(all_procedures[['person_id','rolled_concept_name', 'procedure_concept_id', 'procedure_date', 'procedure_occurrence_id']].drop_duplicates()))\n",
    "print('Labs', 'Unnamed: 0' not in all_labs.columns, len(all_labs) == len(all_labs[['person_id', 'measurement_concept_id', 'measurement_date', 'measurement_id']].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Conds', len(all_conds['rolled_concept_name'].unique()))\n",
    "print('Meds', len(all_meds['rolled_concept_name'].unique()))\n",
    "print('Procedures', len(all_procedures['rolled_concept_name'].unique()))\n",
    "print('Labs', len(all_labs['concept_name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ba5b3",
   "metadata": {},
   "source": [
    "### Make SQL queries for inpatient psych visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPATIENT PSYCH VISITS\n",
    "query = (\"SELECT vo.person_id, vo.visit_occurrence_id, vo.visit_concept_id, co.condition_start_date, vo.visit_start_date, vo.visit_end_date, co.condition_concept_id, c.concept_name as condition_name, p.race_concept_id, p.gender_concept_id \"+\n",
    "         \"FROM dbo.visit_occurrence as vo LEFT JOIN dbo.condition_occurrence as co on co.visit_occurrence_id = vo.visit_occurrence_id \"+\n",
    "         \"LEFT JOIN dbo.concept as c on c.concept_id = co.condition_concept_id \"+\n",
    "         \"LEFT JOIN dbo.person as p on p.person_id = vo.person_id \"+\n",
    "         \"WHERE vo.visit_concept_id = 9201 AND condition_concept_id IN \"+\n",
    "         \"(SELECT DISTINCT concept_id_2 FROM dbo.concept as c LEFT JOIN dbo.concept_relationship on concept_id_1 = concept_id WHERE c.concept_code LIKE 'F%' AND c.vocabulary_id = 'ICD10CM' AND relationship_id = 'Maps to')\")\n",
    "\n",
    "psych_hosp = pd.io.sql.read_sql(query, conn)\n",
    "list_psych_visits = list(psych_hosp['visit_occurrence_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15380d59",
   "metadata": {},
   "source": [
    "# Function for processing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp pop needs to have a \"years obs\" column\n",
    "def make_static_df(temp_pop, temp_conds, temp_meds, temp_visits, temp_procedures, temp_labs):\n",
    "    ### CONDITIONS\n",
    "    conditions_features = temp_conds.pivot_table(index='person_id', columns='rolled_concept_name', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    # get conditions per year\n",
    "    conditions_features = conditions_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    conditions_features = conditions_features.div(conditions_features.years_obs, axis=0) \n",
    "    conditions_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    ### MEDICATIONS\n",
    "    temp_meds['drug_exposure_days'] = (temp_meds['drug_era_end_date']-temp_meds['drug_era_start_date']).dt.days + 1 # +1 so a 1-day prescription will not be 0 days\n",
    "    count_meds = temp_meds[['person_id', 'rolled_concept_name', 'drug_exposure_days']].groupby(['person_id', 'rolled_concept_name']).sum().reset_index()\n",
    "    meds_features = count_meds.pivot_table(index='person_id', columns='rolled_concept_name', values='drug_exposure_days', fill_value=0)\n",
    "    \n",
    "    # get medications per year\n",
    "    meds_features = meds_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    meds_features = meds_features.div(meds_features.years_obs, axis=0) \n",
    "    meds_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "\n",
    "    ###VISITS    \n",
    "    # Number of visits\n",
    "    num_visits = temp_visits.groupby(['person_id', 'visit_concept_id']).count()['cohort_start_date'].reset_index()\n",
    "    num_visits = num_visits.pivot_table(index='person_id', columns = 'visit_concept_id', values = 'cohort_start_date', fill_value=0)\n",
    "    num_visits_columns = [str(i)+'_num_visits' for i in num_visits.columns]\n",
    "    num_visits.columns = num_visits_columns\n",
    "    # adjust so that num_visits is per year of observation\n",
    "    num_visits = num_visits.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    num_visits = num_visits.div(num_visits.years_obs, axis=0) \n",
    "    num_visits.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    # Length of Stay\n",
    "    non_outpatient = temp_visits.loc[temp_visits['visit_concept_id']!=9202]\n",
    "\n",
    "    non_outpatient['los'] = (non_outpatient['visit_end_date']-non_outpatient['visit_start_date']).dt.days\n",
    "    los = non_outpatient.groupby(['person_id', 'visit_concept_id']).agg({'los':['sum', 'max', 'min', 'mean']})\n",
    "    los = los.reset_index()\n",
    "    los.columns = [' '.join(col).strip() for col in los.columns.values]\n",
    "\n",
    "    los = los.pivot_table(index='person_id', columns = 'visit_concept_id', values=['los sum', 'los max', 'los min', 'los mean'], fill_value = 0)\n",
    "    los.columns = [''.join(str(col)).strip() for col in los.columns.values]\n",
    "\n",
    "    visits_features = num_visits.merge(los, how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    #### VISITS: INPATIENT HOSPITALIZATIONS\n",
    "    # limit psych hospitalizations to ones eligible (according to preprocessed visits df)\n",
    "    psych_hospitalizations = temp_visits.loc[temp_visits['visit_occurrence_id'].isin(list_psych_visits)]\n",
    "    \n",
    "    # Number of visits\n",
    "    num_visits = psych_hospitalizations.groupby('person_id').count()['cohort_start_date'].reset_index()\n",
    "    num_visits.rename({'cohort_start_date':'num_psych_hospitalizations'}, inplace=True, axis=1)\n",
    "    num_visits.set_index('person_id', inplace=True)\n",
    "    # adjust so that num_visits is per year of observation\n",
    "    num_visits = num_visits.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    num_visits = num_visits.div(num_visits.years_obs, axis=0) \n",
    "    num_visits.drop(['years_obs'], axis=1, inplace=True)\n",
    "\n",
    "    visits_features = visits_features.merge(num_visits, how = 'left', right_index=True, left_index=True).fillna(0)\n",
    "\n",
    "    # Length of Stay\n",
    "    temp_visits['los'] = (temp_visits['visit_end_date']-temp_visits['visit_start_date']).dt.days\n",
    "    los = temp_visits.groupby(['person_id']).agg({'los':['sum', 'max', 'min', 'mean']})\n",
    "    los.columns = [' '.join(col).strip() for col in los.columns.values]\n",
    "    los.columns = ['los psych sum', 'los psych max', 'los psych min', 'los psych mean']\n",
    "\n",
    "    visits_features = visits_features.merge(los, how = 'left', right_index=True, left_index=True).fillna(0)\n",
    "\n",
    "    ### PROCEDURES\n",
    "    procedures_features = temp_procedures.pivot_table(index='person_id', columns='rolled_concept_name', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    # get procedures per year\n",
    "    procedures_features = procedures_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    procedures_features = procedures_features.div(procedures_features.years_obs, axis=0) \n",
    "    procedures_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    ### LABS\n",
    "    lab_features = temp_labs.pivot_table(index='person_id', columns='concept_name', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # get labs per year\n",
    "    lab_features = lab_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    lab_features = lab_features.div(lab_features.years_obs, axis=0) \n",
    "    lab_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    atemporal_features = pd.concat([conditions_features, meds_features, procedures_features, lab_features, visits_features], axis=1)\n",
    "    return atemporal_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965c2bc",
   "metadata": {},
   "source": [
    "### Create the \"iterative\" population dataframe: \n",
    "- Start at the date of initial psychosis diagnosis, then go every XX days (120 days), cutting off at the censor date (if you go over the censor date, chop to the censor date).\n",
    "- Practically, this means that start date 1 is first visit & end date 1 is psychosis; then start date 2 is psychosis dx date and end date 2 is psychosis dx + 90... \n",
    "- Then, starting at the date of psychosis, go back in XX-day increments for 3 years (9 iterations). The earliest iteration (furthest away from psychosis date) should consist of all prior data, and if a person has less than 3 years of data pre-psychosis, they should have fewer early visits.\n",
    "- Note that start dates are inclusive and end dates are exclusive\n",
    "- **KEEP IN MIND FOR LATER: WE WANT TO PAD AT THE BEGINNING, NOT AT THE END. So then we move each person to be aligned at the end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e034064",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((df_pop['censor_date']-df_pop['psychosis_diagnosis_date']).dt.days/days_per_iter).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = df_pop[['person_id', 'first_visit', 'cohort_start_date', 'psychosis_diagnosis_date', 'censor_date']]\n",
    "\n",
    "df_pop['0_start'] = df_pop['psychosis_diagnosis_date']\n",
    "df_pop['0_end'] = df_pop['psychosis_diagnosis_date'] + pd.Timedelta(days_per_iter, 'days')\n",
    "df_pop.loc[df_pop['0_end']>df_pop['censor_date'], '0_end'] = df_pop.loc[df_pop['0_end']>df_pop['censor_date'], 'censor_date']\n",
    "\n",
    "# after the loops, remove people for whom 0_start-0_end > 0\n",
    "\n",
    "# FORWARD LOOP: starting at psychosis dx, every XX days till censor date\n",
    "for count in range(1, 1+int(np.ceil(((df_pop['censor_date']-df_pop['psychosis_diagnosis_date']).dt.days/days_per_iter).max()))): \n",
    "    # get the start date as the same day as prev end date and the end date as start + 120 days\n",
    "    df_pop[str(count)+'_start'] = df_pop[str(count-1)+'_end']\n",
    "    df_pop[str(count)+'_end'] = df_pop[str(count)+'_start'] + pd.Timedelta(days_per_iter, 'days')\n",
    "    \n",
    "    # update start/end dates to make sure it is at max, the censor date\n",
    "    df_pop.loc[df_pop[str(count)+'_start'] > df_pop['censor_date'], str(count)+'_start'] = df_pop.loc[df_pop[str(count)+'_start'] > df_pop['censor_date'], 'censor_date']\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] > df_pop['censor_date'], str(count)+'_end'] = df_pop.loc[df_pop[str(count)+'_end'] > df_pop['censor_date'], 'censor_date']\n",
    "    \n",
    "    # if start date == censor date: set start and end to NaT\n",
    "    df_pop.loc[df_pop[str(count)+'_start'] == df_pop['censor_date'], [str(count)+'_start', str(count)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')]\n",
    "\n",
    "for count in np.arange(-1, -10, -1):\n",
    "    df_pop[str(count)+'_end'] = df_pop[str(count+1)+'_start']\n",
    "    df_pop[str(count)+'_start'] = df_pop[str(count)+'_end']-pd.Timedelta(days_per_iter, 'days')\n",
    "    \n",
    "    # if the visit starts or ends prior to first_visit, set start/end to first_visit\n",
    "    df_pop.loc[df_pop[str(count)+'_start']<df_pop['first_visit'], str(count)+'_start'] = df_pop.loc[df_pop[str(count)+'_start']<df_pop['first_visit'], 'first_visit']\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] < df_pop['first_visit'], str(count)+'_end'] = df_pop.loc[df_pop[str(count)+'_end'] < df_pop['first_visit'], 'first_visit']\n",
    "    \n",
    "    # if end date == first visit date: set start and end to NaT\n",
    "    df_pop.loc[df_pop[str(count)+'_end'] == df_pop['first_visit'], [str(count)+'_start', str(count)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')]\n",
    "    \n",
    "    df_pop[str(count)+'_end'] = pd.to_datetime(df_pop[str(count)+'_end'], format = '%Y-%m-%d')\n",
    "\n",
    "# set iteration -10 so that the end date is -9 start and (if not NaT) the start date is first visit\n",
    "df_pop['-10_end'] = df_pop['-9_start']\n",
    "df_pop['-10_start'] = np.datetime64('NaT')\n",
    "df_pop.loc[~(df_pop['-10_end'].isna()), '-10_start'] = df_pop.loc[~(df_pop['-10_end'].isna()), 'first_visit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdb403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop.columns[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57181d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all \"timesteps\" are at least 1 day\n",
    "for i in np.arange(-10, 32):\n",
    "    df_pop.loc[df_pop[str(i)+'_end']==df_pop[str(i)+'_start'], [str(i)+'_start', str(i)+'_end']] = [np.datetime64('NaT'), np.datetime64('NaT')] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594569",
   "metadata": {},
   "source": [
    "### Loop through the iterative population dataframe to get the features for each person in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635978c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_feature_dfs = []\n",
    "\n",
    "for iteration in np.arange(-10, 32): \n",
    "    temp_df_iter_pop = df_pop.copy()\n",
    "    temp_df_iter_pop['iter_start_date'] = temp_df_iter_pop[str(iteration)+'_start']\n",
    "    temp_df_iter_pop['iter_end_date'] = temp_df_iter_pop[str(iteration)+'_end']\n",
    "    \n",
    "    # constrict to people with a valid iteration\n",
    "    temp_df_iter_pop = temp_df_iter_pop.loc[~(temp_df_iter_pop['iter_start_date'].isna())]\n",
    "    temp_df_iter_pop['years_obs'] = (temp_df_iter_pop['iter_end_date']-temp_df_iter_pop['iter_start_date']).dt.days/365\n",
    "    \n",
    "    # for conditions, labs, procedures, just compare the start_date to the cutoff date\n",
    "    temp_conds = all_conds.loc[all_conds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_conds = temp_conds.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_conds = temp_conds.loc[temp_conds['condition_start_date']>= temp_conds['iter_start_date']]\n",
    "    temp_conds = temp_conds.loc[temp_conds['condition_start_date']< temp_conds['iter_end_date']]\n",
    "\n",
    "    temp_labs = all_labs.loc[all_labs['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_labs = temp_labs.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_labs = temp_labs.loc[temp_labs['measurement_date']>= temp_labs['iter_start_date']]\n",
    "    temp_labs = temp_labs.loc[temp_labs['measurement_date']< temp_labs['iter_end_date']]\n",
    "    \n",
    "    temp_procedures = all_procedures.loc[all_procedures['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_procedures = temp_procedures.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_procedures = temp_procedures.loc[temp_procedures['procedure_date']>= temp_procedures['iter_start_date']]\n",
    "    temp_procedures = temp_procedures.loc[temp_procedures['procedure_date']< temp_procedures['iter_end_date']]\n",
    "    \n",
    "    \n",
    "    # note: for meds and visits, replace iter_end_date with equal_end_date, which is the \n",
    "    # day before the actual end date since is the last day that we are allowing the visit to \"equal\"\n",
    "    # ie visit < iter_end_date but visit <= equal_end_date\n",
    "    \n",
    "    \n",
    "    #for medications and visits, we want to look at \n",
    "    #1. med start date needs to be before iteration end date\n",
    "    #2. med end date needs to be on or after iteration start date\n",
    "    \n",
    "    #3. if med start date is before iteration start date -- make med start date iteration start date\n",
    "    #4. if med end date is after iteration end date -- make med end date iteration end date\n",
    "        \n",
    "    temp_meds = all_meds.loc[all_meds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_meds = temp_meds.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_meds['equal_end_date'] = temp_meds['iter_end_date']-pd.Timedelta(1, 'days')\n",
    "    \n",
    "    temp_meds = temp_meds.loc[(temp_meds['drug_era_start_date']<temp_meds['iter_end_date'])&(temp_meds['drug_era_end_date']>=temp_meds['iter_start_date'])]\n",
    "    temp_meds.loc[temp_meds['drug_era_start_date']<temp_meds['iter_start_date'], 'drug_era_start_date'] = temp_meds.loc[temp_meds['drug_era_start_date']<temp_meds['iter_start_date'], 'iter_start_date']\n",
    "    temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['equal_end_date'], 'drug_era_end_date'] = temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['equal_end_date'], 'equal_end_date']\n",
    "\n",
    "    temp_meds['days_to_cohort_start'] = (temp_meds['cohort_start_date']-temp_meds['drug_era_start_date']).dt.days\n",
    "    \n",
    "    # Repeat for visits\n",
    "    temp_visits = all_visits.loc[all_visits['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_visits = temp_visits.merge(temp_df_iter_pop[['person_id','iter_start_date', 'iter_end_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_visits['equal_end_date'] = temp_visits['iter_end_date']-pd.Timedelta(1, 'days')\n",
    "    \n",
    "    temp_visits = temp_visits.loc[(temp_visits['visit_start_date']<temp_visits['iter_end_date'])&(temp_visits['visit_end_date']>=temp_visits['iter_start_date'])]\n",
    "    temp_visits.loc[temp_visits['visit_start_date']<temp_visits['iter_start_date'], 'visit_start_date'] = temp_visits.loc[temp_visits['visit_start_date']<temp_visits['iter_start_date'], 'iter_start_date']\n",
    "    temp_visits.loc[temp_visits['visit_end_date']>temp_visits['equal_end_date'], 'visit_end_date'] = temp_visits.loc[temp_visits['visit_end_date']>temp_visits['equal_end_date'], 'equal_end_date']\n",
    "    \n",
    "    temp_visits['days_to_cohort_start'] = (temp_visits['cohort_start_date']-temp_visits['visit_start_date']).dt.days\n",
    "\n",
    "    \n",
    "    if len(temp_conds.loc[temp_conds['condition_start_date']>=temp_conds['iter_end_date']])+len(temp_conds.loc[temp_conds['condition_start_date']<temp_conds['iter_start_date']]) > 0:\n",
    "        print('Leakage in conds')        \n",
    "    if len(temp_labs.loc[temp_labs['measurement_date']>=temp_labs['iter_end_date']])+len(temp_labs.loc[temp_labs['measurement_date']<temp_labs['iter_start_date']]) > 0:\n",
    "        print('Leakage in labs')\n",
    "    if len(temp_procedures.loc[temp_procedures['procedure_date']>=temp_procedures['iter_end_date']])+len(temp_procedures.loc[temp_procedures['procedure_date']<temp_procedures['iter_start_date']]) > 0:\n",
    "        print('Leakage in procedures')\n",
    "        \n",
    "    if len(temp_meds.loc[temp_meds['drug_era_start_date']>temp_meds['iter_end_date']])+len(temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['iter_end_date']]) > 0:\n",
    "        print('Leakage in med ends')\n",
    "    if len(temp_meds.loc[temp_meds['drug_era_end_date']<temp_meds['iter_start_date']])+len(temp_meds.loc[temp_meds['drug_era_start_date']<temp_meds['iter_start_date']]) > 0:\n",
    "        print('Leakage in med starts')\n",
    "        \n",
    "    if len(temp_visits.loc[temp_visits['visit_start_date']>temp_visits['iter_end_date']])+len(temp_visits.loc[temp_visits['visit_end_date']>temp_visits['iter_end_date']]) > 0:\n",
    "        print('Leakage in visit ends')\n",
    "    if len(temp_visits.loc[temp_visits['visit_end_date']<temp_visits['iter_start_date']])+len(temp_visits.loc[temp_visits['visit_start_date']<temp_visits['iter_start_date']]) > 0:\n",
    "        print('Leakage in visit starts')\n",
    "\n",
    "    all_features = make_static_df(temp_df_iter_pop, temp_conds, temp_meds, temp_visits, temp_procedures, temp_labs)\n",
    "    all_features['iteration'] = iteration\n",
    "    \n",
    "    # add: time since psychosis = iter_start_date-psychosis_diagnosis_date\n",
    "    if iteration > 0:\n",
    "        temp_df_iter_pop['iter_start_date'] = pd.to_datetime(temp_df_iter_pop['iter_start_date'], format='mixed', dayfirst = False)\n",
    "        temp_df_iter_pop['psychosis_diagnosis_date'] = pd.to_datetime(temp_df_iter_pop['psychosis_diagnosis_date'], format='mixed', dayfirst = False)\n",
    "        temp_df_iter_pop['time_since_psychosis'] = (temp_df_iter_pop['iter_start_date']-temp_df_iter_pop['psychosis_diagnosis_date']).dt.days/365\n",
    "        all_features = all_features.merge(temp_df_iter_pop[['person_id', 'time_since_psychosis']], how='inner', left_index=True, right_on = 'person_id')\n",
    "    else: \n",
    "        all_features['time_since_psychosis'] = 0\n",
    "        all_features.reset_index(inplace=True)\n",
    "    \n",
    "    \n",
    "    list_feature_dfs.append(all_features)\n",
    "    print(iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters = pd.concat(list_feature_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_all_iters.loc[df_all_iters['person_id'].isna()]))\n",
    "print(len(df_all_iters.loc[df_all_iters['person_id']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ccae_validation_set == True:\n",
    "    missing_cols = list(set(list_mdcd_cols).difference(df_all_iters.columns))\n",
    "    df_all_iters.loc[:,missing_cols] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07842522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove correlated features: cumc\n",
    "if remove_corr_features == True:\n",
    "    def remove_highcorr_cols(threshold, list_colnames, df_corr):\n",
    "        np.random.seed(43)\n",
    "        upper_tri_corr = pd.DataFrame(index = df_corr.index, columns = df_corr.columns, data = np.triu(df_corr.values))\n",
    "        melted_corr = df_corr.melt(ignore_index=False).reset_index()\n",
    "        melted_corr = melted_corr.loc[melted_corr['index']!= melted_corr['variable']]\n",
    "        melted_corr = melted_corr.loc[melted_corr['value']>=threshold]\n",
    "        melted_corr.sort_values('value', ascending=False, inplace=True)\n",
    "\n",
    "        feats_to_remove = []\n",
    "        while(len(melted_corr)) > 0:\n",
    "            ind = melted_corr.iloc[0]['index']\n",
    "            var = melted_corr.iloc[0]['variable']\n",
    "            num_index = len(melted_corr.loc[melted_corr['index']==ind]) + len(melted_corr.loc[melted_corr['variable']==ind])\n",
    "            num_variable = len(melted_corr.loc[melted_corr['index']==var]) + len(melted_corr.loc[melted_corr['variable']==var])\n",
    "\n",
    "            if num_index == num_variable:\n",
    "                # remove a random variable\n",
    "                rand_choice = (np.random.randint(0,2))\n",
    "                if rand_choice == 0: \n",
    "                    remove_var = ind\n",
    "                else: \n",
    "                    remove_var = var\n",
    "            elif num_index < num_variable:\n",
    "                # remove the \"column\"\n",
    "                remove_var = var\n",
    "            elif num_index > num_variable:\n",
    "                # remove the \"index\"\n",
    "                remove_var = ind\n",
    "\n",
    "            # remove remove_var\n",
    "            feats_to_remove.append(remove_var)\n",
    "            melted_corr = melted_corr.loc[~(melted_corr['index'] == remove_var)]\n",
    "            melted_corr = melted_corr.loc[~(melted_corr['variable'] == remove_var)]\n",
    "\n",
    "        colnames_copy = list(list_colnames.copy())\n",
    "        for i in feats_to_remove:\n",
    "            colnames_copy.remove(i)\n",
    "        return colnames_copy\n",
    "\n",
    "    corr_df = df_all_iters.corr()\n",
    "    corr_df.drop(['person_id', 'iteration'], axis=1, inplace=True)\n",
    "    corr_df.drop(['person_id', 'iteration'], axis=0, inplace=True)\n",
    "\n",
    "    print(len(corr_df.columns))\n",
    "    reduced_cols_1 = remove_highcorr_cols(1, corr_df.columns, corr_df)\n",
    "    print(len(reduced_cols_1))\n",
    "    corr_df = corr_df.loc[reduced_cols_1, reduced_cols_1]\n",
    "    reduced_cols_2 = remove_highcorr_cols(0.95, corr_df.columns, corr_df)\n",
    "    print(len(reduced_cols_2))\n",
    "    \n",
    "    df_all_iters = df_all_iters.loc[:, ['person_id', 'iteration']+reduced_cols_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all iterations between min(iteration) and max(iteration) exist, and if not that they are filled in with 0s\n",
    "all_rows = []\n",
    "for pid, group in df_all_iters.groupby('person_id'):\n",
    "    min_iter, max_iter = group['iteration'].min(), group['iteration'].max()\n",
    "    all_iters = pd.DataFrame({'person_id': pid, 'iteration': range(min_iter, max_iter + 1)})\n",
    "    all_rows.append(all_iters)\n",
    "\n",
    "# Combine all possible iterations\n",
    "fillin_iters = pd.concat(all_rows, ignore_index=True)\n",
    "print(len(df_all_iters))\n",
    "df_all_iters = df_all_iters.merge(fillin_iters, on=['person_id', 'iteration'], how='right').fillna(0)\n",
    "print(len(df_all_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters['time_since_psychosis'] = df_all_iters['iteration'] * days_per_iter/365\n",
    "df_all_iters.loc[df_all_iters['iteration'] <= 0, 'time_since_psychosis'] = 0\n",
    "(df_all_iters['iteration']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that time_since_psychosis is correct\n",
    "print('Pre-psychosis tsp', df_all_iters.loc[df_all_iters['iteration']<=0, 'time_since_psychosis'].unique())\n",
    "print('Post-psychosis tsp', df_all_iters.loc[df_all_iters['iteration']>0, 'time_since_psychosis'].unique())\n",
    "print(len(df_all_iters.loc[df_all_iters['person_id'].isna()]))\n",
    "print(len(df_all_iters.loc[df_all_iters['person_id']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_iters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters.to_csv(int_path + 'CUMC_1_27_dl_data_snomed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a458c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99936780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01633f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
