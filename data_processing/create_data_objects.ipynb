{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import sys\n",
    "import gc\n",
    "from scipy.sparse import *\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle \n",
    "import random\n",
    "import math\n",
    "from joblib import dump, load\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_cols = True\n",
    "\n",
    "bw_only = False\n",
    "bm_only = False\n",
    "wm_only = False\n",
    "mf_only = False\n",
    "\n",
    "make_scaler = True\n",
    "demo_aware = True\n",
    "keep_iters = None # [-10, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters = pd.read_csv(int_path + 'CUMC_1_27_dl_data_snomed.csv')\n",
    "if 'Unnamed: 0' in df_all_iters.columns:\n",
    "    print('removing unnamed column')\n",
    "    df_all_iters.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "if keep_iters is not None:\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['iteration']>=keep_iters[0]]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['iteration']<=keep_iters[1]]\n",
    "    print('Check keep iters', df_all_iters['iteration'].min(), df_all_iters['iteration'].max())\n",
    "\n",
    "num_days_prediction = 90\n",
    "df_pop = pd.read_csv(raw_path + 'population.csv')\n",
    "df_pop.rename({'psychosis_dx_date':'psychosis_diagnosis_date'}, axis=1, inplace=True)\n",
    "df_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_pop['psychosis_diagnosis_date'], format=\"mixed\", dayfirst = False)\n",
    "df_pop['cohort_start_date'] = pd.to_datetime(df_pop['cohort_start_date'], format=\"mixed\", dayfirst = False)\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]\n",
    "    \n",
    "print(len(df_all_iters))\n",
    "if bw_only == True:\n",
    "    # LIMIT TO Black and White patients ONLY\n",
    "    df_pop = df_pop.loc[df_pop['race_concept_id'].isin([8516, 8527])]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "    \n",
    "if bm_only == True:\n",
    "    # LIMIT TO non-White patients ONLY\n",
    "    df_pop = df_pop.loc[~(df_pop['race_concept_id'].isin([8527]))]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "\n",
    "if wm_only == True:\n",
    "    # LIMIT TO non-Black patients ONLY\n",
    "    df_pop = df_pop.loc[~(df_pop['race_concept_id'].isin([8516]))]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "    \n",
    "if mf_only == True:\n",
    "    df_pop = df_pop.loc[df_pop['gender_concept_id'].isin([8532, 8507])]\n",
    "    df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(df_pop['person_id'])]\n",
    "print(len(df_all_iters))    \n",
    "df_all_iters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ranked iteration\n",
    "ranked_vals = df_all_iters.reset_index().groupby('person_id')['iteration'].rank(method='first').values\n",
    "df_all_iters['ranked_iteration'] = ranked_vals\n",
    "print(df_all_iters['ranked_iteration'].max())\n",
    "\n",
    "# check that there is at most a difference of 1 between each pid from one iteration to the next\n",
    "def find_largest_diff(df):\n",
    "    # Sort by pid and iteration\n",
    "    df_sorted = df.sort_values(by=['person_id', 'iteration'])\n",
    "    \n",
    "    # Calculate the largest difference for each pid\n",
    "    result = df_sorted.groupby('person_id')['iteration'].apply(\n",
    "        lambda x: x.diff().max()\n",
    "    ).reset_index(name='largest_diff')\n",
    "    \n",
    "    return result\n",
    "find_largest_diff(df_all_iters)['largest_diff'].max() # should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_aware == True:\n",
    "    df_pop['is_White'] = 0\n",
    "    df_pop.loc[df_pop['race_concept_id']==8527, 'is_White'] = 1\n",
    "    df_pop['is_Black'] = 0\n",
    "    df_pop.loc[df_pop['race_concept_id']==8516, 'is_Black'] = 1\n",
    "    df_pop['is_Male'] = 0\n",
    "    df_pop.loc[df_pop['gender_concept_id']==8507, 'is_Male'] = 1\n",
    "    \n",
    "    print(len(df_all_iters))\n",
    "    df_all_iters = df_all_iters.merge(df_pop[['person_id', 'is_White', 'is_Black', 'is_Male']], how='inner', left_on = 'person_id', right_on='person_id')\n",
    "    print(len(df_all_iters))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = pd.read_csv(int_path + 'CUMC_1_27_tvt_split.csv')\n",
    "df_split = df_split.loc[df_split['person_id'].isin(df_all_iters['person_id'])]\n",
    "train_pids = list(df_split.loc[df_split['split']=='train', 'person_id'])\n",
    "val_pids = list(df_split.loc[df_split['split']=='val', 'person_id'])\n",
    "test_pids = list(df_split.loc[df_split['split']=='test', 'person_id'])\n",
    "print(len(train_pids)/len(df_split), len(val_pids)/len(df_split), len(test_pids)/len(df_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvt_split = {\n",
    "    \"train_pids\": tuple(train_pids),\n",
    "    \"val_pids\": tuple(val_pids),\n",
    "    \"test_pids\": tuple(test_pids)\n",
    "}\n",
    "\n",
    "with open(int_path + \"CUMC_2_16_dl_da_tvt_order.json\", \"w\") as f:\n",
    "    json.dump(tvt_split, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b39418",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_max = df_all_iters['ranked_iteration'].max()\n",
    "print(overall_max)\n",
    "df_all_iters.set_index(['person_id','ranked_iteration'], inplace=True)\n",
    "df_all_iters.drop('iteration', axis=1, inplace=True)\n",
    "df_all_iters.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "if shared_cols == False:\n",
    "    save_cols = list(df_all_iters.columns)\n",
    "\n",
    "    with open(int_path + \"CUMC_1_27_dl_da_colnames\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(save_cols, fp)\n",
    "else: \n",
    "    save_cols = load(int_path + 'CUMC_1_27_dl_da_colnames')\n",
    "\n",
    "print('Check for unnamed col (should be False):', 'Unnamed: 0' in save_cols)\n",
    "df_all_iters = df_all_iters[save_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_iters.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_all_iters.loc[train_pids]\n",
    "val_data = df_all_iters.loc[val_pids]\n",
    "test_data = df_all_iters.loc[test_pids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60789d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_scaler:\n",
    "    scaler = StandardScaler()\n",
    "    train_data_mat = scaler.fit_transform(train_data)\n",
    "    print('done with fit/first transform')\n",
    "    val_data_mat = scaler.transform(val_data)\n",
    "    test_data_mat = scaler.transform(test_data)\n",
    "\n",
    "    # save the standard scaler\n",
    "    dump(scaler, int_path + 'CUMC_1_27_dl_da_vanilla_order.bin', compress=True)    \n",
    "else:\n",
    "    scaler = load(path)\n",
    "    train_data_mat = scaler.transform(train_data)\n",
    "    val_data_mat = scaler.transform(val_data)\n",
    "    test_data_mat = scaler.transform(test_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbfb23",
   "metadata": {},
   "source": [
    "### Pad the data: add 0s to the beginning of each patient trajectory\n",
    "- Get the max timestep for each patient --> subtract from the max possible timestep\n",
    "- Add this max per patient to each iteration so that we now have \"backwards-aligned\" patient timesteps\n",
    "- Pad the earliest timesteps with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_df(original_df, scaled_df_mat, pids, overall_max=overall_max):\n",
    "    # get the maximum iterations per patient and subtract from the number of timesteps in the matrix \n",
    "    # for psychosis SCZ, that is 41\n",
    "    \n",
    "    save_cols = original_df.columns\n",
    "    original_df = original_df[original_df.columns[0:1]]\n",
    "    max_iter = original_df.reset_index().groupby('person_id')['ranked_iteration'].max()\n",
    "    max_iter.name = 'max_iter'\n",
    "    max_iter = overall_max-max_iter\n",
    "    \n",
    "    # add the number of padding rows that need to happen per patient to the dataframe\n",
    "    original_df = original_df.merge(max_iter, how='left', left_index=True, right_index=True)\n",
    "    original_df.reset_index(inplace=True)\n",
    "    original_df['ranked_iteration'] = original_df['ranked_iteration']+original_df['max_iter']\n",
    "    \n",
    "    # replace the data with the scaled data\n",
    "    original_df.set_index(['person_id', 'ranked_iteration'], inplace=True)\n",
    "    original_df.drop('max_iter', axis=1, inplace=True)\n",
    "    \n",
    "    # create a new dataframe that goes through each patient and each timestep\n",
    "    new_df = pd.DataFrame(index=[np.repeat(pids, overall_max), np.tile(np.arange(1, overall_max+1), len(pids))], columns=save_cols)\n",
    "    \n",
    "    # then fill it in with the existing data\n",
    "    new_df.loc[original_df.index] = scaled_df_mat\n",
    "    \n",
    "    # convert to matrix and fillna\n",
    "    new_df = new_df.values.astype(float)\n",
    "    new_df[np.isnan(new_df)] = 0\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_pop[['person_id', 'sz_flag']].set_index('person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cddfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_full_df(train_data, train_data_mat, tvt_split['train_pids'])\n",
    "train_data = train_data.reshape(len(train_pids), int(len(train_data)/len(train_pids)), train_data_mat.shape[1])\n",
    "train_labels = labels.loc[train_pids]\n",
    "print(train_data.shape, train_labels.shape)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.Tensor(train_data), torch.Tensor(train_labels.values))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=0, batch_size=1024, shuffle = True, worker_init_fn=np.random.seed(14))\n",
    "torch.save(train_loader, int_path + 'CUMC_2_16_dl_da_vanilla_train_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all(train_labels == labels.loc[list(tvt_split['train_pids'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_loader\n",
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = get_full_df(val_data, val_data_mat, tvt_split['val_pids'])\n",
    "val_data = val_data.reshape(len(val_pids), int(len(val_data)/len(val_pids)), val_data_mat.shape[1])\n",
    "val_labels = labels.loc[val_pids]\n",
    "print(val_data.shape, val_labels.shape)\n",
    "\n",
    "test_data = get_full_df(test_data, test_data_mat, tvt_split['test_pids'])\n",
    "test_data = test_data.reshape(len(test_pids), int(len(test_data)/len(test_pids)), test_data_mat.shape[1])\n",
    "test_labels = labels.loc[test_pids]\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864263b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(val_data), torch.Tensor(val_labels.values))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4096, shuffle=True)\n",
    "torch.save(val_loader, int_path+'MDCD_2_10_dl_da_val_loader_shuffled.pth')\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4096, shuffle=False)\n",
    "torch.save(val_loader, int_path+'MDCD_2_10_dl_da_val_loader_unshuffled.pth')\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.Tensor(test_data), torch.Tensor(test_labels.values))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4096, shuffle = False)\n",
    "torch.save(test_loader, int_path+'MDCD_2_10_dl_da_test_loader_unshuffled.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all(val_labels == labels.loc[list(tvt_split['val_pids'])]))\n",
    "print(all(test_labels == labels.loc[list(tvt_split['test_pids'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab8b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fc0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
