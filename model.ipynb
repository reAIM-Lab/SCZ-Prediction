{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfd74a68",
   "metadata": {},
   "source": [
    "This file is for creating the dataset that contains a new entry every 6 visits per patient. We also run the XGBoost model with hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf5ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "import joblib\n",
    "from itertools import product\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9997b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = (\n",
    "    'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    'SERVER=OMOP.DBMI.COLUMBIA.EDU;'\n",
    "    'DATABASE=cdm_mdcd;'\n",
    "    'TRUSTED_CONNECTION=YES;')\n",
    "\n",
    "conn = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3814b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934800c",
   "metadata": {},
   "source": [
    "# DATASET CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033ff67",
   "metadata": {},
   "source": [
    "### Import the population dataframe and constrict to the correct set of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days_prediction = 90\n",
    "df_pop = pd.read_csv(path+'population.csv')\n",
    "df_pop.rename({'psychosis_dx_date':'psychosis_diagnosis_date'}, axis=1, inplace=True)\n",
    "df_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_pop['psychosis_diagnosis_date'], format=\"mixed\")\n",
    "df_pop['cohort_start_date'] = pd.to_datetime(df_pop['cohort_start_date'])\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = pd.read_csv(path+'temporal_visits.csv')\n",
    "df_pop = df_pop.merge(all_visits.groupby('person_id').min()['visit_start_date'], how='left', left_on='person_id',right_index=True)\n",
    "df_pop.rename({'visit_start_date':'first_visit'}, axis=1, inplace=True)\n",
    "df_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256e2e1",
   "metadata": {},
   "source": [
    "### Import the temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds = pd.read_csv(path+'temporal_conditions.csv')\n",
    "all_meds = pd.read_csv(path+'temporal_medications.csv')\n",
    "all_procedures = pd.read_csv(path+'temporal_procedures.csv')\n",
    "all_labs = pd.read_csv(path+'temporal_labs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468a954",
   "metadata": {},
   "source": [
    "### Restrict to appropriate time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78330c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meds = all_meds.loc[all_meds['person_id'].isin(df_pop['person_id'])]\n",
    "all_meds['cohort_start_date'] = pd.to_datetime(all_meds['cohort_start_date'])\n",
    "all_meds['drug_era_start_date'] = pd.to_datetime(all_meds['drug_era_start_date'])\n",
    "all_meds['drug_era_end_date'] = pd.to_datetime(all_meds['drug_era_end_date'])\n",
    "all_meds = all_meds.loc[(all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days >= num_days_prediction]\n",
    "all_meds['days_to_cohort_start'] = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9382cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = all_visits.loc[all_visits['person_id'].isin(df_pop['person_id'])]\n",
    "all_visits['cohort_start_date'] = pd.to_datetime(all_visits['cohort_start_date'])\n",
    "all_visits['visit_start_date'] = pd.to_datetime(all_visits['visit_start_date'])\n",
    "all_visits['visit_end_date'] = pd.to_datetime(all_visits['visit_end_date'])\n",
    "all_visits = all_visits.loc[(all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days >= num_days_prediction]\n",
    "all_visits['days_to_cohort_start'] = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conds = all_conds.loc[all_conds['person_id'].isin(df_pop['person_id'])]\n",
    "all_conds['cohort_start_date'] = pd.to_datetime(all_conds['cohort_start_date'])\n",
    "all_conds['condition_start_date'] = pd.to_datetime(all_conds['condition_start_date'])\n",
    "all_conds['days_to_cohort_start'] = (all_conds['cohort_start_date']-all_conds['condition_start_date']).dt.days\n",
    "all_conds = all_conds.loc[all_conds['days_to_cohort_start'] >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_procedures = all_procedures.loc[all_procedures['person_id'].isin(df_pop['person_id'])]\n",
    "all_procedures['cohort_start_date'] = pd.to_datetime(all_procedures['cohort_start_date'])\n",
    "all_procedures['procedure_date'] = pd.to_datetime(all_procedures['procedure_date'])\n",
    "all_procedures['days_to_cohort_start'] = (all_procedures['cohort_start_date']-all_procedures['procedure_date']).dt.days\n",
    "all_procedures = all_procedures.loc[all_procedures['days_to_cohort_start'] >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs = all_labs.loc[all_labs['person_id'].isin(df_pop['person_id'])]\n",
    "all_labs['cohort_start_date'] = pd.to_datetime(all_labs['cohort_start_date'])\n",
    "all_labs['measurement_date'] = pd.to_datetime(all_labs['measurement_date'])\n",
    "all_labs['days_to_cohort_start'] = (all_labs['cohort_start_date']-all_labs['measurement_date']).dt.days\n",
    "all_labs = all_labs.loc[all_labs['days_to_cohort_start'] >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs['concept_name'].replace({'Methadone':'Methadone_Lab'}, inplace=True)\n",
    "all_procedures['concept_name'].replace({'Methadone':'Methadone_Procedure'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59500ddb",
   "metadata": {},
   "source": [
    "### delete rare occurrences (e.g. any concept id that does not appear at least once for unique patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82390945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rare_occurrences(df, col_concept, col_id = 'person_id', size_pop = len(df_pop)):\n",
    "    unique_occurrences = df[['person_id', col_concept]].drop_duplicates()\n",
    "    unique_occurrences = unique_occurrences.value_counts(col_concept)\n",
    "    common_occurrences = unique_occurrences[unique_occurrences/size_pop > 0.01].index\n",
    "    return df.loc[df[col_concept].isin(common_occurrences)]\n",
    "all_conds = drop_rare_occurrences(all_conds, 'condition_concept_id')\n",
    "all_meds = drop_rare_occurrences(all_meds, 'drug_concept_id')\n",
    "all_procedures = drop_rare_occurrences(all_procedures, 'procedure_concept_id')\n",
    "all_labs = drop_rare_occurrences(all_labs, 'measurement_concept_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d369f73",
   "metadata": {},
   "source": [
    "### Check that the minimum time between cohort start date and start/end dates for healthcare services is over 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = (all_labs['cohort_start_date']-all_labs['measurement_date']).dt.days\n",
    "print('Labs:', check.min(), check.max())\n",
    "\n",
    "check = (all_procedures['cohort_start_date']-all_procedures['procedure_date']).dt.days\n",
    "print('Procedures:', check.min(), check.max())\n",
    "\n",
    "check = (all_conds['cohort_start_date']-all_conds['condition_start_date']).dt.days\n",
    "print('Conditions:', check.min(), check.max())\n",
    "\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_start_date']).dt.days\n",
    "print('Meds (Start of prescription):', check.min(), check.max())\n",
    "check = (all_meds['cohort_start_date']-all_meds['drug_era_end_date']).dt.days\n",
    "print('Meds (End of prescription):', check.min(), check.max())\n",
    "\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_start_date']).dt.days\n",
    "print('Visits (Start of visit):', check.min(), check.max())\n",
    "check = (all_visits['cohort_start_date']-all_visits['visit_end_date']).dt.days\n",
    "print('Visits (End of visit):', check.min(), check.max())\n",
    "\n",
    "print('Check presence of SCZ:',len(all_conds.loc[all_conds['concept_name'].isin(['Schizophrenia', 'Paranoid schizophrenia'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0683de",
   "metadata": {},
   "source": [
    "### check that the cohort start date is the same across all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cohort_start = df_pop[['person_id','cohort_start_date']]\n",
    "check_cohort_start = check_cohort_start.merge(all_conds[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_pop','_cond'])\n",
    "check_cohort_start = check_cohort_start.merge(all_visits[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes = ['_old1','_visits'])\n",
    "check_cohort_start = check_cohort_start.merge(all_procedures[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old2','_pro'])\n",
    "check_cohort_start = check_cohort_start.merge(all_labs[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old3','_labs'])\n",
    "check_cohort_start = check_cohort_start.merge(all_meds[['person_id','cohort_start_date']].drop_duplicates(),how='left', left_on='person_id', right_on='person_id', suffixes=['_old4','_meds'])\n",
    "check_cohort_start.set_index('person_id',inplace=True)\n",
    "check_cohort_start = check_cohort_start.T\n",
    "num_unique = check_cohort_start.T.apply(lambda x: x.nunique(), axis=1)\n",
    "print('Number of places where cohort start date doesnt align:',(num_unique>1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd370ed",
   "metadata": {},
   "source": [
    "### Make SQL queries for grouping conditions and medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions mapping\n",
    "conditions_mapping_query = (\"SELECT c_icd10.concept_id as icd10_ancestor_concept_id,c_icd10.concept_name as icd10_ancestor_concept_name, c_icd10.concept_code as icd10_code, rel.concept_id_2 as standard_ancestor_concept_id, c_rel.concept_name as standard_ancestor_concept_name, ca.descendant_concept_id as standard_descendant_concept_id, c_new.concept_name as standard_descendant_concept_name, c_new.standard_concept as check_standard \"+\n",
    "    \"FROM dbo.concept as c_icd10 \"+\n",
    "    \"LEFT JOIN dbo.concept_relationship as rel on rel.concept_id_1 = c_icd10.concept_id \"+\n",
    "    \"LEFT JOIN dbo.concept as c_rel on rel.concept_id_2 = c_rel.concept_id \"+\n",
    "    \"LEFT JOIN dbo.concept_ancestor as ca ON ca.ancestor_concept_id = rel.concept_id_2 \"+\n",
    "    \"LEFT JOIN dbo.concept as c_new on c_new.concept_id = ca.descendant_concept_id \"+\n",
    "    \"WHERE c_icd10.concept_class_id = '3-char nonbill code' and c_icd10.vocabulary_id = 'ICD10CM' \"+\n",
    "    \"AND rel.relationship_id = 'Maps to' AND c_rel.standard_concept = 'S'\")\n",
    "conditions_mapping = pd.io.sql.read_sql(conditions_mapping_query, conn)\n",
    "\n",
    "conditions_mapping = conditions_mapping.loc[conditions_mapping['standard_descendant_concept_id'].isin(list(all_conds['condition_concept_id'].unique()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medications mapping \n",
    "medications_mapping_query = (\"SELECT c_atc.concept_id as atc_concept_id, c_atc.concept_name as atc_concept_name, c_standard.concept_id as standard_concept_id, c_standard.concept_name as standard_concept_name \"+\n",
    "                             \"FROM dbo.concept as c_atc \"+\n",
    "                             \"LEFT JOIN dbo.concept_ancestor as ca on ancestor_concept_id=c_atc.concept_id \"+\n",
    "                             \"LEFT JOIN dbo.concept as c_standard on c_standard.concept_id = descendant_concept_id \"+\n",
    "                             \"WHERE c_atc.concept_class_id = 'ATC 3rd' AND c_standard.standard_concept = 'S'\")\n",
    "\n",
    "medications_mapping = pd.io.sql.read_sql(medications_mapping_query, conn)\n",
    "medications_mapping = medications_mapping.loc[medications_mapping['standard_concept_id'].isin(list(all_meds['drug_concept_id'].unique()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e40f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medications mapping: move Lithium to the antiepileptics category\n",
    "def generate_code_list(drugtype, concept_class):\n",
    "    sql_query = (\"SELECT ancestor_concept_id, descendant_concept_id, concept_name \" + \n",
    "               \"FROM dbo.concept_ancestor JOIN dbo.concept ON descendant_concept_id = concept_id \"+\n",
    "               \"WHERE ancestor_concept_id = (SELECT concept_id from dbo.concept WHERE concept_class_id = '\"+concept_class+\"' AND concept_name = '\"+drugtype+\"');\")\n",
    "    codes_list = pd.read_sql(sql_query, conn)\n",
    "    return list(codes_list['descendant_concept_id'])\n",
    "\n",
    "lithium_list = generate_code_list('Lithium', 'ATC 4th')\n",
    "medications_mapping.loc[(medications_mapping['standard_concept_id'].isin(lithium_list))&(medications_mapping['atc_concept_name']=='ANTIPSYCHOTICS'), 'atc_concept_name'] = 'ANTIEPILEPTICS'\n",
    "medications_mapping['atc_concept_name'].replace({'ANTIEPILEPTICS': 'MOOD STABILIZERS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"SELECT vo.person_id, vo.visit_occurrence_id, vo.visit_concept_id, co.condition_start_date, vo.visit_start_date, vo.visit_end_date, co.condition_concept_id, c.concept_name as condition_name, p.race_concept_id, p.gender_concept_id \"+\n",
    "         \"FROM dbo.visit_occurrence as vo LEFT JOIN dbo.condition_occurrence as co on co.visit_occurrence_id = vo.visit_occurrence_id \"+\n",
    "         \"LEFT JOIN dbo.concept as c on c.concept_id = co.condition_concept_id \"+\n",
    "         \"LEFT JOIN dbo.person as p on p.person_id = vo.person_id \"+\n",
    "         \"WHERE vo.visit_concept_id = 9201 AND condition_concept_id IN \"+\n",
    "         \"(SELECT DISTINCT concept_id_2 FROM dbo.concept as c LEFT JOIN dbo.concept_relationship on concept_id_1 = concept_id WHERE c.concept_code LIKE 'F%' AND c.vocabulary_id = 'ICD10CM' AND relationship_id = 'Maps to')\")\n",
    "\n",
    "psych_hosp = pd.io.sql.read_sql(query, conn)\n",
    "list_psych_visits = list(psych_hosp['visit_occurrence_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67faf677",
   "metadata": {},
   "source": [
    "### Define a function that gives us a dataframe of person_id by features (count per year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d215fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp pop needs to have a \"years obs\" column\n",
    "def make_static_df(temp_pop, temp_conds, temp_meds, temp_visits, temp_procedures, temp_labs):\n",
    "    ### CONDITIONS\n",
    "    count_conds = temp_conds.groupby(by=[\"person_id\", \"condition_concept_id\"]).size().reset_index()\n",
    "    cond_features = pd.DataFrame(data=0, index=temp_pop['person_id'], columns=conditions_mapping['icd10_code'].unique())\n",
    "    icd_dict = conditions_mapping.groupby('icd10_code')['standard_descendant_concept_id'].apply(list).to_dict()\n",
    "\n",
    "    list_icd_codes = list(conditions_mapping['icd10_code'].unique())\n",
    "    for i in (range(0,len(icd_dict))):\n",
    "        temp = count_conds.loc[count_conds['condition_concept_id'].isin(icd_dict[list_icd_codes[i]])].groupby('person_id').count()['condition_concept_id']\n",
    "        cond_features.loc[temp.index, list_icd_codes[i]] = temp.values\n",
    "        \n",
    "    cond_features_binary = (cond_features > 0)*1\n",
    "    # eliminate icd10 codes with <= 1% prevalence\n",
    "    #cond_features_binary = cond_features_binary[cond_features_binary.columns[100*cond_features_binary.sum(axis=0)/len(cond_features_binary)>1]]\n",
    "    #cond_features = cond_features[cond_features_binary.columns[100*cond_features_binary.sum(axis=0)/len(cond_features_binary)>1]]\n",
    "    # adjust so that cond_features is per year of observation\n",
    "    cond_features = cond_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    cond_features = cond_features.div(cond_features.years_obs, axis=0) \n",
    "    cond_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    ### MEDICATIONS\n",
    "    med_features = pd.DataFrame(data=0, index=temp_pop['person_id'], columns=medications_mapping['atc_concept_name'].unique())\n",
    "    meds_dict = medications_mapping.groupby('atc_concept_name')['standard_concept_id'].apply(list).to_dict()\n",
    "    temp_meds['drug_exposure_days'] = (temp_meds['drug_era_end_date']-temp_meds['drug_era_start_date']).dt.days\n",
    "    count_meds = temp_meds[['person_id', 'drug_concept_id', 'drug_exposure_days']].groupby(['person_id', 'drug_concept_id']).sum().reset_index()\n",
    "\n",
    "    list_atc_codes = list(medications_mapping['atc_concept_name'].unique())\n",
    "    for i in (range(0,len(meds_dict))):\n",
    "        temp = count_meds.loc[count_meds['drug_concept_id'].isin(meds_dict[list_atc_codes[i]])].groupby('person_id')['drug_exposure_days'].sum()\n",
    "        med_features.loc[temp.index, list_atc_codes[i]] = temp.values\n",
    "        \n",
    "    # limit to only medication classes with > 1% prevalence\n",
    "    #med_features_binary = (med_features > 0)*1\n",
    "    #med_cols = list(med_features_binary.columns[100*med_features_binary.sum(axis=0)/len(med_features_binary)>1])\n",
    "    #med_features = med_features[med_cols]\n",
    "    # adjust so that med_features is per year of observation\n",
    "    med_features = med_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    med_features = med_features.div(med_features.years_obs, axis=0) \n",
    "    med_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "\n",
    "    ###VISITS\n",
    "    # Time from most recent visit to end of observation period\n",
    "    temp_visits.merge(temp_pop[['person_id', 'cutoff_date']], how='left', left_on = 'person_id', right_on='person_id')\n",
    "    temp_visits['days_to_end_obs'] = (temp_visits['cutoff_date']-temp_visits['visit_end_date']).dt.days\n",
    "    if (temp_visits['visit_start_date'] > temp_visits['cutoff_date']).sum()>0:\n",
    "        print('Start date issue')\n",
    "    if (temp_visits['visit_end_date'] > temp_visits['cutoff_date']).sum()>0:\n",
    "        print('End date issue')\n",
    "    if (temp_visits['days_to_end_obs']).max() < 0:\n",
    "        print('Issue with end obs')\n",
    "\n",
    "    visits_timing = temp_visits.groupby(['person_id', 'visit_concept_id']).min()['days_to_end_obs']\n",
    "    visits_timing = visits_timing.reset_index()\n",
    "    visits_timing = visits_timing.pivot_table(index='person_id', columns = 'visit_concept_id', values='days_to_end_obs', fill_value = 2190)\n",
    "    visits_timing.rename({9201:'most_recent_inpatient', 9202: 'most_recent_outpatient', 9203:'most_recent_ED', 42898160:'most_recent_nonhospital'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Number of visits\n",
    "    num_visits = temp_visits.groupby(['person_id', 'visit_concept_id']).count()['cohort_start_date'].reset_index()\n",
    "    num_visits = num_visits.pivot_table(index='person_id', columns = 'visit_concept_id', values = 'cohort_start_date', fill_value=0)\n",
    "    num_visits.rename({9201:'num_visits_inpatient', 9202: 'num_visits_outpatient', 9203:'num_visits_ED', 42898160:'num_visits_nonhospital'}, axis=1, inplace=True)\n",
    "    # adjust so that num_visits is per year of observation\n",
    "    num_visits = num_visits.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    num_visits = num_visits.div(num_visits.years_obs, axis=0) \n",
    "    num_visits.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    # Length of Stay\n",
    "    non_outpatient = temp_visits.loc[temp_visits['visit_concept_id']!=9202]\n",
    "\n",
    "    non_outpatient['los'] = (non_outpatient['visit_end_date']-non_outpatient['visit_start_date']).dt.days\n",
    "    los = non_outpatient.groupby(['person_id', 'visit_concept_id']).agg({'los':['sum', 'max', 'min', 'mean']})\n",
    "    los = los.reset_index()\n",
    "    los.columns = [' '.join(col).strip() for col in los.columns.values]\n",
    "\n",
    "    los = los.pivot_table(index='person_id', columns = 'visit_concept_id', values=['los sum', 'los max', 'los min', 'los mean'], fill_value = 0)\n",
    "    los.columns = [''.join(str(col)).strip() for col in los.columns.values]\n",
    "\n",
    "    #rename columns\n",
    "    if len(los.columns) == 12:\n",
    "        los.columns = ['los_max_inpatient', 'los_max_ed', 'los_max_nonhospitalization',\n",
    "        'los_mean_inpatient', 'los_mean_ed', 'los_mean_nonhospitalization',\n",
    "        'los_min_inpatient', 'los_min_ed', 'los_min_nonhospitalization',\n",
    "        'los_sum_inpatient', 'los_sum_ed', 'los_sum_nonhospitalization']\n",
    "    elif len(los.columns) == 8:\n",
    "        los.columns = ['los_max_inpatient', 'los_max_ed',\n",
    "        'los_mean_inpatient', 'los_mean_ed',\n",
    "        'los_min_inpatient', 'los_min_ed', \n",
    "        'los_sum_inpatient', 'los_sum_ed']\n",
    "\n",
    "    \n",
    "    visits_features = visits_timing.merge(num_visits, how='outer', left_index=True, right_index=True)\n",
    "    visits_features = visits_features.merge(los, how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    #### VISITS: INPATIENT HOSPITALIZATIONS\n",
    "    # limit psych hospitalizations to ones eligible (according to preprocessed visits df)\n",
    "    psych_hospitalizations = temp_visits.loc[temp_visits['visit_occurrence_id'].isin(list_psych_visits)]\n",
    "    \n",
    "    # Number of visits\n",
    "    num_visits = psych_hospitalizations.groupby('person_id').count()['cohort_start_date'].reset_index()\n",
    "    num_visits.rename({'cohort_start_date':'num_psych_hospitalizations'}, inplace=True, axis=1)\n",
    "    num_visits.set_index('person_id', inplace=True)\n",
    "    # adjust so that num_visits is per year of observation\n",
    "    num_visits = num_visits.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    num_visits = num_visits.div(num_visits.years_obs, axis=0) \n",
    "    num_visits.drop(['years_obs'], axis=1, inplace=True)\n",
    "\n",
    "    visits_features = visits_features.merge(num_visits, how = 'left', right_index=True, left_index=True).fillna(0)\n",
    "\n",
    "    # Length of Stay\n",
    "    temp_visits['los'] = (temp_visits['visit_end_date']-temp_visits['visit_start_date']).dt.days\n",
    "    los = temp_visits.groupby(['person_id']).agg({'los':['sum', 'max', 'min', 'mean']})\n",
    "    los.columns = [' '.join(col).strip() for col in los.columns.values]\n",
    "    los.columns = ['los psych sum', 'los psych max', 'los psych min', 'los psych mean']\n",
    "\n",
    "    visits_features = visits_features.merge(los, how = 'left', right_index=True, left_index=True).fillna(0)\n",
    "\n",
    "    # Time from most recent visit to end of observation period\n",
    "    visits_timing = psych_hospitalizations.groupby('person_id').min()['days_to_end_obs']\n",
    "    visits_timing.name = 'most_recent_psych_inpatient'\n",
    "\n",
    "    # merge into visits_features\n",
    "    visits_features = visits_features.merge(visits_timing, how = 'left', right_index=True, left_index=True).fillna(2190)\n",
    "\n",
    "    ### PROCEDURES\n",
    "    \"\"\"\n",
    "    # drop procedures do not occur in at least 1% of patients\n",
    "    #procedure_feature_select = temp_procedures[['person_id', 'procedure_concept_id']].drop_duplicates()\n",
    "    #procedure_feature_select = procedure_feature_select.groupby('procedure_concept_id').count()\n",
    "    #procedure_feature_select['prevalence'] = 100*procedure_feature_select['person_id']/len(temp_pop)\n",
    "    #common_procedures = list(procedure_feature_select.loc[procedure_feature_select['prevalence']>1].index)\n",
    "    \n",
    "    #all_common_procedures = temp_procedures.loc[temp_procedures['procedure_concept_id'].isin(common_procedures)]\n",
    "    # use a pivot table to get the counts and binary occurrence of each procedure code\n",
    "    \"\"\"\n",
    "    procedures_features = temp_procedures.pivot_table(index='person_id', columns='concept_name', aggfunc='size', fill_value=0)\n",
    "    \n",
    "    # get procedures per year\n",
    "    procedures_features = procedures_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    procedures_features = procedures_features.div(procedures_features.years_obs, axis=0) \n",
    "    procedures_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    ### LABS\n",
    "    # drop labs that do not occur in at least 1% of patients\n",
    "    \"\"\"\n",
    "    lab_feature_select = temp_labs[['person_id', 'measurement_concept_id']].drop_duplicates()\n",
    "    lab_feature_select = lab_feature_select.groupby('measurement_concept_id').count()\n",
    "    lab_feature_select['prevalence'] = 100*lab_feature_select['person_id']/len(temp_pop)\n",
    "    common_labs = list(lab_feature_select.loc[lab_feature_select['prevalence']>1].index)\n",
    "\n",
    "    all_common_labs = temp_labs.loc[temp_labs['measurement_concept_id'].isin(common_labs)]\n",
    "    # use a pivot table to get the counts and binary occurrence of each lab code\n",
    "    \"\"\"\n",
    "    lab_features = temp_labs.pivot_table(index='person_id', columns='concept_name', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # get procedures per year\n",
    "    lab_features = lab_features.merge(temp_pop[['person_id','years_obs']].set_index('person_id'), how='left', left_index=True, right_index=True)\n",
    "    lab_features = lab_features.div(lab_features.years_obs, axis=0) \n",
    "    lab_features.drop(['years_obs'], axis=1, inplace=True)\n",
    "    \n",
    "    atemporal_features = pd.concat([cond_features, med_features, procedures_features, lab_features, visits_features], axis=1)\n",
    "    return atemporal_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f253",
   "metadata": {},
   "source": [
    "### Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_race = df_pop.groupby('race_concept_id').count()['cohort_definition_id']\n",
    "scz_race = df_pop.loc[df_pop['sz_flag']==1].groupby('race_concept_id').count()['cohort_definition_id']\n",
    "noscz_race = df_pop.loc[df_pop['sz_flag']==0].groupby('race_concept_id').count()['cohort_definition_id']\n",
    "race_counts = pd.DataFrame(pd.concat([all_race, scz_race, noscz_race], axis=1).values, \n",
    "             index=['Missing', 'Black or African American', 'White'], columns = ['All Patients', 'SCZ Patients', 'No SCZ Patients'])\n",
    "\n",
    "all_gender = df_pop.groupby('gender_concept_id').count()['cohort_definition_id']\n",
    "scz_gender = df_pop.loc[df_pop['sz_flag']==1].groupby('gender_concept_id').count()['cohort_definition_id']\n",
    "noscz_gender = df_pop.loc[df_pop['sz_flag']==0].groupby('gender_concept_id').count()['cohort_definition_id']\n",
    "gender_counts = pd.DataFrame(pd.concat([all_gender, scz_gender, noscz_gender], axis=1).values, \n",
    "             index=['Male', 'Female'], columns = ['All Patients', 'SCZ Patients', 'No SCZ Patients'])\n",
    "\n",
    "age = pd.DataFrame(df_pop.groupby('sz_flag')['age_diagnosis'].agg(['mean','std']).values, index=['SCZ Patients', 'No SCZ Patients'],\n",
    "            columns = ['Mean Age', 'STD Age']).T\n",
    "age['All Patients'] = df_pop['age_diagnosis'].mean(), df_pop['age_diagnosis'].std()\n",
    "\n",
    "t1_counts = pd.concat([race_counts, gender_counts, age])\n",
    "t1_counts.loc['Total Patients'] = len(df_pop), sum(df_pop['sz_flag']), len(df_pop)-sum(df_pop['sz_flag'])\n",
    "t1_counts\n",
    "\n",
    "t1_percents = t1_counts.loc[['Missing', 'Black or African American', 'White', 'Male','Female']]\n",
    "t1_percents = t1_percents/t1_counts.loc['Total Patients']*100\n",
    "t1_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e928ab1",
   "metadata": {},
   "source": [
    "### Create dataframe that identifies the iteration, first visit (start date), last visit (start date), and years of observation\n",
    "\n",
    "We do this by ordering all the visits within each patient and then choosing every 5th visit\n",
    "Then manually want to add back the psychosis date as the first date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_visits = all_visits.groupby('person_id').apply(pd.DataFrame.sort_values, ['visit_start_date'])\n",
    "sorted_visits.reset_index(drop=True, inplace=True)\n",
    "sorted_visits = sorted_visits.merge(df_pop[['person_id', 'psychosis_diagnosis_date', 'first_visit']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "sorted_visits = sorted_visits[['person_id', 'psychosis_diagnosis_date', 'first_visit', 'visit_start_date', 'cohort_start_date']].drop_duplicates()\n",
    "sorted_visits = sorted_visits.loc[(sorted_visits['visit_start_date']>sorted_visits['psychosis_diagnosis_date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b24b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_visit = np.asarray(sorted_visits.groupby('person_id').cumcount())\n",
    "sorted_visits['nth_visit'] = nth_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_visits = sorted_visits.loc[sorted_visits['nth_visit']%6==0]\n",
    "sorted_visits['nth_visit'] = sorted_visits['nth_visit'] // 6\n",
    "sorted_visits['nth_visit'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_psychosis_date = df_pop[['person_id', 'psychosis_diagnosis_date', 'first_visit', 'cohort_start_date']]\n",
    "add_psychosis_date['visit_start_date'] = add_psychosis_date['psychosis_diagnosis_date']\n",
    "add_psychosis_date['nth_visit'] = 0\n",
    "df_iter_pop = pd.concat([sorted_visits, add_psychosis_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_iter_pop['nth_visit'].value_counts())\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Number of patients in that iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e45fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter_pop.rename({'nth_visit':'iteration', 'visit_start_date':'cutoff_date'}, axis=1, inplace=True)\n",
    "\n",
    "df_iter_pop['cohort_start_date'] = pd.to_datetime(df_iter_pop['cohort_start_date'])\n",
    "df_iter_pop['cutoff_date'] = pd.to_datetime(df_iter_pop['cutoff_date'])\n",
    "df_iter_pop['first_visit'] = pd.to_datetime(df_iter_pop['first_visit'])\n",
    "\n",
    "df_iter_pop['years_obs'] = (df_iter_pop['cutoff_date']-df_iter_pop['first_visit']).dt.days/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter_pop['censor_date'] = df_iter_pop['cohort_start_date']-pd.Timedelta(90, 'days') \n",
    "df_iter_pop = df_iter_pop.loc[df_iter_pop['cutoff_date']<=df_iter_pop['censor_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter_pop.to_csv('stored_data/iterated_population_6_visits.csv')\n",
    "print(len(df_iter_pop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5168b28f",
   "metadata": {},
   "source": [
    "### Loop through iteration to get features for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803866a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_feature_dfs = []\n",
    "\n",
    "for iteration in tqdm(range(0,df_iter_pop['iteration'].max())): \n",
    "    # only need iter0 if you change stuff above\n",
    "    temp_df_iter_pop = df_iter_pop.loc[(df_iter_pop['iteration'] == iteration)&df_iter_pop['years_obs']>0.5]\n",
    "\n",
    "    # for conditions, labs, procedures, just compare the start_date to the cutoff date\n",
    "    temp_conds = all_conds.loc[all_conds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_conds = temp_conds.merge(temp_df_iter_pop[['person_id','cutoff_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_conds = temp_conds.loc[temp_conds['condition_start_date']< temp_conds['cutoff_date']]\n",
    "\n",
    "    temp_labs = all_labs.loc[all_labs['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_labs = temp_labs.merge(temp_df_iter_pop[['person_id','cutoff_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_labs = temp_labs.loc[temp_labs['measurement_date']< temp_labs['cutoff_date']]\n",
    "\n",
    "    temp_procedures = all_procedures.loc[all_procedures['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_procedures = temp_procedures.merge(temp_df_iter_pop[['person_id','cutoff_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_procedures = temp_procedures.loc[temp_procedures['procedure_date']< temp_procedures['cutoff_date']]\n",
    "\n",
    "    \"\"\"\n",
    "    for medications and visits, we want to look at \n",
    "    1. limit to visit/medication start dates prior to cutoff date\n",
    "    2. if cutoff date is prior to visit/medication end date, make the cutoff date the new end date\n",
    "    \"\"\"\n",
    "    temp_meds = all_meds.loc[all_meds['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_meds = temp_meds.merge(temp_df_iter_pop[['person_id','cutoff_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_meds = temp_meds.loc[temp_meds['drug_era_start_date']< temp_meds['cutoff_date']]\n",
    "    temp_meds.loc[temp_meds['drug_era_end_date']>temp_meds['cutoff_date'], 'drug_era_end_date']=temp_meds['cutoff_date']\n",
    "\n",
    "    temp_visits = all_visits.loc[all_visits['person_id'].isin(temp_df_iter_pop['person_id'])]\n",
    "    temp_visits = temp_visits.merge(temp_df_iter_pop[['person_id','cutoff_date']], how = 'left', left_on = 'person_id', right_on = 'person_id')\n",
    "    temp_visits = temp_visits.loc[temp_visits['visit_start_date']< temp_visits['cutoff_date']]\n",
    "    temp_visits.loc[temp_visits['visit_end_date']>temp_visits['cutoff_date'], 'visit_end_date']=temp_visits['cutoff_date']\n",
    "    \n",
    "    if min((temp_conds['cutoff_date']-temp_conds['condition_start_date']).dt.days) < 0:\n",
    "        print('Leakage in conds')        \n",
    "    if min((temp_labs['cutoff_date']-temp_labs['measurement_date']).dt.days) < 0:\n",
    "        print('Leakage in labs')\n",
    "    if min((temp_procedures['cutoff_date']-temp_procedures['procedure_date']).dt.days) < 0:\n",
    "        print('Leakage in procedures')\n",
    "    if min((temp_visits['cutoff_date']-temp_visits['visit_start_date']).dt.days) < 0:\n",
    "        print('Leakage in visit starts')\n",
    "    if min((temp_visits['cutoff_date']-temp_visits['visit_end_date']).dt.days) < 0:\n",
    "        print('Leakage in visit ends')\n",
    "    if min((temp_meds['cutoff_date']-temp_meds['drug_era_start_date']).dt.days) < 0:\n",
    "        print('Leakage in med starts')\n",
    "    if min((temp_meds['cutoff_date']-temp_meds['drug_era_end_date']).dt.days) < 0:\n",
    "        print('Leakage in med ends')\n",
    "\n",
    "    all_features = make_static_df(temp_df_iter_pop, temp_conds, temp_meds, temp_visits, temp_procedures, temp_labs)\n",
    "    all_features['iteration'] = iteration\n",
    "    \n",
    "    all_features.to_csv('stored_data/visit_iters_6/pre_psychosis_features_iter_'+str(iteration)+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c50f1",
   "metadata": {},
   "source": [
    "### Read in and concatenate the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e3721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 590/590 [04:55<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "list_files = []\n",
    "list_filenames = os.listdir('stored_data/visit_iters_6')\n",
    "for filename_ind in tqdm(range(len(list_filenames))):\n",
    "    filename = list_filenames[filename_ind]\n",
    "    list_files.append(pd.read_csv('stored_data/visit_iters_6/'+filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9a8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_iters = pd.concat(list_files)\n",
    "df_all_iters.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934734af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "print('Unnamed: 0' in df_all_iters.columns, 'Unnamed:0' in df_all_iters.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f457a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_files\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edbf8b",
   "metadata": {},
   "source": [
    "# Training the XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd4672",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51484069",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days_prediction = 90\n",
    "df_pop = pd.read_csv(path+'population.csv')\n",
    "df_pop.rename({'psychosis_dx_date':'psychosis_diagnosis_date'}, axis=1, inplace=True)\n",
    "df_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_pop['psychosis_diagnosis_date'], format=\"mixed\")\n",
    "df_pop['cohort_start_date'] = pd.to_datetime(df_pop['cohort_start_date'])\n",
    "df_pop = df_pop.loc[(df_pop['cohort_start_date']-df_pop['psychosis_diagnosis_date']).dt.days >= num_days_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d6f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY IF YOU ARE RESTRICTING THE DATA IN SOME WAY\n",
    "\n",
    "\"\"\"\n",
    "#restricting to some number of iterations\n",
    "df_all_iters = df_all_iters.loc[df_all_iters['iteration']<=209]\n",
    "df_pop = df_pop.loc[df_pop['person_id'].isin(df_all_iters['person_id'])]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "### restricting to only data points where we have 7 years of data\n",
    "\n",
    "df_iter_pop = pd.read_csv('stored_data/iterated_population_6_visits.csv')\n",
    "\n",
    "df_iter_pop['cutoff_date'] = pd.to_datetime(df_iter_pop['cutoff_date'])\n",
    "df_iter_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_iter_pop['psychosis_diagnosis_date'])\n",
    "df_iter_pop['years_since_psychosis_dx'] = (df_iter_pop['cutoff_date']-df_iter_pop['psychosis_diagnosis_date']).dt.days/365\n",
    "df_iter_pop = df_iter_pop.loc[df_iter_pop['years_since_psychosis_dx']<=7]\n",
    "\n",
    "df_all_iters.set_index(['person_id', 'iteration'], inplace=True)\n",
    "\n",
    "arr_vals = df_iter_pop[['person_id', 'iteration']].values\n",
    "pid_iter = [tuple(x) for x in arr_vals]\n",
    "\n",
    "df_all_iters = df_all_iters.loc[df_all_iters.index.isin(pid_iter)]\n",
    "df_all_iters.reset_index(inplace=True)\n",
    "df_pop = df_pop.loc[df_pop['person_id'].isin(df_all_iters['person_id'])]\n",
    "\n",
    "\n",
    "### restricting to only people who have <= 7 years of data between psychosis and observation\n",
    "df_iter_pop = pd.read_csv('stored_data/iterated_population_6_visits.csv')\n",
    "\n",
    "df_iter_pop['cutoff_date'] = pd.to_datetime(df_iter_pop['cutoff_date'])\n",
    "df_iter_pop['psychosis_diagnosis_date'] = pd.to_datetime(df_iter_pop['psychosis_diagnosis_date'])\n",
    "df_iter_pop['years_since_psychosis_dx'] = (df_iter_pop['cutoff_date']-df_iter_pop['psychosis_diagnosis_date']).dt.days/365\n",
    "max_obs = df_iter_pop.groupby('person_id').max()['years_since_psychosis_dx']\n",
    "\n",
    "patient_list = list(max_obs.loc[max_obs<=7].index)\n",
    "df_all_iters = df_all_iters.loc[df_all_iters['person_id'].isin(patient_list)]\n",
    "df_pop = df_pop.loc[df_pop['person_id'].isin(df_all_iters['person_id'])]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b14fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_all_iters[['person_id', 'iteration']].merge(df_pop[['person_id','sz_flag']], how='left', left_on = 'person_id', right_on='person_id')\n",
    "labels.set_index('person_id', inplace=True)\n",
    "\n",
    "df_all_iters.set_index('person_id', inplace=True)\n",
    "df_all_iters.drop(['iteration'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c27f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iteration    0\n",
       "sz_flag      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c62f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_train, pid_test, y_train, y_test = train_test_split(df_pop['person_id'], df_pop['sz_flag'], stratify=df_pop['sz_flag'], test_size=0.3, random_state = 4)\n",
    "\n",
    "X_train = df_all_iters.loc[pid_train.values]\n",
    "X_test = df_all_iters.loc[pid_test.values]\n",
    "\n",
    "y_train = labels.loc[pid_train.values, 'sz_flag']\n",
    "y_test = labels.loc[pid_test.values, 'sz_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19139eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cols = df_all_iters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1299fa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_all_iters\n",
    "del labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac66172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n",
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n",
      "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:507: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  array.dtypes.apply(is_sparse).any()):\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5909c4",
   "metadata": {},
   "source": [
    "### Fit XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b48e3146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9685988661047122\n",
      "(3, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9782467080326438\n",
      "(3, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9836718692655071\n",
      "(3, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9868792923824653\n",
      "(4, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9845632869392483\n",
      "(4, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9894929909001565\n",
      "(4, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992253728583081\n",
      "(4, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9938729835946502\n",
      "(5, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912097458933499\n",
      "(5, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9936492290997687\n",
      "(5, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9951481578915796\n",
      "(5, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9959400794977056\n"
     ]
    }
   ],
   "source": [
    "max_depths = np.asarray([3,4,5])\n",
    "estimators = np.asarray([150,200,250,300])\n",
    "params = list(product(max_depths, estimators))\n",
    "\n",
    "list_aucs = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=3)\n",
    "for p in params:\n",
    "    print(p)\n",
    "    depth, est = p\n",
    "    param_aucs = []\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "        train_temp, val_temp = X_train[train_index,:], X_train[val_index]\n",
    "        y_train_temp, y_val_temp = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        clf = XGBClassifier(seed=3, max_depth = depth, n_estimators = est)\n",
    "        clf.fit(train_temp, y_train_temp)\n",
    "        y_pred_temp = clf.predict(val_temp)\n",
    "        param_aucs.append(roc_auc_score(y_val_temp, y_pred_temp))\n",
    "        \n",
    "        del train_temp\n",
    "        del val_temp\n",
    "        gc.collect()\n",
    "    list_aucs.append(param_aucs)\n",
    "    print(np.mean(param_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab732a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 150) 0.9685988661047122\n",
      "(3, 200) 0.9782467080326438\n",
      "(3, 250) 0.9836718692655071\n",
      "(3, 300) 0.9868792923824653\n",
      "(4, 150) 0.9845632869392483\n",
      "(4, 200) 0.9894929909001565\n",
      "(4, 250) 0.992253728583081\n",
      "(4, 300) 0.9938729835946502\n",
      "(5, 150) 0.9912097458933499\n",
      "(5, 200) 0.9936492290997687\n",
      "(5, 250) 0.9951481578915796\n",
      "(5, 300) 0.9959400794977056\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(params, list_aucs):\n",
    "    print(i, np.mean(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47b6e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ak4885/.local/lib/python3.10/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(seed=3, max_depth = 5, n_estimators = 300)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "with open('models/xgb_6_visits_7yr_patient_cutoff.pkl','wb') as f:\n",
    "    pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
